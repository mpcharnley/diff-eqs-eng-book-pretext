<?xml version="1.0" encoding="UTF-8" ?>
<!-- Generated by Pandoc using pretext.lua -->



	<section xml:id="eigenmethod-repeat-section">
		<title>Eigenvalue method with repeated eigenvalues</title>

		<p>
			There is one remaining case for the two-component first-order linear system: repeated eigenvalues. As we have seen previously, it may happen that a matrix <m>A</m> has some eigenvalues. That is, the characteristic equation <m>\det(A-\lambda I) = 0</m> may have repeated roots. This is actually unlikely to happen for a random matrix. If we take a small perturbation of <m>A</m> (we change the entries of <m>A</m> slightly), we get a matrix with distinct eigenvalues. As any system we want to solve in practice is an approximation to reality anyway, it is not absolutely indispensable to know how to solve these corner cases. On the other hand, these cases do come up in applications from time to time. Furthermore, if we have distinct but very close eigenvalues, the behavior is similar to that of repeated eigenvalues, and so understanding that case will give us insight into what is going on.
		</p>


		<subsubsection xml:id="geometric-multiplicity">
			<title>Geometric multiplicity</title>

			<p>
				Take the diagonal matrix 
<me>
A =
\begin{bmatrix}
3 &amp; 0 \\ 0 &amp; 3
\end{bmatrix} .
</me>
 <m>A</m> has an eigenvalue 3 of multiplicity 2. We call the multiplicity of the eigenvalue in the characteristic equation the <em></em>. In this case, there also exist 2 linearly independent eigenvectors, <m>\left[ \begin{smallmatrix} 1 \\ 0 \end{smallmatrix} \right]</m> and <m>\left[ \begin{smallmatrix} 0 \\ 1 \end{smallmatrix} \right]</m> corresponding to the eigenvalue 3. This means that the so-called <em></em> of this eigenvalue is also 2. These terms have all been discussed previously in .
			</p>

			<p>
				In all the theorems where we required a matrix to have <m>n</m> distinct eigenvalues, we only really needed to have <m>n</m> linearly independent eigenvectors. For example, <m>{\vec{x}}' = A\vec{x}</m> has the general solution 
<me>
\vec{x} = 
c_1 \begin{bmatrix} 1 \\ 0 \end{bmatrix} e^{3t}
+ c_2 \begin{bmatrix} 0 \\ 1 \end{bmatrix} e^{3t} .
</me>
 Let us restate the theorem about real eigenvalues. In the following theorem we will repeat eigenvalues according to (algebraic) multiplicity. So for the matrix <m>A</m> above, we would say that it has eigenvalues 3 and 3.
			</p>

<!-- div attr= class="theorem1"-->
			<p>
				Suppose the <m>n \times n</m> matrix <m>P</m> has <m>n</m> real eigenvalues (not necessarily distinct), <m>\lambda_1</m>, <m>\lambda_2</m>, …, <m>\lambda_n</m>, and there are <m>n</m> linearly independent corresponding eigenvectors <m>\vec{v}_1</m>, <m>\vec{v}_2</m>, …, <m>\vec{v}_n</m>. Then the general solution to <m>{\vec{x}}' = P\vec{x}</m> can be written as 
<me>
\vec{x} = c_1 \vec{v}_1 e^{\lambda_1 t} +
c_2 \vec{v}_2 e^{\lambda_2 t} + \cdots +
c_n \vec{v}_n e^{\lambda_n t} .
</me>

			</p><!--</div attr= class="theorem1">-->

			<p>
				The main difference in the statement here from the theorem in is that we are no longer assuming that we have <m>n</m> <em>distinct</em> eigenvalues. Instead, we need to assume that we end up with <m>n</m> linearly independent eigenvectors, which we get for free if the eigenvalues are all distinct, but we might also have that if we do not have all distinct eigenvalues.
			</p>

			<p>
				The <em>geometric multiplicity</em> of an eigenvalue of algebraic multiplicity <m>n</m> is equal to the number of corresponding linearly independent eigenvectors. The geometric multiplicity is always less than or equal to the algebraic multiplicity. The theorem handles the case when these two multiplicities are equal for all eigenvalues. If for an eigenvalue the geometric multiplicity is equal to the algebraic multiplicity, then we say the eigenvalue is <em>complete</em>.
			</p>

			<p>
				In other words, the hypothesis of the theorem could be stated as saying that if all the eigenvalues of <m>P</m> are complete, then there are <m>n</m> linearly independent eigenvectors and thus we have the given general solution.
			</p>

			<p>
				If the geometric multiplicity of an eigenvalue is 2 or greater, then the set of linearly independent eigenvectors is not unique up to multiples as it was before. For example, for the diagonal matrix <m>A =
\left[ \begin{smallmatrix} 3 &amp; 0 \\ 0 &amp; 3 \end{smallmatrix} \right]</m> we could also pick eigenvectors <m>\left[ \begin{smallmatrix} 1 \\ 1 \end{smallmatrix} \right]</m> and <m>\left[ \begin{smallmatrix} 1 \\ -1 \end{smallmatrix} \right]</m>, or in fact any pair of two linearly independent vectors. The number of linearly independent eigenvectors corresponding to <m>\lambda</m> is the number of free variables we obtain when solving <m>A\vec{v} =
\lambda \vec{v}</m>. We pick specific values for those free variables to obtain eigenvectors. If you pick different values, you may get different eigenvectors.
			</p>


			<subsubsection xml:id="defective-eigenvalues">
				<title>Defective eigenvalues</title>

				<p>
					If an <m>n \times n</m> matrix has less than <m>n</m> linearly independent eigenvectors, it is said to be <em>deficient</em>. Then there is at least one eigenvalue with an algebraic multiplicity that is higher than its geometric multiplicity. We call this eigenvalue <em>defective</em> and the difference between the two multiplicities we call the <em></em>.
				</p>


<example>
<title> </title>
<statement>				<p>
					The matrix 
<me>
\begin{bmatrix}
3 &amp; 1 \\ 0 &amp; 3
\end{bmatrix}
</me>
 has an eigenvalue 3 of algebraic multiplicity 2. Let us try to compute eigenvectors. 
<me>
\begin{bmatrix}
0 &amp; 1 \\ 0 &amp; 0
\end{bmatrix}
\begin{bmatrix}
v_1 \\ v_2
\end{bmatrix}
= \vec{0} .
</me>
 We must have that <m>v_2 = 0</m>. Hence any eigenvector is of the form <m>\left[ \begin{smallmatrix} v_1 \\ 0 \end{smallmatrix} \right]</m>. Any two such vectors are linearly dependent, and hence the geometric multiplicity of the eigenvalue is 1. Therefore, the defect is 1, and we can no longer apply the eigenvalue method directly to a system of ODEs with such a coefficient matrix.
				</p>

				<p>
					Roughly, the key observation is that if <m>\lambda</m> is an eigenvalue of <m>A</m> of algebraic multiplicity <m>m</m>, then we can find certain <m>m</m> linearly independent vectors solving <m>{(A-\lambda I)}^k \vec{v} = \vec{0}</m> for various powers <m>k</m>. We will call these <em></em>.
				</p>

				<p>
					Let us continue with the example <m>A = \left[ \begin{smallmatrix}
3 &amp; 1 \\ 0 &amp; 3
\end{smallmatrix} \right]</m> and the equation <m>{\vec{x}}' = A\vec{x}</m>. We found an eigenvalue <m>\lambda=3</m> of (algebraic) multiplicity 2 and defect 1. We found one eigenvector <m>\vec{v} = \left[ \begin{smallmatrix} 1 \\ 0 \end{smallmatrix} \right]</m>. We have one solution 
<me>
\vec{x}_1 = \vec{v} e^{3t} = \begin{bmatrix} 1 \\ 0 \end{bmatrix} e^{3t} .
</me>
 We are now stuck, we get no other solutions from standard eigenvectors. But we need two linearly independent solutions to find the general solution of the equation.
				</p>

				<p>
					Let us try (in the spirit of repeated roots of the characteristic equation for a single equation) another solution of the form 
<me>
\vec{x}_2 = \vec{v}_1 te^{3t},
</me>
 since our modified guess for repeated roots from second order equations was <m>te^{3t}</m>. If we plug this guess into the equation, we get that 
<me>
\vec{x}_2' = \vec{v}_1 e^{3t} + 3\vec{v}_1te^{3t}
</me>
 and since the right-hand side of the equation is <m>A\vec{v}_1te^{3t}</m>, we need <m>v_1</m> to satisfy 
<me>
\vec{v}_1 e^{3t} + 3\vec{v}_1te^{3t} = A\vec{v}_1te^{3t}.
</me>
 Since there is no <m>e^{3t}</m> term on the right-hand side of the equation, we are forced to pick <m>\vec{v}_1 = \vec{0}</m>, and so we get the solution <m>\vec{x}_2 = \vec{0}</m>, which is not good. This guess did not work.
				</p>

				<p>
					The issue here is that we didn’t have enough flexibility to actually get another solution to the differential equation, so we need something a little more complicated to make it work. To this end, we take a new guess of the form 
<me>
\vec{x}_2 = ( \vec{v}_2 +  \vec{v}_1 t )\, e^{3t} .
</me>
 We differentiate to get 
<me>
{\vec{x}_2}' =
\vec{v}_1 e^{3t} +
3 ( \vec{v}_2 +  \vec{v}_1 t )\, e^{3t}
=
( 3 \vec{v}_2 + \vec{v}_1 )\, e^{3t} +  3 \vec{v}_1 t e^{3t} .
</me>
 As we are assuming that <m>\vec{x}_2</m> is a solution, <m>{\vec{x}_2}'</m> must equal <m>A \vec{x}_2</m>. So let’s compute <m>A \vec{x}_2</m>: 
<me>
A \vec{x}_2 = 
A ( \vec{v}_2 +  \vec{v}_1 t )\, e^{3t}
=
A \vec{v}_2 e^{3t} +  A \vec{v}_1 t e^{3t} .
</me>
 By looking at the coefficients of <m>e^{3t}</m> and <m>t e^{3t}</m> we see <m>3 \vec{v}_2 + \vec{v}_1 = A \vec{v}_2</m> and <m>3 \vec{v}_1 = A \vec{v}_1</m>. This means that 
<me>
(A-3I)\vec{v}_2 = \vec{v}_1,
\qquad \text{and} \qquad
(A-3I)\vec{v}_1 = \vec{0}.
</me>
 Therefore, <m>\vec{x}_2</m> is a solution if these two equations are satisfied. The second equation is satisfied if <m>\vec{v}_1</m> is an eigenvector, and we found the eigenvector above, so let <m>\vec{v}_1 = 
\left[ \begin{smallmatrix} 1 \\ 0 \end{smallmatrix} \right]</m>. So, if we can find a <m>\vec{v}_2</m> that solves <m>(A-3I)\vec{v}_2 = \vec{v}_1</m>, then we are done. This is just a bunch of linear equations to solve and we are by now very good at that. Let us solve <m>(A-3I)\vec{v}_2 = \vec{v}_1</m>. Write 
<me>
\begin{bmatrix}
0 &amp; 1 \\ 0 &amp; 0
\end{bmatrix}
\begin{bmatrix}
a \\ b
\end{bmatrix}
=
\begin{bmatrix}
1 \\ 0
\end{bmatrix} .
</me>
 By inspection we see that letting <m>a=0</m> (<m>a</m> could be anything in fact) and <m>b=1</m> does the job. Hence we can take <m>\vec{v}_2 = 
\left[ \begin{smallmatrix} 0 \\ 1 \end{smallmatrix} \right]</m>. Our general solution to <m>{\vec{x}}' = A\vec{x}</m> is 
<me>
\vec{x} =
c_1 
\begin{bmatrix}
1 \\ 0
\end{bmatrix}
e^{3t}
+
c_2
\left(
\begin{bmatrix}
0 \\ 1
\end{bmatrix}
+
\begin{bmatrix}
1 \\ 0
\end{bmatrix}
t
\right)
\,
e^{3t}
=
\begin{bmatrix}
c_1 e^{3t}+c_2 te^{3t} \\
c_2 e^{3t}
\end{bmatrix} .
</me>
 Let us check that we really do have the solution. First <m>x_1' = 
c_1 3 e^{3t}+c_2 e^{3t} + 3 c_2 te^{3t} = 3 x_1 + x_2</m>. Good. Now <m>x_2' = 3 c_2 e^{3t} = 3x_2</m>. Good.
				</p>
</statement>
				<p>
					In the example, if we plug <m>(A-3I)\vec{v}_2 = \vec{v}_1</m> into <m>(A-3I)\vec{v}_1 = \vec{0}</m> we find 
<me>
(A-3I)(A-3I) \vec{v}_2 = \vec{0},
\qquad \text{or} \qquad
{(A-3I)}^2\vec{v}_2 = \vec{0}.
</me>
 Furthermore, if <m>(A-3I) \vec{w} \not= \vec{0}</m>, then <m>(A-3I) \vec{w}</m> is an eigenvector, a multiple of <m>\vec{v}_1</m>. In this <m>2 \times 2</m> case <m>{(A-3I)}^2</m> is just the zero matrix (exercise). So any vector <m>\vec{w}</m> solves <m>{(A-3I)}^2\vec{w} = \vec{0}</m> and we just need a <m>\vec{w}</m> such that <m>(A-3I)\vec{w} \not= \vec{0}</m>. Then we could use <m>\vec{w}</m> for <m>\vec{v}_2</m>, and <m>(A-3I)\vec{w}</m> for <m>\vec{v}_1</m>.
				</p>

				<p>
					Note that the system <m>{\vec{x}}' = A \vec{x}</m> has a simpler solution since <m>A</m> is a so-called <em></em>, that is every entry below the diagonal is zero. In particular, the equation for <m>x_2</m> does not depend on <m>x_1</m>. Mind you, not every defective matrix is triangular.
				</p>

</example><exercise>
<statement>
				<p>
					Solve <m>{\vec{x}}' = \left[ \begin{smallmatrix}
3 &amp; 1 \\ 0 &amp; 3
\end{smallmatrix} \right] \vec{x}</m> by first solving for <m>x_2</m> and then for <m>x_1</m> independently. Check that you got the same solution as we did above.
				</p>
</statement>

<answer>				<p>
					Let us describe the general algorithm. Suppose that <m>\lambda</m> is an eigenvalue of multiplicity 2, defect 1. First find an eigenvector <m>\vec{v}_1</m> of <m>\lambda</m>. That is, <m>\vec{v}_1</m> solves <m>(A-\lambda I)\vec{v}_1  = \vec{0}</m>. Then, find a vector <m>\vec{v}_2</m> such that 
<me>
%{(A-\lambda I)}^2\vec{v}_2 &amp; = \vec{0} , \\
(A-\lambda I)\vec{v}_2 = \vec{v}_1 .
</me>
 This gives us two linearly independent solutions <me>\begin{align*}
\vec{x}_1 &amp; = \vec{v}_1 e^{\lambda t} , \\
\vec{x}_2 &amp; = \left( \vec{v}_2 + \vec{v}_1 t \right) e^{\lambda t},
\end{align*}</me> and so our general solution to the differential equation is 
<me>
\vec{x}(t) = c_1\vec{v}_1e^{\lambda t} + c_2\left(\vec{v}_2 + \vec{v}_1 t \right) e^{\lambda t}.
</me>

				</p>
</answer>
</exercise>


<example>
<title> </title>
<statement>				<p>
					Solve the initial value problem 
<me>
\vec{x}' = \begin{bmatrix} -2 &amp; 3 \\ -3 &amp; 4 \end{bmatrix}\vec{x} \qquad \vec{x}(0) = \begin{bmatrix} 2 \\ -1 \end{bmatrix}.
</me>

				</p>
</statement>

<solution>				<p>
					First, we need to look for the eigenvalues of the coefficient matrix. These are found by 
<me>
\det(A - \lambda I) = (-2-\lambda)(4-\lambda) - (3)(-3) = \lambda^2 - 2\lambda + 1 = 0.
</me>
 Since this polynomial is <m>(\lambda - 1)^2</m>, this has a double root at <m>\lambda = 1</m>.
				</p>

				<p>
					For <m>\lambda = 1</m>, we can hunt for the eigenvector as solutions to 
<me>
(A - I)\vec{v} = \begin{bmatrix} -3 &amp; 3 \\ -3 &amp; 3 \end{bmatrix} \vec{v} = 0.
</me>
 These two equations are redundant, and the first equation is <m>-3v_1 + 3v_2 = 0</m>, which can be solved by <m>v_1 = v_2 = 1</m>. Therefore, and an eigenvector for <m>\lambda = 1</m> is <m>\left[ \begin{smallmatrix} 1 \\ 1 \end{smallmatrix} \right]</m>. Thus, we have a solution to this system of the form 
<me>
\vec{x}_1(t) = \begin{bmatrix} 1 \\ 1 \end{bmatrix} e^t.
</me>

				</p>

				<p>
					Since we only found one eigenvector, we need to look for a generalized eigenvector as well. To do this, we want to solve the equation 
<me>
(A - I)\vec{w} = \vec{v}
</me>
 for the eigenvector <m>\vec{v}</m> that we found previously. This means we need to solve 
<me>
\begin{bmatrix} -3 &amp; 3 \\ -3 &amp; 3 \end{bmatrix} \vec{w} = \begin{bmatrix} 1 \\ 1 \end{bmatrix}
</me>
 and both rows of the vector equation result in the equation <m>-3w_1 + 3w_2 = 1</m> for <m>\vec{w}</m>. We can pick <em>any</em> value of <m>w_1</m> and <m>w_2</m> to make this work. For the sake of this example, we will pick <m>w_1 = 0</m> and <m>w_2 = \nicefrac{1}{3}</m>. Then, we have that our second linearly independent solution to the differential equation is 
<me>
\vec{x}_2(t) = \left(\begin{bmatrix} 0 \\ 1/3 \end{bmatrix} + \begin{bmatrix} 1 \\ 1 \end{bmatrix} t \right) e^{t}
</me>
 and so the general solution to this system is 
<me>
\vec{x}(t) = c_1 \begin{bmatrix} 1 \\ 1 \end{bmatrix} e^t + c_2 \left(\begin{bmatrix} 0 \\ 1/3 \end{bmatrix} + \begin{bmatrix} 1 \\ 1 \end{bmatrix} t \right) e^{t}.
</me>

				</p>

				<p>
					Finally, we can solve the initial value problem. Plugging in <m>t=0</m> gives 
<me>
\vec{x}(0) = c_1 \begin{bmatrix} 1 \\ 1 \end{bmatrix} + c_2 \begin{bmatrix} 0 \\ 1/3 \end{bmatrix} = \begin{bmatrix} 2 \\ -1 \end{bmatrix},
</me>
 which gives that <m>c_1 = 2</m> and then <m>2 + \nicefrac{1}{3}c_2 = -1</m>, or <m>c_2 = -9</m>. Therefore, the solution to the initial value problem is 
<me>
\vec{x}(t) = 2 \begin{bmatrix} 1 \\ 1 \end{bmatrix} e^t - 9 \left(\begin{bmatrix} 0 \\ 1/3 \end{bmatrix} + \begin{bmatrix} 1 \\ 1 \end{bmatrix} t \right) e^{t}.
</me>

				</p>
</solution>
</example>
<exercise>
<statement>
				<p>
					We could have also chosen <m>w_1 = -\nicefrac{1}{3}</m> and <m>w_2 = 0</m> for the vector <m>\vec{w}</m>. Use this to get a different looking general solution. Then solve the same initial value problem to see that you end up with the same answer at the end of the process.
				</p>
</statement>

</exercise>

<example>
<title> </title>
<statement>				<p>
					Consider the system 
<me>
\vec{x}' =
\begin{bmatrix}
2 &amp; -5 &amp; 0 \\
0 &amp; 2 &amp; 0 \\
-1 &amp; 4 &amp; 1
\end{bmatrix}
\vec{x} .
</me>
 Find the general solution to this system using eigenvalues and eigenvectors.
				</p>
</statement>

<solution>				<p>
					Even though this is a three-component system, the process is exactly the same: find the eigenvalues, compute corresponding eigenvectors, then build them together into a general solution. Compute the eigenvalues, 
<me>
0 =
\det(A-\lambda I) = 
\det\left(
\begin{bmatrix}
2-\lambda &amp; -5 &amp; 0 \\
0 &amp; 2-\lambda &amp; 0 \\
-1 &amp; 4 &amp; 1-\lambda
\end{bmatrix}
\right)
= (2-\lambda)^2(1-\lambda) .
</me>
 The eigenvalues are 1 and 2, where 2 has multiplicity 2. We leave it to the reader to find that <m>\left[ \begin{smallmatrix} 0 \\ 0 \\ 1 \end{smallmatrix} \right]</m> is an eigenvector for the eigenvalue <m>\lambda = 1</m>.
				</p>

				<p>
					Let’s focus on <m>\lambda = 2</m>. We compute eigenvectors: 
<me>
\vec{0} =
(A - 2 I) \vec{v}
=
\begin{bmatrix}
0 &amp; -5 &amp; 0 \\
0 &amp; 0 &amp; 0 \\
-1 &amp; 4 &amp; -1
\end{bmatrix}
\begin{bmatrix}
v_1 \\ v_2 \\ v_3
\end{bmatrix}
.
</me>
 The first equation says that <m>v_2 = 0</m>, so the last equation is <m>-v_1 -v_3 = 0</m>. Let <m>v_3</m> be the free variable to find that <m>v_1 = -v_3</m>. Perhaps let <m>v_3 = -1</m> to find an eigenvector <m>\left[ \begin{smallmatrix} 1 \\ 0 \\ -1 \end{smallmatrix} \right]</m>. Problem is that setting <m>v_3</m> to anything else just gets multiples of this vector and so we have a defect of 1. Let <m>\vec{v}_1</m> be the eigenvector and let’s look for a generalized eigenvector <m>\vec{v}_2</m>: 
<me>
(A - 2 I) \vec{v}_2 = \vec{v}_1 , 
</me>
 or 
<me>
\begin{bmatrix}
0 &amp; -5 &amp; 0 \\
0 &amp; 0 &amp; 0 \\
-1 &amp; 4 &amp; -1
\end{bmatrix}
\begin{bmatrix}
a \\ b \\ c
\end{bmatrix}
=
\begin{bmatrix}
1 \\ 0 \\ -1
\end{bmatrix} ,
</me>
 where we used <m>a</m>, <m>b</m>, <m>c</m> as components of <m>\vec{v}_2</m> for simplicity. The first equation says <m>-5b = 1</m> so <m>b = \nicefrac{-1}{5}</m>. The second equation says nothing. The last equation is <m>-a + 4b - c = -1</m>, or <m>a + \nicefrac{4}{5} + c = 1</m>, or <m>a + c = \nicefrac{1}{5}</m>. We let <m>c</m> be the free variable and we choose <m>c=0</m>. We find <m>\vec{v}_2 = \left[ \begin{smallmatrix} \nicefrac{1}{5} \\ \nicefrac{-1}{5}
\\ 0 \end{smallmatrix} \right]</m>.
				</p>

				<p>
					The general solution is therefore, 
<me>
\vec{x} =
c_1
\begin{bmatrix} 0 \\ 0 \\ 1 \end{bmatrix}
e^t
+
c_2 
\begin{bmatrix} 1 \\ 0 \\ -1 \end{bmatrix}
e^{2t}
+
c_3
\left(
\begin{bmatrix} \nicefrac{1}{5} \\ \nicefrac{-1}{5} \\ 0 \end{bmatrix}
+
\begin{bmatrix} 1 \\ 0 \\ -1 \end{bmatrix}
t
\right)
e^{2t}
.
</me>

				</p>
</solution>
</example>
				<p>
					This machinery can also be generalized to higher multiplicities and higher defects. We will not go over this method in detail, but let us just sketch the ideas. Suppose that <m>A</m> has an eigenvalue <m>\lambda</m> of multiplicity <m>m</m>. We find vectors such that 
<me>
{(A - \lambda I)}^k \vec{v}_k = \vec{0},
\qquad \text{but} \qquad
{(A - \lambda I)}^{k-1} \vec{v}_k \not= \vec{0}.
</me>
 Such vectors are called <em></em> (then <m>\vec{v}_1 = {(A - \lambda I)}^{k-1} \vec{v}_k</m> is an eigenvector). For the eigenvector <m>\vec{v}_1</m> there is a chain of generalized eigenvectors <m>\vec{v}_2</m> through <m>\vec{v}_k</m> such that: <me>\begin{align*}
(A - \lambda I) \vec{v}_1 &amp; = \vec{0} , \\
(A - \lambda I) \vec{v}_2 &amp; = \vec{v}_1 , \\
&amp; ~~\vdots \\
(A - \lambda I) \vec{v}_k &amp; = \vec{v}_{k-1} .
\end{align*}</me> Really once you find the <m>\vec{v}_k</m> such that <m>{(A - \lambda I)}^k \vec{v}_k = \vec{0}</m> but <m>{(A - \lambda I)}^{k-1} \vec{v}_k \not= \vec{0}</m>, you find the entire chain since you can compute the rest, <m>\vec{v}_{k-1} = (A - \lambda I) \vec{v}_k</m>, <m>\vec{v}_{k-2} = (A - \lambda I) \vec{v}_{k-1}</m>, etc. We form the linearly independent solutions <me>\begin{align*}
\vec{x}_1 &amp; = \vec{v}_1 e^{\lambda t} , \\
\vec{x}_2 &amp; = ( \vec{v}_2 + \vec{v}_1 t ) \, e^{\lambda t} , \\
&amp; ~~\vdots \\
\vec{x}_k &amp; = \left( \vec{v}_k + \vec{v}_{k-1} t +
\vec{v}_{k-2} \frac{t^2}{2} +
\cdots + \vec{v}_2 \frac{t^{k-2}}{(k-2)!} + \vec{v}_1 \frac{t^{k-1}}{(k-1)!}
\right) \, e^{\lambda t} .
\end{align*}</me> Recall that <m>k! = 1 \cdot 2 \cdot 3 \cdots (k-1) \cdot k</m> is the factorial. If you have an eigenvalue of geometric multiplicity <m>\ell</m>, you will have to find <m>\ell</m> such chains (some of them might be short: just the single eigenvector equation). We go until we form <m>m</m> linearly independent solutions where <m>m</m> is the algebraic multiplicity. We don’t quite know which specific eigenvectors go with which chain, so start by finding <m>\vec{v}_k</m> first for the longest possible chain and go from there.
				</p>

				<p>
					For example, if <m>\lambda</m> is an eigenvalue of <m>A</m> of algebraic multiplicity 3 and defect 2, then solve 
<me>
(A - \lambda I) \vec{v}_1 = \vec{0} , \qquad
(A - \lambda I) \vec{v}_2 = \vec{v}_1 , \qquad
(A - \lambda I) \vec{v}_3 = \vec{v}_2 .
</me>
 That is, find <m>\vec{v}_3</m> such that <m>{(A - \lambda I)}^3 \vec{v}_3 = \vec{0}</m>, but <m>{(A - \lambda I)}^2 \vec{v}_3 \not= \vec{0}</m>. Then you are done as <m>\vec{v}_2 = (A - \lambda I) \vec{v}_3</m> and <m>\vec{v}_1 = (A - \lambda I) \vec{v}_2</m>. The 3 linearly independent solutions are 
<me>
\vec{x}_1 = \vec{v}_1 e^{\lambda t} , \qquad
\vec{x}_2 = ( \vec{v}_2 + \vec{v}_1 t ) \, e^{\lambda t} , \qquad
\vec{x}_3 = \left( \vec{v}_3 + \vec{v}_2 t +
\vec{v}_{1} \frac{t^2}{2} \right) \, e^{\lambda t} .
</me>

				</p>

				<p>
					If on the other hand <m>A</m> has an eigenvalue <m>\lambda</m> of algebraic multiplicity 3 and defect 1, then solve 
<me>
(A - \lambda I) \vec{v}_1 = \vec{0} , \qquad
(A - \lambda I) \vec{v}_2 = \vec{0} , \qquad
(A - \lambda I) \vec{v}_3 = \vec{v}_2 .
</me>
 Here <m>\vec{v}_1</m> and <m>\vec{v}_2</m> are actual honest eigenvectors, and <m>\vec{v}_3</m> is a generalized eigenvector. So there are two chains. To solve, first find a <m>\vec{v}_3</m> such that <m>{(A - \lambda I)}^2 \vec{v}_3 = \vec{0}</m>, but <m>(A - \lambda I) \vec{v}_3 \not= \vec{0}</m>. Then <m>\vec{v}_2 = (A - \lambda I) \vec{v}_3</m> is going to be an eigenvector. Then solve for an eigenvector <m>\vec{v}_1</m> that is linearly independent from <m>\vec{v}_2</m>. You get 3 linearly independent solutions 
<me>
\vec{x}_1 = \vec{v}_1 e^{\lambda t} , \qquad
\vec{x}_2 = \vec{v}_2 e^{\lambda t} , \qquad
\vec{x}_3 = ( \vec{v}_3 + \vec{v}_2 t ) \, e^{\lambda t} .
</me>

				</p>

			</subsubsection>
		</subsubsection>

		<subsection xml:id="phase-portraits-repeated">
			<title>Phase Portraits</title>

			<p>
				We also want to look at the phase portraits and direction field diagrams for repeated eigenvalues. There are two different options here, depending on if there are two linearly independent eigenvectors or not.
			</p>

			<p>
				<em>Case 1.</em> If we have a repeated eigenvalue with two linearly independent eigenvectors, this means that our matrix <m>A</m> is of the form <me>A = \begin{bmatrix} \lambda &amp; 0 \\ 0 &amp; \lambda \end{bmatrix}</me> for the repeated eigenvalue <m>\lambda</m>. This means that <m>A \vec{v} = \lambda \vec{v}</m> for all vectors <m>\vec{v}</m>. So, every vector is part of a straight line solution, and so every solution goes either directly towards or directly away from the origin. This gives a <em></em> which can be a sink or a source depending on whether the eigenvalue is positive or negative.
			</p>

<!-- div attr= class="myfig"-->
<!--</div attr= class="myfig">-->

			<p>
				<em>Case 2.</em> If we have a repeated eigenvalue with only one linearly independent eigenvector, then we only have one straight-line solution. For instance, the matrix <me>A = \begin{bmatrix} 4 &amp; -1 \\ 1 &amp; 2 \end{bmatrix}</me> has only one eigenvector of <m>\begin{bmatrix} 1 \\ 1 \end{bmatrix}</m> for eigenvalue <m>3</m>. Like the nodal sources and sinks, the solutions will go to zero and infinity along the straight line solutions. In this case, because there is only one straight line, the phase portrait looks somewhere between a node and a spiral. This gives an <em></em> which can be a source or sink depending on the sign of the eigenvalue.
			</p>

<!-- div attr= class="myfig"-->
<!--</div attr= class="myfig">-->

			<p>
				We also need to think about the direction of rotation for these improper nodes as well, as they somewhat act both like spirals and nodes. There are two ways of analyzing these phase portrait graphs again, and they follow the same tricks as their spiral counterparts.
			</p>

			<p>
				For the first method, we need the general solution. For the solution drawn in , which has differential equation <me>{\vec{x}}' = \begin{bmatrix} 4 &amp; -1 \\ 1 &amp; 2 \end{bmatrix} \vec{x},</me> the general solution is given by <me>\vec{x}(t) = C_1 \begin{bmatrix} 1 \\ 1 \end{bmatrix}e^{3t} + C_2\left( \begin{bmatrix} 1 \\ 1 \end{bmatrix}te^{3t} + \begin{bmatrix} 1 \\ 0 \end{bmatrix}e^{3t}\right).</me> The <m>C_1</m> term gives our straight-line solution, so we need to consider the <m>C_2</m> term. We can rewrite this term as <m>e^{3t}\left(\left[\begin{smallmatrix} 1 \\ 1 \end{smallmatrix}\right]t + \left[\begin{smallmatrix} 1 \\ 0 \end{smallmatrix}\right] \right)</m>. At <m>t=0</m>, this term is at <m>\left[\begin{smallmatrix} 1 \\ 0 \end{smallmatrix}\right]</m>, and as <m>t \rightarrow \infty</m>, the solution ends up in the direction of <m>\left[\begin{smallmatrix} 1 \\ 1 \end{smallmatrix}\right]</m>, so it is in the first quadrant. Therefore, our solution starts at the point <m>(1,0)</m> and ends up in the first quadrant as <m>t \rightarrow \infty</m>. Since this is an improper source, this means the solution is also going away from the origin as <m>t \rightarrow \infty</m>. Therefore, the solution goes off to <m>\infty</m> in the first quadrant (along the straight line solution) as <m>t\rightarrow \infty</m>, and going backwards, it has to flip around to the opposite side of the straight-line solution. This gives the sketch that we see in .
			</p>

			<p>
				The second method again involves the bottom-left entry of the matrix. Again, this is positive, which means that our rotation is in the counter-clockwise direction. It is a little trickier to prove in this case (you need to use the structure of the generalized eigenvector), but it works out in the same way. This means that below the <m>\left[\begin{smallmatrix} 1 \\ 1 \end{smallmatrix}\right]</m> straight-line solution, the solution spirals counter-clockwise as <m>t</m> gets larger, which means that it has to leave the origin along the <m>\left[\begin{smallmatrix} -1 \\ -1 \end{smallmatrix}\right]</m> direction, rotate around counter-clockwise, and head away from the origin along the <m>\left[\begin{smallmatrix} 1 \\ 1 \end{smallmatrix}\right]</m> direction. That’s the only way it can rotate counter-clockwise.
			</p>

			<p>
				In both of these cases, the curve on the other side of the straight-line solution must mirror it. This puts together the entire sketch of the phase portrait for these improper nodes. The other point to be especially careful about is that having an improper nodal source and improper nodal sink look opposite, even if they have the same direction of rotation. The direction always refers to <m>t</m> increasing, not going into or away from the origin.
			</p>

		</subsection>

<exercises>
			<title>Exercises</title>

<exercise>
<statement>
			<p>
				Compute eigenvalues and eigenvectors of <m>\left[ \begin{smallmatrix}
-2 &amp; -1 &amp; -1 \\
3 &amp; 2 &amp; 1 \\
-3 &amp; -1 &amp; 0 \\
\end{smallmatrix} \right]</m>.
			</p>
</statement>

<answer>			<p>
				 <m>\lambda_1 = -2</m>, <m>\vec{v}_1 = \left[\begin{smallmatrix} -1 \\ 1 \\ -1 \end{smallmatrix}\right]</m>. <m>\lambda_2 = 1</m>, two-dimensional space of eigenvectors, options: <m>\left\{ \left[\begin{smallmatrix} -1 \\ 0 \\ 3 \end{smallmatrix}\right],\ \left[\begin{smallmatrix} -1 \\ 3 \\ 0 \end{smallmatrix}\right]\right\}</m> 
			</p>
</answer>
</exercise>

<exercise>
<statement>
			<p>
				Let <m>A = \left[ \begin{smallmatrix} 5 &amp; -3 \\ 3 &amp; -1 \end{smallmatrix} \right]</m>. Find the general solution of <m>{\vec{x}}' = A \vec{x}</m> and sketch the corresponding phase portrait.
			</p>
</statement>

<answer>			<p>
				 <m>\vec{x}(t) = C_1 \left[\begin{smallmatrix}  1\\ 1 \end{smallmatrix}\right]e^{2t} + C_2\left( \left[\begin{smallmatrix} 1\\1 \end{smallmatrix}\right]te^{2t} + \left[\begin{smallmatrix} 1/3 \\ 0 \end{smallmatrix}\right]e^{2t}\right)</m> 
			</p>
</answer>
</exercise>

<exercise>
<statement>
			<p>
				Solve the initial value problem <me>{\vec{x}}' = \begin{bmatrix} -3 &amp; 2 \\ 0 &amp; -3 \end{bmatrix} \vec{x} \qquad \vec{x}(0) = \begin{bmatrix} 2 \\ 1 \end{bmatrix}</me> and sketch the phase portrait for this system.
			</p>
</statement>

<answer>			<p>
				 <m>\vec{x}(t) = \left[\begin{smallmatrix} (2t+2)e^{-3t} \\ (2t+1)e^{-3t} \end{smallmatrix}\right]</m> 
			</p>
</answer>
</exercise>

<exercise>
<statement>
			<p>
				Solve the initial value problem <me>{\vec{x}}' = \begin{bmatrix} -5 &amp; -2 \\ 8 &amp; 3 \end{bmatrix} \vec{x} \qquad \vec{x}(0) = \begin{bmatrix} 1 \\ 1 \end{bmatrix}</me> and sketch the phase portrait for this system.
			</p>
</statement>

<answer>			<p>
				 <m>\vec{x}(t) = \left[\begin{smallmatrix} (-6t+1)e^{-t} \\ (12t+1)e^{-t} \end{smallmatrix}\right]</m> 
			</p>
</answer>
</exercise>

<exercise>
<statement>
			<p>
				Solve the initial value problem <me>{\vec{x}}' = \begin{bmatrix} -3 &amp; -12 \\ 3 &amp; 9 \end{bmatrix}\vec{x} \qquad \vec{x}(0) = \begin{bmatrix} 3 \\ 2  \end{bmatrix}</me> and sketch the phase portrait for this system.
			</p>
</statement>

<answer>			<p>
				 <m>\vec{x}(t) = \left[\begin{smallmatrix} (-42t+3)e^{3t} \\ (21t + 2)e^{3t} \end{smallmatrix}\right]</m> 
			</p>
</answer>
</exercise>

<exercise>
<statement>
			<p>
				Assume <m>A</m> is a <m>3\times 3</m> matrix. The row-reduced echelon forms of <m>A-\lambda I</m> are given for three different values of <m>\lambda</m>: <me>A-3I \rightsquigarrow \begin{pmatrix} 1&amp;0&amp;0\\ 0&amp;1&amp;0\\ 0&amp;0&amp;1 \end{pmatrix} \qquad\qquad 
A-5I \rightsquigarrow \begin{pmatrix} 1&amp;4&amp;0\\ 0&amp;0&amp;1\\ 0&amp;0&amp;0 \end{pmatrix}\qquad\qquad
A-7I \rightsquigarrow \begin{pmatrix} 1&amp;-1&amp;1/2\\ 0&amp;0&amp;0\\ 0&amp;0&amp;0 \end{pmatrix}</me>
			</p>

			<p>
				Find the general solution of the homogeneous system <m>\vec{x}'=A\vec{x}</m>.
			</p>
</statement>

<answer>			<p>
				 <m>\vec{x}(t) = C_1\left[\begin{smallmatrix} 4 \\ -1 \\ 0 \end{smallmatrix}\right]e^{5t} + C_2\left[\begin{smallmatrix} 1 \\ 1 \\ 0 \end{smallmatrix}\right]e^{7t} + C_2\left[\begin{smallmatrix} -1/2 \\ 0 \\ 1 \end{smallmatrix}\right]e^{7t}</m> 
			</p>
</answer>
</exercise>

<exercise>
<statement>
			<p>
				Consider the matrix <m>A=\displaystyle \begin{pmatrix} 
7 &amp; 5 &amp; -6 \\
0 &amp; -3 &amp; 2 \\
0  &amp; -4 &amp; 1 \end{pmatrix}</m>
			</p>

<!-- div attr= class="tasks"-->
			<p>
				Determine the characteristic polynomial of <m>A</m> and give its eigenvalues. How many (linearly independent) straight-line solutions does the system <m>{\vec{x}}'=A\vec{x}</m> have? How do you know, without solving?
			</p><!--</div attr= class="tasks">-->
</statement>

<answer>			<p>
				 a) <m>\lambda = 7,\ -1\pm 2i</m> b) Only 1 
			</p>
</answer>
</exercise>

<exercise>
<statement>
			<p>
				Let <m>A=\begin{bmatrix} 1&amp;5&amp;-18\\ 2&amp;-1&amp;-5 \\ 1&amp;1&amp;-6 \end{bmatrix}</m>.
			</p>

<!-- div attr= class="tasks"-->
			<p>
				Show directly that <m>\vec{v}_1=\begin{bmatrix} 1\\3\\1 \end{bmatrix}</m> is an eigenvector of <m>A</m>. All eigenvalues of <m>A</m> are the same. Find the general solution to <m>{\vec{x}}'=A\vec{x}</m>.
			</p><!--</div attr= class="tasks">-->
</statement>

<answer>			<p>
				 b) <m>\vec{x}(t) = C_1\left[\begin{smallmatrix} 1 \\ 3 \\1 \end{smallmatrix}\right]e^{-2t} + C_2\left(\left[\begin{smallmatrix} 1 \\ 3 \\ 1 \end{smallmatrix}\right] te^{-2t} + \left[\begin{smallmatrix} 2 \\ -1 \\ 0 \end{smallmatrix}\right]e^{-2t}\right) + C_3\left( \left[\begin{smallmatrix} 1 \\ 3 \\1 \end{smallmatrix}\right] \frac{t^2}{2}e^{-2t} + \left[\begin{smallmatrix} 2 \\ -1 \\ 0 \end{smallmatrix}\right]te^{-2t} + \left[\begin{smallmatrix} -1 \\ 1 \\ 0 \end{smallmatrix}\right] e^{-2t}\right)</m> 
			</p>
</answer>
</exercise>

<!-- div attr= class="samepage"-->
<exercise>
<statement>
			<p>
				Let <m>A = \left[ \begin{smallmatrix}
5 &amp; -4 &amp; 4 \\
0 &amp; 3 &amp; 0 \\
-2 &amp; 4 &amp; -1
\end{smallmatrix} \right]</m>.
			</p>

<!-- div attr= class="tasks"-->
			<p>
				What are the eigenvalues? What is/are the defect(s) of the eigenvalue(s)? Find the general solution of <m>{\vec{x}}' = A \vec{x}</m>.
			</p><!--</div attr= class="tasks">--><!--</div attr= class="samepage">-->
</statement>

<answer>			<p>
				 a) 3,1,1 b) No defects c)  <m>\vec{x}(t) = C_1\left[\begin{smallmatrix} 1 \\ 0 \\ -1 \end{smallmatrix}\right]e^t + C_2\left[\begin{smallmatrix} 2 \\ 1 \\ 0 \end{smallmatrix}\right]e^{3t} + C_3\left[\begin{smallmatrix} -2 \\ 0 \\ 1 \end{smallmatrix}\right]e^{3t}</m> 
			</p>
</answer>
</exercise>

<exercise>
<statement>
			<p>
				Let <m>A =
\left[ \begin{smallmatrix}
1 &amp; 1 &amp; 1 \\
1 &amp; 1 &amp; 1 \\
1 &amp; 1 &amp; 1 
\end{smallmatrix}\right]</m>.
			</p>

<!-- div attr= class="tasks"-->
			<p>
				What are the eigenvalues? What is/are the defect(s) of the eigenvalue(s)? Find the general solution of <m>\vec{x}\,' = A\vec{x}</m>.
			</p><!--</div attr= class="tasks">-->
</statement>

</exercise>
<exercise>
<statement>
			<p>
				Let <m>A = \left[ \begin{smallmatrix} 2 &amp; 1 &amp; 0 \\ 0 &amp; 2 &amp; 0 \\ 0 &amp; 0 &amp; 2 \end{smallmatrix} \right]</m>.
			</p>

<!-- div attr= class="tasks"-->
			<p>
				What are the eigenvalues? What is/are the defect(s) of the eigenvalue(s)? Find the general solution of <m>{\vec{x}}' = A \vec{x}</m> in two different ways and verify you get the same answer.
			</p><!--</div attr= class="tasks">-->
</statement>

<answer>			<p>
				 a) 2 b) Defect 1 c) <m>\vec{x}(t) = C_1 \left[\begin{smallmatrix} 1 \\ 0 \\ 0 \end{smallmatrix}\right]e^{2t} + C_2\left[\begin{smallmatrix} 0 \\ 0 \\ 1 \end{smallmatrix}\right]e^{2t} + C_3\left(\left[\begin{smallmatrix} 1 \\ 0 \\ 0 \end{smallmatrix}\right]te^{2t} + \left[\begin{smallmatrix} 0 \\ 1 \\ 0 \end{smallmatrix}\right]e^{2t}\right)</m> 
			</p>
</answer>
</exercise>

<exercise>
<statement>
			<p>
				Let <m>A =
\left[ \begin{smallmatrix}
1 &amp; 3 &amp; 3 \\
1 &amp; 1 &amp; 0 \\
-1 &amp; 1 &amp; 2 \\
\end{smallmatrix}\right]</m>.
			</p>

<!-- div attr= class="tasks"-->
			<p>
				What are the eigenvalues? What is/are the defect(s) of the eigenvalue(s)? Find the general solution of <m>\vec{x}\,' = A\vec{x}</m>.
			</p><!--</div attr= class="tasks">-->
</statement>

</exercise>
<exercise>
<statement>
			<p>
				Let <m>A = \left[ \begin{smallmatrix}
0 &amp; 1 &amp; 2 \\
-1 &amp; -2 &amp; -2 \\
-4 &amp; 4 &amp; 7
\end{smallmatrix} \right]</m>.
			</p>

<!-- div attr= class="tasks"-->
			<p>
				What are the eigenvalues? What is/are the defect(s) of the eigenvalue(s)? Find the general solution of <m>{\vec{x}}' = A \vec{x}</m>.
			</p><!--</div attr= class="tasks">-->
</statement>

<answer>			<p>
				 a) -1, 3b) 3 has defect 1 c) <m>\vec{x}(t) = C_1\left[\begin{smallmatrix} -2 \\ 0 \\ 1 \end{smallmatrix}\right]e^{-t} + C_2\left[\begin{smallmatrix} 1 \\ -1 \\ 2 \end{smallmatrix}\right]e^{3t} + C_3\left(\left[\begin{smallmatrix} 1 \\ -1 \\ 2 \end{smallmatrix}\right]te^{3t} + \left[\begin{smallmatrix} 0 \\ 0 \\ 1/2 \end{smallmatrix}\right]e^{3t}\right)</m> 
			</p>
</answer>
</exercise>

<exercise>
<statement>
			<p>
				Let <m>A =
\left[ \begin{smallmatrix}
2 &amp; 0 &amp; 0 \\
-1 &amp; -1 &amp; 9 \\
0 &amp; -1 &amp; 5
\end{smallmatrix}\right]</m>.
			</p>

<!-- div attr= class="tasks"-->
			<p>
				What are the eigenvalues? What is/are the defect(s) of the eigenvalue(s)? Find the general solution of <m>\vec{x}\,' = A\vec{x}</m>.
			</p><!--</div attr= class="tasks">-->
</statement>

</exercise>
<exercise>
<statement>
			<p>
				Let <m>A = \left[ \begin{smallmatrix}
0 &amp; 4 &amp; -2 \\
-1 &amp; -4 &amp; 1 \\
0 &amp; 0 &amp; -2
\end{smallmatrix} \right]</m>.
			</p>

<!-- div attr= class="tasks"-->
			<p>
				What are the eigenvalues? What is/are the defect(s) of the eigenvalue(s)? Find the general solution of <m>{\vec{x}}' = A \vec{x}</m>.
			</p><!--</div attr= class="tasks">-->
</statement>

<answer>			<p>
				 a) -2 b) Defect 1 c) <m>\vec{x}(t) = C_1 \left[\begin{smallmatrix} 1 \\ 0 \\ 1 \end{smallmatrix}\right]e^{-2t} + C_2\left[\begin{smallmatrix} 0 \\1 \\ 2 \end{smallmatrix}\right]e^{-2t} + C_3\left( \left[\begin{smallmatrix} 2 \\ -1 \\ 0 \end{smallmatrix}\right]te^{-2t} + \left[\begin{smallmatrix} 1 \\ 0 \\ 0 \end{smallmatrix}\right]e^{-2t}\right)</m> 
			</p>
</answer>
</exercise>

<exercise>
<statement>
			<p>
				Let <m>A = \left[ \begin{smallmatrix}
2 &amp; 1 &amp; -1 \\
-1 &amp; 0 &amp; 2 \\
-1 &amp; -2 &amp; 4
\end{smallmatrix} \right]</m>.
			</p>

<!-- div attr= class="tasks"-->
			<p>
				What are the eigenvalues? What is/are the defect(s) of the eigenvalue(s)? Find the general solution of <m>{\vec{x}}' = A \vec{x}</m>.
			</p><!--</div attr= class="tasks">-->
</statement>

<answer>			<p>
				 a) 2 b)  Defect of 2<!-- linebreak -->c)  <m>\vec{x}(t) = C_1 \left[\begin{smallmatrix} 0 \\ 1 \\ 1 \end{smallmatrix}\right]e^{2t} + C_2\left(\left[\begin{smallmatrix} 0 \\1 \\ 1 \end{smallmatrix}\right]te^{2t} + \left[\begin{smallmatrix} -1 \\ 0 \\ 0 \end{smallmatrix}\right]e^{2t}\right) + C_3\left(\left[\begin{smallmatrix} 0 \\1 \\ 1 \end{smallmatrix}\right]\frac{t^2}{2}e^{2t} + \left[\begin{smallmatrix} -1 \\ 0 \\ 0 \end{smallmatrix}\right]te^{2t} + \left[\begin{smallmatrix} -1 \\-1 \\0 \end{smallmatrix}\right]e^{2t}\right)</m> 
			</p>
</answer>
</exercise>

<exercise>
<statement>
			<p>
				Suppose that <m>A</m> is a <m>2 \times 2</m> matrix with a repeated eigenvalue <m>\lambda</m>. Suppose that there are two linearly independent eigenvectors. Show that <m>A = \lambda I</m>.
			</p>
</statement>

<answer>			<p>
				 Hint: This means everything is an eigenvector. Or, this can be set up as a system of equations. 
			</p>
</answer>
</exercise>

<exercise>
<statement>
			<p>
				Let <m>A =
\left[ \begin{smallmatrix}
a &amp; a \\
b &amp; c
\end{smallmatrix}\right]</m>, where <m>a</m>, <m>b</m>, and <m>c</m> are unknowns. Suppose that <m>5</m> is a doubled eigenvalue of defect 1, and suppose that <m>\left[ \begin{smallmatrix}
1 \\ 0
\end{smallmatrix}\right]</m> is a corresponding eigenvector. Find <m>A</m> and show that there is only one such matrix <m>A</m>.
			</p>
</statement>

</exercise>
<exercise>
<statement>
			<p>
				For each system, (i) classify the system according to type as one of sink/source/saddle/center/ spiral source/spiral sink; (ii) solve the systems; (iii) sketch the phase portrait. Both real and complex eigenvalues appear.
			</p>

<!-- div attr= class="multicols"-->
			<p>
				2
			</p>

<!-- div attr= class="tasks"-->
			<p>
				<m>\vec{x}'=\begin{pmatrix} 2&amp;0 \\ 1&amp;1 \end{pmatrix}\vec{x}</m> <m>\vec{x}'=\begin{pmatrix} -2&amp; -2\\ 8&amp; -2\end{pmatrix}\vec{x}</m> <m>\vec{x}'=\begin{pmatrix} 3&amp; 5\\ -5&amp; -3  \end{pmatrix}\vec{x}</m> <m>\vec{x}' = \begin{pmatrix} -1 &amp; -3 \\ 3 &amp; -7 \end{pmatrix}\vec{x}</m> <m>\vec{x}'=\begin{pmatrix} 3&amp; 2\\ 0&amp;-2 \end{pmatrix}\vec{x}</m> <m>\vec{x}'=\begin{pmatrix} 2&amp; 1/2 \\-1 &amp; 1  \end{pmatrix}\vec{x}</m> <m>\vec{x}'=\begin{pmatrix} 3&amp; 3/2\\ 3/2 &amp; -1 \end{pmatrix}\vec{x}</m>
			</p><!--</div attr= class="tasks">--><!--</div attr= class="multicols">-->
</statement>

</exercise>
<exercise>
<statement>
			<p>
				Consider the second order equation given by 
<me>
y'' + 2y' - 8y = 0.
</me>

			</p>

<!-- div attr= class="tasks"-->
			<p>
				Find the general solution of this problem using the methods of . Convert this equation into a first order linear system using the transformation <m>\vec{x} = \left[ \begin{smallmatrix} y \\ y' \end{smallmatrix} \right]</m>. Find the eigenvalues and eigenvalues of the coefficient matrix and use that to find the general solution to the system. Extract the first component of the general solution and compare that to the solution from part (a). How do they relate? Look back through the work. How do the equation used to find the roots in (a) and the eigenvalues in (c) relate to each other?
			</p><!--</div attr= class="tasks">-->
</statement>

<answer>			<p>
				 b) <m>{\vec{x}}' = \left[\begin{smallmatrix} 0 &amp; 1 \\8 &amp; -2 \end{smallmatrix}\right]\vec{x}</m> d) It’s the same! 
			</p>
</answer>
</exercise>

<exercise>
<statement>
			<p>
				Consider the second order equation given by 
<me>
y'' + 4y' + 5y = 0.
</me>

			</p>

<!-- div attr= class="tasks"-->
			<p>
				Find the general solution of this problem using the methods of . Convert this equation into a first order linear system using the transformation <m>\vec{x} = \left[ \begin{smallmatrix} y \\ y' \end{smallmatrix} \right]</m>. Find the eigenvalues and eigenvalues of the coefficient matrix and use that to find the general solution to the system. Extract the first component of the general solution and compare that to the solution from part (a). How do they relate? Look back through the work. How do the equation used to find the roots in (a) and the eigenvalues in (c) relate to each other?
			</p><!--</div attr= class="tasks">-->
</statement>

<answer>			<p>
				 b) <m>{\vec{x}}' = \left[\begin{smallmatrix} 0 &amp; 1 \\-4 &amp; -4 \end{smallmatrix}\right]\vec{x}</m> d) It’s the same, up to potentially needing to rename the constants (<m>D_1 = 2C_1 + C_2</m> and <m>D_2 = 2C_2 - C_1</m>) 
			</p>
</answer>
</exercise>

<exercise>
<statement>
			<p>
				Consider the second order equation given by 
<me>
y'' - 4y' + 4y = 0.
</me>
 for <m>b</m> and <m>c</m> two real numbers.
			</p>

<!-- div attr= class="tasks"-->
			<p>
				Find the general solution of this problem using the methods of . Convert this equation into a first order linear system using the transformation <m>\vec{x} = \left[ \begin{smallmatrix} y \\ y' \end{smallmatrix} \right]</m>. Find the eigenvalues and eigenvalues of the coefficient matrix and use that to find the general solution to the system. Extract the first component of the general solution and compare that to the solution from part (a). How do they relate? Look back through the work. How do the equation used to find the roots in (a) and the eigenvalues in (c) relate to each other?
			</p><!--</div attr= class="tasks">-->
</statement>

<answer>			<p>
				 b) <m>{\vec{x}}' = \left[\begin{smallmatrix} 0 &amp; 1 \\-4 &amp; 4 \end{smallmatrix}\right]\vec{x}</m> d) It’s the same, up to potentially renaming the constants. 
			</p>
</answer>
</exercise>
</exercises>

	</section>



