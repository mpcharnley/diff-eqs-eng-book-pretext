<?xml version="1.0" encoding="UTF-8" ?>
<!-- Generated by Pandoc using pretext.lua -->



	<section xml:id="eigenmethod-section">
		<title>Eigenvalue method</title>
<introduction>
		<p>
			In this section we will learn how to solve linear homogeneous constant coefficient systems of ODEs by the eigenvalue method. Suppose we have such a system 
<me>
{\vec{x}}' = P\vec{x} ,
</me>
 where <m>P</m> is a constant square matrix. We wish to adapt the method for the single constant coefficient equation by trying the function <m>e^{\lambda t}</m>. However, <m>\vec{x}</m> is a vector. So we try <m>\vec{x} = \vec{v} e^{\lambda t}</m>, where <m>\vec{v}</m> is an arbitrary constant vector. We plug this <m>\vec{x}</m> into the equation to get 
<me>
\underbrace{\lambda \vec{v} e^{\lambda t}}_{{\vec{x}}'} =
\underbrace{P\vec{v} e^{\lambda t}}_{P\vec{x}} .
</me>
 We divide by <m>e^{\lambda t}</m> and notice that we are looking for a scalar <m>\lambda</m> and a vector <m>\vec{v}</m> that satisfy the equation 
<me>
\lambda \vec{v} = P\vec{v} .
</me>

		</p>

		<p>
			This means that we are looking for an eigenvalue <m>\lambda</m> with corresponding eigenvector <m>\vec{v}</m> for the matrix <m>P</m>. When we can find these, we will get solutions to the original system of differential equations of the form <me>\vec{x}(t) = \vec{v}e^{\lambda t}.</me> We get the easiest route to solutions when the matrix <m>P</m> has all real eigenvalues and the eigenvalues are all distinct, and can extend to deal with the complications that arise from complex and repeated eigenvalues.
		</p>

		<p>
			Another way to view these types of solutions are as <q>straight-line solutions.</q> A system of differential equations of the form 
<me>
{\vec{x}}' = P\vec{x} ,
</me>
 is an autonomous system of differential equations, because there is no explicit dependence on <m>t</m> on the right-hand side. When we solved autonomous equations in , we started by looking for equilibrium solutions and built up from there. In this particular case, we are looking for vectors <m>\vec{x}</m> so that <m>P\vec{x} = 0</m>. As long as <m>P</m> is invertible, the only vector that satisfies this is <m>\vec{x} = 0</m>. So, that’s not super interesting, and doesn’t really tell us too much about the solution to the problem.
		</p>

<!-- div attr= class="mywrapfig"-->
		<p>
			3.25in
		</p><!--</div attr= class="mywrapfig">-->

		<p>
			The next more involved type of solution we could look for is a straight-line solution. The idea is that this solution will either move directly (in a straight-line) towards or away from the origin. In the first order autonomous equation case, all of our solutions did this; they either moved towards or away from these equilibrium solutions. This may not be the case for systems, but we can try to find them. If a solution is going to move directly towards or away from the origin, then the direction of change for the solution must be parallel to the position vector. In , the vectors that point in the same or opposite direction of <m>\vec{x}</m> will give rise to a straight-line solution, but vectors that do not point in this direction will give solutions that do not follow a straight-line through the origin.
		</p>

		<p>
			This criterion means that we need to have 
<me>
\vec{x}' = \lambda \vec{x}
</me>
 for some constant <m>\lambda</m>. If this is the case, then we have 
<me>
P\vec{x} = \lambda \vec{x}
</me>
 and this is the equation for eigenvalues and eigenvectors of <m>P</m>. We are back to the same type of solution that we found previously.
		</p>
	</introduction>

		<subsection xml:id="the-eigenvalue-method-with-distinct-real-eigenvalues">
			<title>The eigenvalue method with distinct real eigenvalues</title>

			<p>
				OK. We have the system of equations 
<me>
{\vec{x}}' = P\vec{x} .
</me>
 We find the eigenvalues <m>\lambda_1</m>, <m>\lambda_2</m>, …, <m>\lambda_n</m> of the matrix <m>P</m>, and corresponding eigenvectors <m>\vec{v}_1</m>, <m>\vec{v}_2</m>, …, <m>\vec{v}_n</m>. Now we notice that the functions <m>\vec{v}_1 e^{\lambda_1 t}</m>, <m>\vec{v}_2 e^{\lambda_2 t}</m>, …, <m>\vec{v}_n e^{\lambda_n t}</m> are solutions of the homogeneous system of equations and hence <m>\vec{x} = c_1 \vec{v}_1 e^{\lambda_1 t} +
c_2 \vec{v}_2 e^{\lambda_2 t} + \cdots +
c_n \vec{v}_n e^{\lambda_n t}</m> is a solution by superposition.
			</p>

<!-- div attr= class="theorem1"-->
			<p>
				Take <m>{\vec{x}}' = P\vec{x}</m>. If <m>P</m> is an <m>n \times n</m> constant matrix that has <m>n</m> distinct real eigenvalues <m>\lambda_1</m>, <m>\lambda_2</m>, …, <m>\lambda_n</m>, then there exist <m>n</m> linearly independent corresponding eigenvectors <m>\vec{v}_1</m>, <m>\vec{v}_2</m>, …, <m>\vec{v}_n</m>, and the general solution to <m>{\vec{x}}' = P\vec{x}</m> can be written as 
<me>
%\mybxbg{~~
\vec{x} = c_1 \vec{v}_1 e^{\lambda_1 t} +
c_2 \vec{v}_2 e^{\lambda_2 t} + \cdots +
c_n \vec{v}_n e^{\lambda_n t} .
%~~}
</me>

			</p><!--</div attr= class="theorem1">-->

			<p>
				The corresponding fundamental matrix solution is 
<me>
X(t) = \bigl[\, \vec{v}_1 e^{\lambda_1 t} \quad \vec{v}_2 e^{\lambda_2 t}
\quad \cdots \quad \vec{v}_n e^{\lambda_n t} \,\bigr].
</me>
 That is, <m>X(t)</m> is the matrix whose <m>j^{\text{th}}</m> column is <m>\vec{v}_j e^{\lambda_j t}</m>.
			</p>


<example>
<title> </title>
<statement>			<p>
				Consider the system 
<me>
{\vec{x}}'
=
\begin{bmatrix}
2 &amp; 1 &amp; 1 \\
1 &amp; 2 &amp; 0 \\
0 &amp; 0 &amp; 2
\end{bmatrix}
\vec{x} .
</me>
 Find the general solution.
			</p>
</statement>

<solution>			<p>
				Earlier, we found the eigenvalues are <m>1,2,3</m>. We found the eigenvector <m>\left[ \begin{smallmatrix} 1 \\ 1 \\ 0 \end{smallmatrix} \right]</m> for the eigenvalue 3. Similarly we find the eigenvector <m>\left[ \begin{smallmatrix} 1 \\ -1 \\ 0 \end{smallmatrix} \right]</m> for the eigenvalue 1, and <m>\left[ \begin{smallmatrix} 0 \\ 1 \\ -1 \end{smallmatrix} \right]</m> for the eigenvalue 2 (exercise: check). Hence our general solution is 
<me>
\vec{x} =
c_1
\begin{bmatrix}
1 \\ -1 \\ 0
\end{bmatrix}
e^t
+
c_2
\begin{bmatrix}
0 \\ 1 \\ -1
\end{bmatrix}
e^{2t}
+
c_3
\begin{bmatrix}
1 \\ 1 \\ 0
\end{bmatrix}
e^{3t} 
=
\begin{bmatrix}
c_1 e^t+c_3 e^{3t} \\ -c_1 e^t + c_2 e^{2t} + c_3 e^{3t} \\ - c_2 e^{2t}
\end{bmatrix} .
</me>
 In terms of a fundamental matrix solution, 
<me>
\vec{x} = X(t)\, \vec{c}
=
\begin{bmatrix}
e^t &amp; 0 &amp; e^{3t} \\
-e^t &amp; e^{2t} &amp; e^{3t} \\
0 &amp; -e^{2t} &amp; 0
\end{bmatrix}
\begin{bmatrix}
c_1 \\ c_2 \\ c_3
\end{bmatrix} .
</me>

			</p>
</solution>
</example>
<exercise>
<statement>
			<p>
				Check that this <m>\vec{x}</m> really solves the system.
			</p>
</statement>

<answer>			<p>
				Overall, the process for finding the solution for real and distinct eigenvalues is to first find the eigenvalues and eigenvectors of the matrix <m>P</m>. Once we have these, we get <m>n</m> linearly independent solutions of the form <m>\vec{x}_i(t) = \vec{v}_ie^{\lambda_i t}</m>, so that the general solution is of the form <me>\vec{x}(t) = c_1\vec{v}_1e^{\lambda_1 t} + c_2\vec{v}_2e^{\lambda_2 t} + \cdots + c_n\vec{v}_ne^{\lambda_n t}.</me> Then, if we need to solve for an initial condition, we figure out the coefficients <m>c_1</m>, <m>c_2</m>, ..., <m>c_n</m> to satisfy this condition.
			</p>
</answer>
</exercise>

			<p>
				Note: If we write a single homogeneous linear constant coefficient <m>n^{\text{th}}</m> order equation as a first order system (as we did in ), then the eigenvalue equation 
<me>
\det(P - \lambda I) = 0
</me>
 is essentially the same as the characteristic equation we got in and . See the exercises for details about this.
			</p>


<example>
<title> </title>
<statement>			<p>
				Solve the initial value problem 
<me>
\vec{x}' = \begin{bmatrix} 0 &amp; 4 \\ -3 &amp; -7 \end{bmatrix}\vec{x} \qquad \vec{x}(0) = \begin{bmatrix} 1 \\ 1 \end{bmatrix}.
</me>

			</p>
</statement>

<solution>			<p>
				Since we are in the case of a constant-coefficient linear system, we start by looking for the eigenvalues and eigenvectors of the coefficient matrix <m>P</m>. To do this, we compute 
<me>
\det(P - \lambda I) = (0-\lambda)(-7-\lambda) - (4)(-3) = \lambda^2 + 7\lambda + 12.
</me>
 This polynomial factors as <m>(\lambda + 3)(\lambda + 4)</m>, and so the two eigenvalues are <m>\lambda_1 = -3</m> and <m>\lambda_2 = -4</m>.
			</p>

			<p>
				Next, we need to find the corresponding eigenvectors. For <m>\lambda = -3</m>, we get the matrix equation 
<me>
(P + 3I)\vec{v} = \begin{bmatrix} 3 &amp; 4 \\ -3 &amp; -4 \end{bmatrix} \vec{v} = \vec{0}. 
</me>
 The two equations that you get here are redundant, which is <m>3v_1 + 4v_2 = 0</m>. One way to satisfy this is <m>v_1 = 4</m>, <m>v_2 = -3</m>, so that the eigenvector is <m>\left[\begin{smallmatrix} 4 \\ -3 \end{smallmatrix} \right]</m>.
			</p>

			<p>
				For <m>\lambda = -4</m>, the matrix becomes 
<me>
(P + 4I)\vec{v} = \begin{bmatrix} 4 &amp; 4 \\ -3 &amp; -3 \end{bmatrix}\vec{v} = 0
</me>
 so the eigenvector here is <m>\left[ \begin{smallmatrix} 1 \\ -1 \end{smallmatrix} \right]</m>. Therefore, the general solution to this differential equation, by superposition, is 
<me>
\vec{x}(t) = c_1 \begin{bmatrix} 4 \\ -3 \end{bmatrix}e^{-3t} + c_2 \begin{bmatrix} 1 \\ -1 \end{bmatrix}e^{-4t}.
</me>

			</p>

			<p>
				Finally, we have to solve the initial value problem using the initial conditions. If we plug in <m>t=0</m>, we get the equation 
<me>
\vec{x}(0) = c_1 \begin{bmatrix} 4 \\ -3 \end{bmatrix} + c_2 \begin{bmatrix} 1 \\ -1 \end{bmatrix} = \begin{bmatrix} 1 \\ 1 \end{bmatrix}.
</me>
 This results in needing to solve the system of equations 
<me>
4c_1 + c_2 = 1 \qquad -3c_1 - c_2 = 1.
</me>
 These can be solved in any way, including row reduction. We will start by adding the two equations together, which gives <m>c_1 = 2</m>, and then the first equation implies that <m>c_2 = -7</m>. Therefore, the solution to the initial value problem is 
<me>
\vec{x}(t) = 2 \begin{bmatrix} 4 \\ -3 \end{bmatrix}e^{-3t}  - 7 \begin{bmatrix} 1 \\ -1 \end{bmatrix}e^{-4t} = \begin{bmatrix} 8e^{-3t} - 7e^{-4t} \\ -6e^{-3t} + 7e^{-4t} \end{bmatrix}.
</me>

			</p>
</solution>
</example>
		</subsection>

		<subsection xml:id="phase-portraits">
			<title>Phase Portraits</title>

			<p>
				Now that we have these solutions, we want to get an idea for what they look like in the plane. We spent a lot of time in first order equations looking at direction fields, as well as phase lines for autonomous equations. We want to develope the same type of intuition for two-component systems in the plane, because much intuition can be obtained by studying this simple case. Suppose we use coordinates <m>(x,y)</m> for the plane as usual, and suppose <m>P = \left[ \begin{smallmatrix} a &amp; b \\ c &amp; d \end{smallmatrix} \right]</m> is a <m>2 \times 2</m> matrix. Consider the system 
<men xml:id="pln-eq">
 \begin{bmatrix} x \\ y \end{bmatrix} ' =
P \begin{bmatrix} x \\ y \end{bmatrix} 
\qquad 
\text{or}
\qquad
\begin{bmatrix} x \\ y \end{bmatrix} ' =
\begin{bmatrix} a &amp; b \\ c &amp; d \end{bmatrix} 
\begin{bmatrix} x \\ y \end{bmatrix} 
.
</men>
The system is autonomous (compare this section to ) and so we can draw a vector field (see the end of ). We will be able to visually tell what the vector field looks like and how the solutions behave, once we find the eigenvalues and eigenvectors of the matrix <m>P</m>. The goal is to be able to sketch what the different trajectories of the solutions look like for a variety of initial conditions, as well as classify the general type of picture that results depending on the matrix <m>P</m>.
			</p>

			<p>
				<em>Case 1.</em> Suppose that the eigenvalues of <m>P</m> are real and positive. We find two corresponding eigenvectors and plot them in the plane. For example, take the matrix <m>\left[ \begin{smallmatrix} 1 &amp; 1 \\ 0 &amp; 2 \end{smallmatrix}
\right]</m>. The eigenvalues are 1 and 2 and corresponding eigenvectors are <m>\left[ \begin{smallmatrix} 1 \\ 0 \end{smallmatrix} \right]</m> and <m>\left[ \begin{smallmatrix} 1 \\ 1 \end{smallmatrix} \right]</m>. See .
			</p>

<!-- div attr= class="mywrapfig"-->
			<p>
				3.25in
			</p><!--</div attr= class="mywrapfig">-->

			<p>
				Suppose the point <m>(x,y)</m> is on the line determined by an eigenvector <m>\vec{v}</m> for an eigenvalue <m>\lambda</m>. That is, <m>\left[ \begin{smallmatrix} x \\ y \end{smallmatrix} \right] = \alpha \vec{v}</m> for some scalar <m>\alpha</m>. Then 
<me>
\begin{bmatrix} x \\ y \end{bmatrix} '
=
P \begin{bmatrix} x \\ y \end{bmatrix}
=
P ( \alpha \vec{v} ) =  \alpha ( P \vec{v} )
= \alpha \lambda \vec{v} .
</me>
 The derivative is a multiple of <m>\vec{v}</m> and hence points along the line determined by <m>\vec{v}</m>. As <m>\lambda &gt; 0</m>, the derivative points in the direction of <m>\vec{v}</m> when <m>\alpha</m> is positive and in the opposite direction when <m>\alpha</m> is negative. Let us draw the lines determined by the eigenvectors, and let us draw arrows on the lines to indicate the directions. See .
			</p>

			<p>
				We fill in the rest of the arrows for the vector field and we also draw a few solutions. See . The picture looks like a source with arrows coming out from the origin. Hence we call this type of picture a <em></em> or sometimes an <em></em>. Notice the two eigenvectors are drawn on the entire vector field figure with arrows, and the straight-line solutions follow them.
			</p>

<!-- div attr= class="myfig"-->
<!--</div attr= class="myfig">-->

			<p>
				<em>Case 2.</em> Suppose both eigenvalues are negative. For example, take the negation of the matrix in case 1, <m>\left[ \begin{smallmatrix} -1 &amp; -1 \\ 0 &amp; -2 \end{smallmatrix} \right]</m>. The eigenvalues are <m>-1</m> and <m>-2</m> and corresponding eigenvectors are the same, <m>\left[ \begin{smallmatrix} 1 \\ 0 \end{smallmatrix} \right]</m> and <m>\left[ \begin{smallmatrix} 1 \\ 1 \end{smallmatrix} \right]</m>. The calculation and the picture are almost the same. The only difference is that the eigenvalues are negative and hence all arrows are reversed. We get the picture in . We call this kind of picture a <em></em> or a <em></em>.
			</p>

<!-- div attr= class="myfig"-->
<!--</div attr= class="myfig">-->

			<p>
				<em>Case 3.</em> Suppose one eigenvalue is positive and one is negative. For example the matrix <m>\left[ \begin{smallmatrix} 1 &amp; 1 \\ 0 &amp; -2 \end{smallmatrix} \right]</m>. The eigenvalues are 1 and <m>-2</m> and corresponding eigenvectors are <m>\left[ \begin{smallmatrix} 1 \\ 0 \end{smallmatrix} \right]</m> and <m>\left[ \begin{smallmatrix} 1 \\ -3 \end{smallmatrix} \right]</m>. We reverse the arrows on one line (corresponding to the negative eigenvalue) and we obtain the picture in . We call this picture a <em></em>.
			</p>

		</subsection>

<exercises>
			<title>Exercises</title>

<exercise>
<statement>
<!-- div attr= class="tasks"-->
			<p>
				Find the general solution of <m>x_1' = 2 x_1</m>, <m>x_2' = 3 x_2</m> using the eigenvalue method (first write the system in the form <m>{\vec{x}}' = A \vec{x}</m>). Solve the system by solving each equation separately and verify you get the same general solution.
			</p><!--</div attr= class="tasks">-->
</statement>

<answer>			<p>
				 <m>C_1\left[\begin{smallmatrix} 1 \\ 0 \end{smallmatrix}\right]e^{2t} + C_2\left[\begin{smallmatrix} 0 \\ 1  \end{smallmatrix}\right]e^{3t}</m> 
			</p>
</answer>
</exercise>

<exercise>
<statement>
			<p>
				Find the general solution of <m>x_1' = 3 x_1 + x_2</m>, <m>x_2' = 2 x_1 + 4 x_2</m> using the eigenvalue method and sketch the phase portrait for this system of differential equations.
			</p>
</statement>

<answer>			<p>
				 <m>C_1 \left[\begin{smallmatrix} 1 \\ -1 \end{smallmatrix}\right]e^{2t} + C_2\left[\begin{smallmatrix} 1 \\ 2 \end{smallmatrix}\right]e^{-t}</m> 
			</p>
</answer>
</exercise>

<exercise>
<statement>
			<p>
				Solve <m>x_1' = x_2</m>, <m>x_2' = x_1</m> using the eigenvalue method and sketch the phase portrait for this system of differential equations.
			</p>
</statement>

</exercise>
<exercise>
<statement>
			<p>
				Amino acid dating can be used by forensic scientists to determine the time of death in situations where other techniques might not work. These amino acids are sneaky, and they exist in a left-handed form (L) and a right-handed form (D), which are called <em>enantiomers</em>. While you’re alive, your body keeps all your amino acids in the L form. Once you die, your body no longer regulates your amino acids, and every so often they flip a coin and decide whether to switch into the opposite form. This way, when someone finds your body in a dumpster, they can pull out your teeth and measure the <em>racemization ratio</em>, which is the ratio of D-enantiomers to L-enantiomers.
			</p>

			<p>
				Denote by <m>D(t)</m> and <m>L(t)</m>, respectively, the proportions of D- and L-enantiomers found in your teeth, where <m>t</m> is measured in years after death. Since this is Math class, the proportions are governed by a system of differential equations, such as 
<men xml:id="eq-AminoEqn">
\begin{bmatrix} L' \\ D' \end{bmatrix} = \begin{bmatrix} -.02&amp; .02\\ .02 &amp; -.02 \end{bmatrix}\begin{bmatrix} L\\ D \end{bmatrix}.</men>
			</p>

<!-- div attr= class="tasks"-->
			<p>
				Find the general solution to <xref ref="eq-AminoEqn" />. Solve <xref ref="eq-AminoEqn" /> with initial conditions <m>D(0) = 0</m> and <m>L(0) = 1</m>, and express the solution in component form. Describe what happens to the quantities <m>D(t)</m> and <m>L(t)</m> in the long run. Given the above initial conditions, if the racemization ratio in your teeth is currently 1:3, how long ago did you die?
			</p><!--</div attr= class="tasks">-->
</statement>

<answer>			<p>
				 a)  <m>\vec{x}(t) = C_1 \left[\begin{smallmatrix} -1 \\ 1 \end{smallmatrix}\right]e^{-t/25} + C_2\left[\begin{smallmatrix} 1 \\ 1 \end{smallmatrix}\right]</m><!-- linebreak -->b)  <m>L(t) = \frac{1}{2} + \frac{1}{2}e^{-t/25},\ D(t) = \frac{1}{2} - \frac{1}{2}e^{-t/25}</m>. Both go to <m>1/2</m>.<!-- linebreak -->c)  <m>t = 25\ln(2)\approx 17.33</m> years 
			</p>
</answer>
</exercise>

<exercise>
<statement>
<!-- div attr= class="tasks"-->
			<p>
				Compute eigenvalues and eigenvectors of <m>A = \left[ \begin{smallmatrix}
9 &amp; -2 &amp; -6 \\
-8 &amp; 3 &amp; 6 \\
10 &amp; -2 &amp; -6
\end{smallmatrix} \right]</m>. Find the general solution of <m>{\vec{x}}' = A \vec{x}</m>.
			</p><!--</div attr= class="tasks">-->
</statement>

<answer>			<p>
				 a)  <m>\lambda_1 = 1</m>, <m>\vec{v}_1 = \left[\begin{smallmatrix} 1/2 \\ -1 \\ 1 \end{smallmatrix}\right]</m>. <m>\lambda_2 = 2</m>, <m>\vec{v}_2 = \left[\begin{smallmatrix} 2 \\ -2 \\ 3 \end{smallmatrix}\right]</m>. <m>\lambda_3 = 3</m>, <m>\vec{v}_3 = \left[\begin{smallmatrix} 3 \\ -3 \\ 4 \end{smallmatrix}\right]</m><!-- linebreak -->b)  <m>\vec{x}(t) = C_1\left[\begin{smallmatrix} 1/2 \\ -1 \\ 1 \end{smallmatrix}\right]e^t + C_2\left[\begin{smallmatrix} 2 \\ -2 \\ 3 \end{smallmatrix}\right]e^{2t} + C_3\left[\begin{smallmatrix} 3 \\ -3 \\ 4 \end{smallmatrix}\right]e^{3t}</m> 
			</p>
</answer>
</exercise>

<exercise>
<statement>
<!-- div attr= class="tasks"-->
			<p>
				Compute eigenvalues and eigenvectors of <m>A= \left[ \begin{smallmatrix}
1 &amp; 0 &amp; 3 \\
-1 &amp; 0 &amp; 1 \\
2 &amp; 0 &amp; 2
\end{smallmatrix}\right]</m>. Solve the system <m>\vec{x}\,' = A \vec{x}</m>.
			</p><!--</div attr= class="tasks">-->
</statement>

</exercise>
<exercise>
<statement>
			<p>
				Let <m>a,b,c,d,e,f</m> be numbers. Find the eigenvalues of <m>\left[ \begin{smallmatrix}
a &amp; b &amp; c \\
0 &amp; d &amp; e \\
0 &amp; 0 &amp; f \\
\end{smallmatrix} \right]</m>.
			</p>
</statement>

<answer>			<p>
				 a, d, f 
			</p>
</answer>
</exercise>

<exercise>
<statement>
			<p>
				Find the general solution of the system 
<me>
{\vec{x}}' = \begin{bmatrix} -7 &amp; 1 \\ -12 &amp; 0 \end{bmatrix} \vec{x}
</me>
 and sketch the phase portrait for this system.
			</p>
</statement>

</exercise>
<exercise>
<statement>
			<p>
				Find the general solution of the system 
<me>
{\vec{x}}' = \begin{bmatrix} -13 &amp; -12 \\ 9 &amp; 8 \end{bmatrix} \vec{x}
</me>
 and draw a sketch for the phase portrait.
			</p>
</statement>

</exercise>
<exercise>
<statement>
			<p>
				Find the general solution of the system 
<me>
{\vec{x}}' = \begin{bmatrix} -2 &amp; -6 &amp; 0 \\ 4 &amp; 8 &amp; 0 \\ -4 &amp; -7 &amp; 3 \end{bmatrix} \vec{x}.
</me>

			</p>
</statement>

</exercise>
<exercise>
<statement>
			<p>
				Find the general solution of the system 
<me>
{\vec{x}}' = \begin{bmatrix} -6 &amp; 2 &amp; 4 \\ -2 &amp; -1 &amp; 4 \\ -2 &amp; 1 &amp; 0 \end{bmatrix} \vec{x}.
</me>

			</p>
</statement>

</exercise>
<exercise>
<statement>
			<p>
				Solve the initial value problem <me>{\vec{x}}' = \begin{bmatrix} -3 &amp; 0 \\ 3 &amp; -4 \end{bmatrix} \vec{x} \qquad \vec{x}(0) = \begin{bmatrix} -1 \\ 2 \end{bmatrix}.</me>
			</p>
</statement>

<answer>			<p>
				 <m>\vec{x}(t) = -\left[\begin{smallmatrix} 1 \\ 3 \end{smallmatrix}\right]e^{-3t} + 5\left[\begin{smallmatrix} 0 \\ 1 \end{smallmatrix}\right]e^{-4t}</m> 
			</p>
</answer>
</exercise>

<exercise>
<statement>
			<p>
				Solve the initial value problem <me>{\vec{x}}' = \begin{bmatrix} 1 &amp; -3 \\ 2 &amp; 6 \end{bmatrix} \vec{x} \qquad \vec{x}(0) = \begin{bmatrix} 1 \\ 1 \end{bmatrix}.</me>
			</p>
</statement>

<answer>			<p>
				 <m>\vec{x}(t) = -5\left[\begin{smallmatrix} 1 \\ -1 \end{smallmatrix}\right]e^{4t} + 2\left[\begin{smallmatrix} 3 \\ -2 \end{smallmatrix}\right]e^{3t}</m> 
			</p>
</answer>
</exercise>

<exercise>
<statement>
			<p>
				Solve the initial value problem <me>{\vec{x}}' = \begin{bmatrix} 7 &amp; 4 &amp; 0 \\ -8 &amp; -5 &amp; 0 \\ 17 &amp; 7 &amp; -2 \end{bmatrix} \vec{x} \qquad \vec{x}(0) = \begin{bmatrix} -3 \\ 2 \\ 2 \end{bmatrix}.</me>
			</p>
</statement>

<answer>			<p>
				 <m>\vec{x}(t) = \left[\begin{smallmatrix} 1 \\ -2 \\ 3 \end{smallmatrix}\right]e^{-t} + 7\left[\begin{smallmatrix} 0 \\ 0 \\ 1 \end{smallmatrix}\right]e^{-2t} - 4\left[\begin{smallmatrix} 1 \\ -1 \\ 2 \end{smallmatrix}\right]e^{3t}</m> 
			</p>
</answer>
</exercise>
</exercises>

	</section>



