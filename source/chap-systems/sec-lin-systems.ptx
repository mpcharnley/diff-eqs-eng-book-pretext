<?xml version="1.0" encoding="UTF-8" ?>
<!-- Generated by Pandoc using pretext.lua -->



	<section xml:id="linsystems-section">
		<title>Linear systems of ODEs</title>

		<p>
			In order to get into the details of how to talk about and deal with linear systems of differential equations, we first need to talk about matrix- or vector-valued functions. Such a function is just a matrix or vector whose entries depend on some variable. If <m>t</m> is the independent variable, we write a <em></em> <m>\vec{x}(t)</m> as 
<me>
\vec{x}(t) = \begin{bmatrix}
x_1(t) \\
x_2(t) \\
\vdots \\
x_n(t)
\end{bmatrix} .
</me>
 Similarly a <em></em> <m>A(t)</m> is 
<me>
A(t) =
\begin{bmatrix}
a_{11}(t) &amp; a_{12}(t) &amp; \cdots &amp; a_{1n}(t) \\
a_{21}(t) &amp; a_{22}(t) &amp; \cdots &amp; a_{2n}(t) \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
a_{n1}(t) &amp; a_{n2}(t) &amp; \cdots &amp; a_{nn}(t)
\end{bmatrix} .
</me>
 As long as the addition of vectors is defined, we can add vector-valued functions, and as long as the addition and multiplication of matrices are defined (like if they are the right size) we can multiply matrix-valued functions. In addition, the derivative <m>A'(t)</m> or <m>\frac{dA}{dt}</m> is just the matrix-valued function whose <m>ij^{\text{th}}</m> entry is <m>a_{ij}'(t)</m>. We used this idea previously when talking about how to write first order systems from higher order equations in .
		</p>

		<p>
			Rules of differentiation of matrix-valued functions are similar to rules for normal functions. Let <m>A(t)</m> and <m>B(t)</m> be matrix-valued functions. Let <m>c</m> a scalar and let <m>C</m> be a constant matrix. Then <me>\begin{align*}
\bigl(A(t)+B(t)\bigr)' &amp; = A'(t) + B'(t), \\
\bigl(A(t)B(t)\bigr)' &amp; = A'(t)B(t) + A(t)B'(t), \\
\bigl(cA(t)\bigr)' &amp; = cA'(t), \\
\bigl(CA(t)\bigr)' &amp; = CA'(t), \\
\bigl(A(t)\,C\bigr)' &amp; = A'(t)\,C .
\end{align*}</me> Note the order of the multiplication in the last two expressions because matrix multiplication is not commutative.
		</p>

		<p>
			A <em></em> is a system that can be written as the vector equation 
<me>
{\vec{x}}'(t) = P(t)\vec{x}(t) + \vec{f}(t),
</me>
 where <m>P(t)</m> is a matrix-valued function, and <m>\vec{x}(t)</m> and <m>\vec{f}(t)</m> are vector-valued functions. We will often suppress the dependence on <m>t</m> and only write <m>{\vec{x}}' = P\vec{x} + \vec{f}</m>. A solution of the system is a vector-valued function <m>\vec{x}</m> satisfying the vector equation.
		</p>

		<p>
			For example, the equations <me>\begin{align*}
x_1' &amp;= 2t x_1 + e^t x_2 + t^2 , \\
x_2' &amp;= \frac{x_1}{t} -x_2 + e^t ,
\end{align*}</me> can be written as 
<me>
{\vec{x}}' = 
\begin{bmatrix}
2t &amp; e^t \\
\nicefrac{1}{t} &amp; -1
\end{bmatrix}
\vec{x}
+
\begin{bmatrix}
t^2 \\
e^t
\end{bmatrix} .
</me>

		</p>

		<p>
			We will mostly concentrate on equations that are not just linear, but are in fact <em></em> equations. That is, the matrix <m>P</m> will be constant; it will not depend on <m>t</m>.
		</p>

		<p>
			When <m>\vec{f} = \vec{0}</m> (the zero vector), then we say the system is <em>homogeneous</em>. For homogeneous linear systems we have the principle of superposition, just like for single homogeneous equations.
		</p>

<!-- div attr= class="theorem1"-->
		<p>
			Superposition Let <m>{\vec{x}}' = P\vec{x}</m> be a linear homogeneous system of ODEs. Suppose that <m>\vec{x}_1,\vec{x}_2,\ldots,\vec{x}_n</m> are <m>n</m> solutions of the equation and <m>c_1,c_2,\ldots,c_n</m> are any constants, then 
<men xml:id="syshom-eq1">
 \vec{x} = c_1 \vec{x}_1 + c_2 \vec{x}_2 + \cdots + c_n \vec{x}_n ,
</men>
is also a solution. Furthermore, if this is a system of <m>n</m> equations (<m>P</m> is <m>n\times n</m>), and <m>\vec{x}_1,\vec{x}_2,\ldots,\vec{x}_n</m> are linearly independent, then every solution <m>\vec{x}</m> can be written as <xref ref="syshom-eq1" />.
		</p><!--</div attr= class="theorem1">-->

		<p>
			Linear independence for vector-valued functions is the same idea as for normal functions. The vector-valued functions <m>\vec{x}_1,\vec{x}_2,\ldots,\vec{x}_n</m> are linearly independent if the only way to satisfy the equation 
<me>
c_1 \vec{x}_1 + c_2 \vec{x}_2 + \cdots + c_n \vec{x}_n  = \vec{0}
</me>
 is by choosing the parameters <m>c_1 = c_2 = \cdots = c_n = 0</m>, where the equation must hold for all <m>t</m>.
		</p>


<example>
<title> </title>
<statement>		<p>
			Determine if the sets <m>S_1 = \Bigl\{ \vec{x}_1 = \Bigl[ \begin{smallmatrix} t^2 \\ t \end{smallmatrix} \Bigr],\ \vec{x}_2 = \Bigl[ \begin{smallmatrix} 0 \\ 1+t \end{smallmatrix} \Bigr],\ \vec{x}_3 = \Bigl[ \begin{smallmatrix} -t^2 \\ 1 \end{smallmatrix} \Bigr] \Bigr\}</m> and <m>S_2 = \Bigl\{ \vec{x}_1 = \Bigl[ \begin{smallmatrix} t^2 \\ t \end{smallmatrix} \Bigr],\ \vec{x}_2 = \Bigl[ \begin{smallmatrix} 0 \\ t \end{smallmatrix} \Bigr],\ \vec{x}_3 = \Bigl[ \begin{smallmatrix} -t^2 \\ 1 \end{smallmatrix}
\Bigr]\Bigr\}</m> are linearly independent.
		</p>
</statement>

<solution>		<p>
			The vector functions in <m>S_1</m> are linearly dependent because <m>\vec{x}_1 + \vec{x}_3 = \vec{x}_2</m>, and this holds for all <m>t</m>. So <m>c_1 =
1</m>, <m>c_2 = -1</m>, and <m>c_3 = 1</m> above will work.
		</p>

		<p>
			On the other hand, the vector functions in <m>S_2</m> are linearly independent, even though this is only a slight change from <m>S_1</m>. First write <m>c_1 \vec{x}_1 + c_2 \vec{x}_2 + c_3 \vec{x}_3  = \vec{0}</m> and note that it has to hold for all <m>t</m>. We get that 
<me>
c_1 \vec{x}_1 + c_2 \vec{x}_2 + c_3 \vec{x}_3
=
\begin{bmatrix}
c_1 t^2 - c_3 t^2
\\
c_1 t + c_2 t + c_3 
\end{bmatrix}
=
\begin{bmatrix}
0
\\
0
\end{bmatrix} .
</me>
 In other words <m>c_1 t^2 - c_3 t^2 = 0</m> and <m>c_1 t + c_2 t + c_3 = 0</m>. If we set <m>t = 0</m>, then the second equation becomes <m>c_3 = 0</m>. But then the first equation becomes <m>c_1 t^2 = 0</m> for all <m>t</m> and so <m>c_1 = 0</m>. Thus the second equation is just <m>c_2 t = 0</m>, which means <m>c_2 = 0</m>. So <m>c_1 = c_2 = c_3 = 0</m> is the only solution and <m>\vec{x}_1</m>, <m>\vec{x}_2</m>, and <m>\vec{x}_3</m> are linearly independent.
		</p>
</solution>
</example>
		<p>
			The linear combination <m>c_1 \vec{x}_1 + c_2 \vec{x}_2 + \cdots + c_n
\vec{x}_n</m> could always be also as 
<me>
X(t)\,\vec{c} ,
</me>
 where <m>X(t)</m> is the matrix with columns <m>\vec{x}_1, \vec{x}_2, \ldots, \vec{x}_n</m>, and <m>\vec{c}</m> is the column vector with entries <m>c_1, c_2, \ldots, c_n</m>. This is similar to the way that we could write linear combinations of vectors by putting them into a matrix, including how we talked about rank in . Assuming that <m>\vec{x}_1,\vec{x}_2,\ldots,\vec{x}_n</m> are linearly independent and solutions to a given system of differential equations, the matrix-valued function <m>X(t)</m> is called a <em></em>, or a <em></em>.
		</p>

		<p>
			To solve nonhomogeneous first order linear systems, we use the same technique as we applied to solve single linear nonhomogeneous equations.
		</p>

<!-- div attr= class="theorem1"-->
		<p>
			Let <m>{\vec{x}}' = P\vec{x} + \vec{f}</m> be a linear system of ODEs. Suppose <m>\vec{x}_p</m> is one particular solution. Then every solution can be written as 
<me>
\vec{x} = \vec{x}_c + \vec{x}_p ,
</me>
 where <m>\vec{x}_c</m> is a solution to the (<m>{\vec{x}}' = P\vec{x}</m>).
		</p><!--</div attr= class="theorem1">-->

		<p>
			The procedure for systems is the same as for single equations. We find a particular solution to the nonhomogeneous equation, then we find the general solution to the associated homogeneous equation, and finally we add the two together.
		</p>

		<p>
			Alright, suppose you have found the general solution of <m>{\vec{x}}' = P\vec{x} + \vec{f}</m>. Next suppose you are given an initial condition of the form 
<me>
\vec{x}(t_0) = \vec{b}
</me>
 for some fixed <m>t_0</m> and a constant vector <m>\vec{b}</m>. Let <m>X(t)</m> be a fundamental matrix solution of the associated homogeneous equation (i.e.Â columns of <m>X(t)</m> are solutions). The general solution can be written as 
<me>
\vec{x}(t) = X(t)\,\vec{c} + \vec{x}_p(t).
</me>
 We are seeking a vector <m>\vec{c}</m> such that 
<me>
\vec{b} = \vec{x}(t_0) = X(t_0)\,\vec{c} + \vec{x}_p(t_0).
</me>
 In other words, we are solving for <m>\vec{c}</m> in the nonhomogeneous system of linear equations 
<me>
X(t_0)\,\vec{c} = \vec{b} - \vec{x}_p(t_0) .
</me>

		</p>


<example>
<title> </title>
<statement>		<p>
			In we solved the system <me>\begin{align*}
x_1' &amp; = x_1 , \\
x_2' &amp; = x_1 - x_2 ,
\end{align*}</me> with initial conditions <m>x_1(0) = 1</m>, <m>x_2(0) = 2</m>. Let us consider this problem in the language of this section.
		</p>

		<p>
			The system is homogeneous, so <m>\vec{f}(t) = \vec{0}</m>. We write the system and the initial conditions as 
<me>
{\vec{x}}'
=
\begin{bmatrix}
1 &amp; 0 \\
1 &amp; -1
\end{bmatrix}
\vec{x} ,
\qquad
\vec{x}(0) = 
\begin{bmatrix}
1 \\
2
\end{bmatrix} .
</me>

		</p>

		<p>
			We found the general solution is <m>x_1 = c_1 e^t</m> and <m>x_2 = \frac{c_1}{2}e^{t} + c_2e^{-t}</m>. Letting <m>c_1=1</m> and <m>c_2=0</m>, we obtain the solution <m>\left[ \begin{smallmatrix} e^t \\ (1/2) e^t \end{smallmatrix} \right]</m>. Letting <m>c_1=0</m> and <m>c_2=1</m>, we obtain <m>\left[ \begin{smallmatrix} 0 \\ e^{-t} \end{smallmatrix} \right]</m>. These two solutions are linearly independent, as can be seen by setting <m>t=0</m>, and noting that the resulting constant vectors are linearly independent. In matrix notation, a fundamental matrix solution is, therefore, 
<me>
X(t) = 
\begin{bmatrix}
e^t &amp; 0 \\
\frac{1}{2} e^t &amp; e^{-t}
\end{bmatrix} .
</me>

		</p>

		<p>
			To solve the initial value problem we solve for <m>\vec{c}</m> in the equation 
<me>
X(0)\,\vec{c} = \vec{b} ,
</me>
 or in other words, 
<me>
\begin{bmatrix}
1 &amp; 0 \\
\frac{1}{2} &amp; 1
\end{bmatrix} 
\vec{c} = 
\begin{bmatrix}
1 \\ 2
\end{bmatrix} .
</me>
 A single elementary row operation shows <m>\vec{c} =
\left[ \begin{smallmatrix} 1 \\ 3/2 \end{smallmatrix} \right]</m>. Our solution is 
<me>
\vec{x}(t) = 
X(t)\,\vec{c} = 
\begin{bmatrix}
e^t &amp; 0 \\
\frac{1}{2} e^t &amp; e^{-t}
\end{bmatrix}
\begin{bmatrix}
1 \\ \frac{3}{2}
\end{bmatrix} =
\begin{bmatrix}
e^t \\
\frac{1}{2} e^t + \frac{3}{2} e^{-t}
\end{bmatrix} .
</me>
 This new solution agrees with our previous solution from .
		</p>
</statement>
</example>

<exercises>
			<title>Exercises</title>

<exercise>
<statement>
			<p>
				Write the system <m>x_1' = 2 x_1 - 3t x_2 + \sin t</m>, <m>x_2' = e^t x_1 + 3 x_2 + \cos t</m> in the form <m>{\vec{x}}' = P(t) \vec{x} + \vec{f}(t)</m>.
			</p>
</statement>

<answer>			<p>
				 <m>P(t) = \left[\begin{smallmatrix} 2 &amp; -3t \\ e^t &amp; 3 \end{smallmatrix}\right]</m>, <m>\vec{f}(t) = \left[\begin{smallmatrix}  \sin(t)\\ \cos(t) \end{smallmatrix}\right]</m> 
			</p>
</answer>
</exercise>

<exercise>
<statement>
			<p>
				Write <m>x'=3x-y+e^t</m>, <m>y'=tx</m> in matrix notation.
			</p>
</statement>

</exercise>
<exercise>
<statement>
			<p>
				Consider the third order differential equation 
<me>
y''' + (y'+1)^2 = e^y + \sin(t+1).
</me>
 Convert this to a first order system and simplify as much as possible. Can you write this in the form <m>\vec{x}' = A\vec{x} + \vec{f}</m>? Why or why not?
			</p>
</statement>

<answer>			<p>
				 No, it is non-linear. <m>\vec{u}' = \left[\begin{smallmatrix}  u_2 \\ u_3 \\ e^{u_1} - (u_2 + 1)^2 + \sin(t+1) \end{smallmatrix}\right]</m> 
			</p>
</answer>
</exercise>

<exercise>
<statement>
<!-- div attr= class="tasks"-->
			<p>
				Verify that the system <m>{\vec{x}}' =
\left[ \begin{smallmatrix}
1 &amp; 3 \\ 3 &amp; 1
\end{smallmatrix} \right] \vec{x}</m> has the two solutions <m>\left[ \begin{smallmatrix}
1 \\ 1
\end{smallmatrix} \right] e^{4t}</m> and <m>\left[ \begin{smallmatrix}
1 \\ -1
\end{smallmatrix} \right] e^{-2t}</m>. Write down the general solution. Write down the general solution in the form <m>x_1 = ?</m>, <m>x_2 = ?</m> (i.e.Â write down a formula for each element of the solution).
			</p><!--</div attr= class="tasks">-->
</statement>

<answer>			<p>
				 b)Â  <m>C_1\left[\begin{smallmatrix} 1 \\ 1 \end{smallmatrix}\right]e^{4t} + C_2\left[\begin{smallmatrix} 1 \\ -1 \end{smallmatrix}\right]e^{-2t}</m><!-- linebreak -->c)Â <m>x_1(t) = C_1e^{4t} + C_2e^{-2t},\ x_2(t) = C_1e^{4t} - C_2e^{-2t}</m> 
			</p>
</answer>
</exercise>

<exercise>
<statement>
			<p>
				Verify that <m>\left[ \begin{smallmatrix}
1 \\ 1
\end{smallmatrix} \right] e^{t}</m> and <m>\left[ \begin{smallmatrix}
1 \\ -1
\end{smallmatrix} \right] e^{t}</m> are linearly independent. Hint: Just plug in <m>t=0</m>.
			</p>
</statement>

<answer>			<p>
				 Yes 
			</p>
</answer>
</exercise>

<exercise>
<statement>
			<p>
				Are <m>\left[ \begin{smallmatrix}
e^{2t} \\ e^t
\end{smallmatrix}\right]</m> and <m>\left[ \begin{smallmatrix}
e^{t} \\ e^{2t}
\end{smallmatrix}\right]</m> linearly independent? Justify.
			</p>
</statement>

</exercise>
<exercise>
<statement>
			<p>
				Verify that <m>\left[ \begin{smallmatrix}
1 \\ 1 \\ 0
\end{smallmatrix} \right] e^{t}</m> and <m>\left[ \begin{smallmatrix}
1 \\ -1 \\ 1
\end{smallmatrix} \right] e^{t}</m> and <m>\left[ \begin{smallmatrix}
1 \\ -1 \\ 1
\end{smallmatrix} \right] e^{2t}</m> are linearly independent. Hint: You must be a bit more tricky than in the previous exercises.
			</p>
</statement>

<answer>			<p>
				 Yes. Write this out as three equations that all must equal zero, see what combining the first two gets you, then go from there. 
			</p>
</answer>
</exercise>

<exercise>
<statement>
			<p>
				Are <m>\left[ \begin{smallmatrix}
\cosh(t) \\ 1
\end{smallmatrix}\right]</m>, <m>\left[ \begin{smallmatrix}
e^{t} \\ 1
\end{smallmatrix}\right]</m>, and <m>\left[ \begin{smallmatrix}
e^{-t} \\ 1
\end{smallmatrix}\right]</m> linearly independent? Justify.
			</p>
</statement>

</exercise>
<exercise>
<statement>
			<p>
				Verify that <m>\left[ \begin{smallmatrix}
t \\ t^2
\end{smallmatrix} \right]</m> and <m>\left[ \begin{smallmatrix}
t^3 \\ t^4
\end{smallmatrix} \right]</m> are linearly independent.
			</p>
</statement>

<answer>			<p>
				 Yes, write out the equations and see when they can be zero. 
			</p>
</answer>
</exercise>

<exercise>
<statement>
			<p>
				Take the system <m>x_1' + x_2' = x_1</m>, <m>x_1' - x_2' = x_2</m>.
			</p>

<!-- div attr= class="tasks"-->
			<p>
				Write it in the form <m>A {\vec{x}}' = B \vec{x}</m> for matrices <m>A</m> and <m>B</m>. Compute <m>A^{-1}</m> and use that to write the system in the form <m>{\vec{x}}' = P \vec{x}</m>.
			</p><!--</div attr= class="tasks">-->
</statement>

<answer>			<p>
				 <m>A = \left[\begin{smallmatrix}  1 &amp; 1 \\ 1 &amp; -1 \end{smallmatrix}\right]</m>, <m>B = \left[\begin{smallmatrix} 1 &amp; 0 \\ 0 &amp; 1 \end{smallmatrix}\right]</m>, <m>P = \left[\begin{smallmatrix} 1/2 &amp; 1/2 \\ 1/2 &amp; -1/2 \end{smallmatrix}\right]</m> 
			</p>
</answer>
</exercise>

<exercise>
<statement>
<!-- div attr= class="tasks"-->
			<p>
				Write <m>x_1'=2tx_2</m>, <m>x_2'=2tx_2</m> in matrix notation. Solve and write the solution in matrix notation.
			</p><!--</div attr= class="tasks">-->
</statement>
</exercise></exercises>

	</section>



