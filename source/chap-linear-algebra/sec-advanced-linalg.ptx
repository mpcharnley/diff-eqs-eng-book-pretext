<?xml version="1.0" encoding="UTF-8" ?>
<!-- Generated by Pandoc using pretext.lua -->



	<section xml:id="sec-kernel">
		<title>Related Topics in Linear Algebra</title>


		<subsection xml:id="subspaces-and-span">
			<title>Subspaces and span</title>

			<p>
				Assume that we find two vectors that solve <m>A\vec{x} = 0</m>. What other vectors also solve this equation? In our discussion of linear combinations, we saw that if <m>\vec{x}_1</m> and <m>\vec{x}_2</m> solve <m>A\vec{x} = 0</m>, then so does <m>A(\alpha_1\vec{x}_1 + \alpha_2\vec{x}_2)</m> for any constants <m>\alpha_1</m> and <m>\alpha_2</m>. Thus, all linear combinations will also solve the equation. This leads to the definition of the span of a set of vectors.
			</p>

<!-- div attr= class="definition"-->
			<p>
				The set of all linear combinations of a set of vectors is called their <em></em>. 
<me>
\operatorname{span} \bigl\{ \vec{x}_1, \vec{x}_2 , \ldots , \vec{x}_n \bigr\}
=
\bigl\{
\text{Set of all linear combinations of
$\vec{x}_1, \vec{x}_2 , \ldots , \vec{x}_n$}
\bigr\} .
</me>

			</p><!--</div attr= class="definition">-->

			<p>
				Thus, if two vectors solve a homogeneous equation, so does everything in the span of those two vectors. The span of a collection of vectors is an example of a subspace, which is a common object in linear algebra. We say that a set <m>S</m> of vectors in <m>{\mathbb R}^n</m> is a <em></em> if whenever <m>\vec{x}</m> and <m>\vec{y}</m> are members of <m>S</m> and <m>\alpha</m> is a scalar, then 
<me>
\vec{x} + \vec{y}, \qquad \text{and} \qquad \alpha \vec{x}
</me>
 are also members of <m>S</m>. That is, we can add and multiply by scalars and we still land in <m>S</m>. So every linear combination of vectors of <m>S</m> is still in <m>S</m>. That is really what a subspace is. It is a subset where we can take linear combinations and still end up being in the subset.
			</p>


<example>
<title> </title>
<statement>			<p>
				 If we let <m>S = {\mathbb R}^n</m>, then this <m>S</m> is a subspace of <m>{\mathbb R}^n</m>. Adding any two vectors in <m>{\mathbb R}^n</m> gets a vector in <m>{\mathbb R}^n</m>, and so does multiplying by scalars.
			</p>

			<p>
				The set <m>S' = \{ \vec{0} \}</m>, that is, the set of the zero vector by itself, is also a subspace of <m>{\mathbb R}^n</m>. There is only one vector in this subspace, so we only need to check for that one vector, and everything checks out: <m>\vec{0}+\vec{0} = \vec{0}</m> and <m>\alpha \vec{0} = \vec{0}</m>.
			</p>

			<p>
				The set <m>S''</m> of all the vectors of the form <m>(a,a)</m> for any real number <m>a</m>, such as <m>(1,1)</m>, <m>(3,3)</m>, or <m>(-0.5,-0.5)</m> is a subspace of <m>{\mathbb R}^2</m>. Adding two such vectors, say <m>(1,1)+(3,3) = (4,4)</m> again gets a vector of the same form, and so does multiplying by a scalar, say <m>8(1,1) = (8,8)</m>.
			</p>
</statement>
</example>
			<p>
				We can apply these ideas to the vectors that live inside a matrix. The span of the rows of a matrix <m>A</m> is called the <em></em>. The row space of <m>A</m> and the row space of the row echelon form of <m>A</m> are the same, because reducing the matrix <m>A</m> to its row echelon form involves taking linear combinations, which will preserve the span. In the example, 
<me>
\begin{split}
\text{row space of }
\begin{bmatrix}
1 &amp; 2 &amp; 3 \\
4 &amp; 5 &amp; 6 \\
7 &amp; 8 &amp; 9
\end{bmatrix}
&amp; =
\operatorname{span}
\left\{
\begin{bmatrix}
1 &amp; 2 &amp; 3
\end{bmatrix}
,
\begin{bmatrix}
4 &amp; 5 &amp; 6
\end{bmatrix}
,
\begin{bmatrix}
7 &amp; 8 &amp; 9
\end{bmatrix}
\right\}
\\
&amp; =
\operatorname{span}
\left\{
\begin{bmatrix}
1 &amp; 2 &amp; 3
\end{bmatrix}
,
\begin{bmatrix}
0 &amp; 1 &amp; 2
\end{bmatrix}
\right\} .
\end{split}
</me>

			</p>

			<p>
				Similarly to row space, the span of columns is called the <em></em>. 
<me>
\text{column space of }
\begin{bmatrix}
1 &amp; 2 &amp; 3 \\
4 &amp; 5 &amp; 6 \\
7 &amp; 8 &amp; 9
\end{bmatrix}
=
\operatorname{span}
\left\{
\begin{bmatrix}
1 \\ 4 \\ 7
\end{bmatrix}
,
\begin{bmatrix}
2 \\ 5 \\ 8
\end{bmatrix}
,
\begin{bmatrix}
3 \\ 6 \\ 9
\end{bmatrix}
\right\} .
</me>

			</p>

			<p>
				In particular, to find a set of linearly independent columns we need to look at where the pivots were. If you recall above, when solving <m>A \vec{x}
= \vec{0}</m> the key was finding the pivots, any non-pivot columns corresponded to free variables. That means we can solve for the non-pivot columns in terms of the pivot columns. Let’s see an example.
			</p>


<example>
<title> </title>
<statement>			<p>
				Find the linearly independent columns of the matrix 
<me>
\begin{bmatrix}
1 &amp; 2 &amp; 3 &amp; 4 \\
2 &amp; 4 &amp; 5 &amp; 6 \\
3 &amp; 6 &amp; 7 &amp; 8
\end{bmatrix} .
</me>

			</p>
</statement>

<solution>			<p>
				We find a pivot and reduce the rows below: 
<me>
\begin{bmatrix}
\mybxsm{1} &amp; 2 &amp; 3 &amp; 4 \\
2 &amp; 4 &amp; 5 &amp; 6 \\
3 &amp; 6 &amp; 7 &amp; 8
\end{bmatrix} 
\to
\begin{bmatrix}
\mybxsm{1} &amp; 2 &amp; 3 &amp; 4 \\
0 &amp; 0 &amp; -1 &amp; -2 \\
3 &amp; 6 &amp; 7 &amp; 8
\end{bmatrix} 
\to
\begin{bmatrix}
\mybxsm{1} &amp; 2 &amp; 3 &amp; 4 \\
0 &amp; 0 &amp; -1 &amp; -2 \\
0 &amp; 0 &amp; -2 &amp; -4
\end{bmatrix} .
</me>
 We find the next pivot, make it one, and rinse and repeat: 
<me>
\begin{bmatrix}
\mybxsm{1} &amp; 2 &amp; 3 &amp; 4 \\
0 &amp; 0 &amp; \mybxsm{-1} &amp; -2 \\
0 &amp; 0 &amp; -2 &amp; -4
\end{bmatrix} 
\to
\begin{bmatrix}
\mybxsm{1} &amp; 2 &amp; 3 &amp; 4 \\
0 &amp; 0 &amp; \mybxsm{1} &amp; 2 \\
0 &amp; 0 &amp; -2 &amp; -4
\end{bmatrix} 
\to
\begin{bmatrix}
\mybxsm{1} &amp; 2 &amp; 3 &amp; 4 \\
0 &amp; 0 &amp; \mybxsm{1} &amp; 2 \\
0 &amp; 0 &amp; 0 &amp; 0
\end{bmatrix} . 
</me>
 The final matrix is the row echelon form of the matrix. Consider the pivots that we marked. The pivot columns are the first and the third column. All other columns correspond to free variables when solving <m>A \vec{x} = \vec{0}</m>, so all other columns can be solved in terms of the first and the third column. In other words 
<me>
\text{column space of }
\begin{bmatrix}
1 &amp; 2 &amp; 3 &amp; 4 \\
2 &amp; 4 &amp; 5 &amp; 6 \\
3 &amp; 6 &amp; 7 &amp; 8
\end{bmatrix}
=
\operatorname{span}
\left\{
\begin{bmatrix}
1 \\
2 \\
3 
\end{bmatrix}
,
\begin{bmatrix}
2 \\
4 \\
6
\end{bmatrix}
,
\begin{bmatrix}
3 \\
5 \\
7
\end{bmatrix}
,
\begin{bmatrix}
4 \\
6 \\
8
\end{bmatrix}
\right\}
=
\operatorname{span}
\left\{
\begin{bmatrix}
1 \\
2 \\
3
\end{bmatrix}
,
\begin{bmatrix}
3 \\
5 \\
7
\end{bmatrix}
\right\} .
</me>

			</p>
</solution>
</example>
			<p>
				We could perhaps use another pair of columns to get the same span, but the first and the third are guaranteed to work because they are pivot columns.
			</p>

			<p>
				In the previous example, this means that only the first and third columns are <q>important</q> in the sense of generating the full column space as a span. We would like to have a way to talk about what these first and third columns do.
			</p>

<!-- div attr= class="definition"-->
			<p>
				Let <m>S</m> be a subspace of a vector space. The set <m>\{\vec{v}_1, \vec{v}_2, ..., \vec{v}_k\}</m> is a <em></em> for the subspace <m>S</m> if each of these vectors are in <m>S</m> and the span of <m>\{\vec{v}_1, \vec{v}_2, ..., \vec{v}_k\}</m> is equal to <m>S</m>.
			</p><!--</div attr= class="definition">-->

			<p>
				In the context of the previous example, for the matrix 
<me>
A = \begin{bmatrix}
1 &amp; 2 &amp; 3 &amp; 4 \\
2 &amp; 4 &amp; 5 &amp; 6 \\
3 &amp; 6 &amp; 7 &amp; 8
\end{bmatrix} 
</me>
 we know that 
<me>
\text{column space of }
\begin{bmatrix}
1 &amp; 2 &amp; 3 &amp; 4 \\
2 &amp; 4 &amp; 5 &amp; 6 \\
3 &amp; 6 &amp; 7 &amp; 8
\end{bmatrix}
=
\operatorname{span}
\left\{
\begin{bmatrix}
1 \\
2 \\
3 
\end{bmatrix}
,
\begin{bmatrix}
2 \\
4 \\
6
\end{bmatrix}
,
\begin{bmatrix}
3 \\
5 \\
7
\end{bmatrix}
,
\begin{bmatrix}
4 \\
6 \\
8
\end{bmatrix}
\right\}
=
\operatorname{span}
\left\{
\begin{bmatrix}
1 \\
2 \\
3
\end{bmatrix}
,
\begin{bmatrix}
3 \\
5 \\
7
\end{bmatrix}
\right\} .
</me>
 This means that both 
<me>
\left\{
\begin{bmatrix}
1 \\
2 \\
3 
\end{bmatrix}
,
\begin{bmatrix}
2 \\
4 \\
6
\end{bmatrix}
,
\begin{bmatrix}
3 \\
5 \\
7
\end{bmatrix}
,
\begin{bmatrix}
4 \\
6 \\
8
\end{bmatrix}
\right\}
\quad \text{ and } \quad 
\left\{
\begin{bmatrix}
1 \\
2 \\
3
\end{bmatrix}
,
\begin{bmatrix}
3 \\
5 \\
7
\end{bmatrix}
\right\} 
</me>
 are spanning sets for this column space.
			</p>

			<p>
				The idea also works in reverse. Suppose we have a bunch of column vectors and we just need to find a linearly independent set. For example, suppose we started with the vectors 
<me>
\vec{v}_1 =
\begin{bmatrix}
1 \\
2 \\
3 
\end{bmatrix}
,
\quad
\vec{v}_2 =
\begin{bmatrix}
2 \\
4 \\
6
\end{bmatrix}
,
\quad
\vec{v}_3 =
\begin{bmatrix}
3 \\
5 \\
7
\end{bmatrix}
,
\quad
\vec{v}_4 =
\begin{bmatrix}
4 \\
6 \\
8
\end{bmatrix} .
</me>
 These vectors are not linearly independent as we saw above. In particular, the span <m>\vec{v}_1</m> and <m>\vec{v}_3</m> is the same as the span of all four of the vectors. So <m>\vec{v}_2</m> and <m>\vec{v}_4</m> can both be written as linear combinations of <m>\vec{v}_1</m> and <m>\vec{v}_3</m>. A common thing that comes up in practice is that one gets a set of vectors whose span is the set of solutions of some problem. But perhaps we get way too many vectors, we want to simplify. For example above, all vectors in the span of <m>\vec{v}_1, \vec{v}_2, \vec{v}_3, \vec{v}_4</m> can be written <m>\alpha_1 \vec{v}_1 + \alpha_2 \vec{v}_2 + \alpha_3 \vec{v}_3 + \alpha_4
\vec{v}_4</m> for some numbers <m>\alpha_1,\alpha_2,\alpha_3,\alpha_4</m>. But it is also true that every such vector can be written as <m>a \vec{v}_1 + b \vec{v}_3</m> for two numbers <m>a</m> and <m>b</m>. And one has to admit, that looks much simpler. Moreover, these numbers <m>a</m> and <m>b</m> are unique. More on that later in this section.
			</p>

			<p>
				To find this linearly independent set we simply take our vectors and form the matrix <m>[ \vec{v}_1 ~ \vec{v}_2 ~ \vec{v}_3 ~ \vec{v}_4 ]</m>, that is, the matrix 
<me>
\begin{bmatrix}
1 &amp; 2 &amp; 3 &amp; 4 \\
2 &amp; 4 &amp; 5 &amp; 6 \\
3 &amp; 6 &amp; 7 &amp; 8
\end{bmatrix} .
</me>
 We crank up the row-reduction machine, feed this matrix into it, and find the pivot columns and pick those. In this case, <m>\vec{v}_1</m> and <m>\vec{v}_3</m>.
			</p>

		</subsection>

		<subsection xml:id="basis-and-dimension">
			<title>Basis and dimension</title>

			<p>
				At this point, we have talked about subspaces, and two other properties of sets of vectors: linear independence and being a spanning set for a subspace. In some sense, these two properties are in opposition to each other. If I add more vectors to a set, I am more likely to become a spanning set (because I have more options for adding to get other vectors), but less likely to be independent (because there are more possibilities for a linear combination to be zero). Similarly, the reverse is true; removing vectors means the set is more likely to be linearly independent, but less likely to span a given subspace. The question then becomes if there is a sweet spot where both things are true, and that leads to the definition of a basis.
			</p>

<!-- div attr= class="definition"-->
			<p>
				 If <m>S</m> is a subspace and we can find <m>k</m> linearly independent vectors in <m>S</m> 
<me>
\vec{v}_1, \vec{v}_2, \ldots, \vec{v}_k ,
</me>
 such that every other vector in <m>S</m> is a linear combination of <m>\vec{v}_1,
\vec{v}_2,\ldots, \vec{v}_k</m>, then the set <m>\{ \vec{v}_1, \vec{v}_2, \ldots, \vec{v}_k \}</m> is called a <em></em> of <m>S</m>. In other words, <m>S</m> is the span of <m>\{ \vec{v}_1, \vec{v}_2, \ldots, \vec{v}_k \}</m>. We say that <m>S</m> has <em></em> <m>k</m>, and we write 
<me>
\dim S = k .
</me>

			</p><!--</div attr= class="definition">-->

			<p>
				The next theorem illustrates the main properties and classification of a basis of a vector space.
			</p>

<!-- div attr= class="theorem1"-->
			<p>
				If <m>S \subset {\mathbb R}^n</m> is a subspace and <m>S</m> is not the trivial subspace <m>\{ \vec{0} \}</m>, then there exists a unique positive integer <m>k</m> (the dimension) and a (not unique) basis <m>\{ \vec{v}_1, \vec{v}_2, \ldots, \vec{v}_k \}</m>, such that every <m>\vec{w}</m> in <m>S</m> can be uniquely represented by 
<me>
\vec{w} = 
\alpha_1 \vec{v}_1 + 
\alpha_2 \vec{v}_2 + 
\cdots
+
\alpha_k \vec{v}_k ,
</me>
 for some scalars <m>\alpha_1</m>, <m>\alpha_2</m>, …, <m>\alpha_k</m>.
			</p><!--</div attr= class="theorem1">-->

			<p>
				We should reiterate that while <m>k</m> is unique (a subspace cannot have two different dimensions), the set of basis vectors is not at all unique. There are lots of different bases for any given subspace. Finding just the right basis for a subspace is a large part of what one does in linear algebra. In fact, that is what we spend a lot of time on in linear differential equations, although at first glance it may not seem like that is what we are doing.
			</p>


<example>
<title> </title>
<statement>			<p>
				The standard basis 
<me>
\vec{e}_1, \vec{e}_2, \ldots, \vec{e}_n ,
</me>
 is a basis of <m>{\mathbb R}^n</m> (hence the name). So as expected 
<me>
\dim {\mathbb R}^n = n .
</me>

			</p>

			<p>
				On the other hand the subspace <m>\{ \vec{0} \}</m> is of dimension <m>0</m>.
			</p>

			<p>
				The subspace <m>S''</m> from a previous example, that is, the set of vectors <m>(a,a)</m> is of dimension 1. One possible basis is simply <m>\{ (1,1) \}</m>, the single vector <m>(1,1)</m>: every vector in <m>S''</m> can be represented by <m>a (1,1) =
(a,a)</m>. Similarly another possible basis would be <m>\{ (-1,-1) \}</m>. Then the vector <m>(a,a)</m> would be represented as <m>(-a) (-1,-1)</m>. In this case, the subspace <m>S''</m> has many different bases, two of which are <m>\{(1,1)\}</m> and <m>\{(-1,-1)\}</m>, and the vector <m>(a,a)</m> has a different representation (different constant) for the different bases.
			</p>
</statement>
</example>
			<p>
				Row and column spaces of a matrix are also examples of subspaces, as they are given as the span of vectors. We can use what we know about row spaces and column spaces from the previous section to find a basis.
			</p>


<example>
<title> </title>
<statement>			<p>
				Earlier, we considered the matrix 
<me>
A =
\begin{bmatrix}
1 &amp; 2 &amp; 3 &amp; 4 \\
2 &amp; 4 &amp; 5 &amp; 6 \\
3 &amp; 6 &amp; 7 &amp; 8
\end{bmatrix} .
</me>
 Using row reduction to find the pivot columns, we found 
<me>
\text{column space of $A$} \left(
\begin{bmatrix}
1 &amp; 2 &amp; 3 &amp; 4 \\
2 &amp; 4 &amp; 5 &amp; 6 \\
3 &amp; 6 &amp; 7 &amp; 8
\end{bmatrix} 
\right)
=
\operatorname{span}
\left\{
\begin{bmatrix}
1 \\
2 \\
3 
\end{bmatrix} 
,
\begin{bmatrix}
3 \\
5 \\
7 
\end{bmatrix} 
\right\} .
</me>
 What we did was we found the basis of the column space. The basis has two elements, and so the column space of <m>A</m> is two dimensional.
			</p>
</statement>
</example>
			<p>
				We would have followed the same procedure if we wanted to find the basis of the subspace <m>X</m> spanned by 
<me>
\begin{bmatrix}
1 \\
2 \\
3 
\end{bmatrix} 
,
\begin{bmatrix}
2 \\
4 \\
6 
\end{bmatrix} 
,
\begin{bmatrix}
3 \\
5 \\
7 
\end{bmatrix} 
,
\begin{bmatrix}
4 \\
6 \\
8 
\end{bmatrix}
.
</me>
 We would have simply formed the matrix <m>A</m> with these vectors as columns and repeated the computation above. The subspace <m>X</m> is then the column space of <m>A</m>.
			</p>


<example>
<title> </title>
<statement>			<p>
				Consider the matrix 
<me>
L =
\begin{bmatrix}
{1} &amp; 2 &amp; 0 &amp; 0 &amp; 3 \\
0 &amp; 0 &amp; {1} &amp; 0 &amp; 4 \\
0 &amp; 0 &amp; 0 &amp; {1} &amp; 5
\end{bmatrix} 
</me>
 Conveniently, the matrix is in reduced row echelon form. The column space is the span of the pivot columns, because the pivot columns always form a basis for the column space of a matrix. It is the 3-dimensional space 
<me>
\text{column space of $L$} =
\operatorname{span} \left\{
\begin{bmatrix}
1 \\
0 \\
0
\end{bmatrix} 
,
\begin{bmatrix}
0 \\
1 \\
0
\end{bmatrix} 
,
\begin{bmatrix}
0 \\
0 \\
1
\end{bmatrix} 
\right\}
= {\mathbb{R}}^3 .
</me>
 The row space is the 3-dimensional space 
<me>
\text{row space of $L$} =
\operatorname{span} \left\{
\begin{bmatrix}
1 &amp; 2 &amp; 0 &amp; 0 &amp; 3
\end{bmatrix} 
,
\begin{bmatrix}
0 &amp; 0 &amp; 1 &amp; 0 &amp; 4
\end{bmatrix} 
,
\begin{bmatrix}
0 &amp; 0 &amp; 0 &amp; 1 &amp; 5
\end{bmatrix} 
\right\} .
</me>
 As these vectors have 5 components, we think of the row space of <m>L</m> as a subspace of <m>{\mathbb{R}}^5</m>.
			</p>
</statement>
</example>
		</subsection>

		<subsection xml:id="rank">
			<title>Rank</title>

			<p>
				In that last example, we noticed that the dimension of the row space and the column space were the same. It turns out that this is not a coincidence. In order to describe this in more detail, we need to define one more term.
			</p>

<!-- div attr= class="definition"-->
			<p>
				Given a matrix <m>A</m>, the maximal number of linearly independent rows is called the <em></em> of <m>A</m>, and we write for the rank.
			</p><!--</div attr= class="definition">-->

			<p>
				For example, 
<me>
\operatorname{rank}
\begin{bmatrix}
1 &amp; 1 &amp; 1 \\
2 &amp; 2 &amp; 2 \\
-1 &amp; -1 &amp; -1
\end{bmatrix}
=
1 .
</me>
 The second and third row are multiples of the first one. We cannot choose more than one row and still have a linearly independent set. But what is 
<me>
\operatorname{rank}
\begin{bmatrix}
1 &amp; 2 &amp; 3 \\
4 &amp; 5 &amp; 6 \\
7 &amp; 8 &amp; 9
\end{bmatrix} \quad = \quad ?
</me>
 That seems to be a tougher question to answer. The first two rows are linearly independent, so the rank is at least two. If we would set up the equations for the <m>\alpha_1</m>, <m>\alpha_2</m>, and <m>\alpha_3</m>, we would find a system with infinitely many solutions. One solution is 
<me>
\begin{bmatrix}
1 &amp; 2 &amp; 3
\end{bmatrix} -2
\begin{bmatrix}
4 &amp; 5 &amp; 6 
\end{bmatrix} +
\begin{bmatrix}
7 &amp; 8 &amp; 9
\end{bmatrix} =
\begin{bmatrix}
0 &amp; 0 &amp; 0
\end{bmatrix} .
</me>
 So the set of all three rows is linearly dependent, the rank cannot be 3. Therefore the rank is 2.
			</p>

			<p>
				But how can we do this in a more systematic way? We find the row echelon form! 
<me>
\text{Row echelon form of}
\quad
\begin{bmatrix}
1 &amp; 2 &amp; 3 \\
4 &amp; 5 &amp; 6  \\
7 &amp; 8 &amp; 9
\end{bmatrix}
\quad
\text{is}
\quad
\begin{bmatrix}
1 &amp; 2 &amp; 3 \\
0 &amp; 1 &amp; 2  \\
0 &amp; 0 &amp; 0
\end{bmatrix} .
</me>
 The elementary row operations do not change the set of linear combinations of the rows (that was one of the main reasons for defining them as they were). In other words, the span of the rows of the <m>A</m> is the same as the span of the rows of the row echelon form of <m>A</m>. In particular, the number of linearly independent rows is the same. And in the row echelon form, all nonzero rows are linearly independent. This is not hard to see. Consider the two nonzero rows in the example above. Suppose we tried to solve for the <m>\alpha_1</m> and <m>\alpha_2</m> in 
<me>
\alpha_1
\begin{bmatrix}
1 &amp; 2 &amp; 3
\end{bmatrix} 
+
\alpha_2
\begin{bmatrix}
0 &amp; 1 &amp; 2 
\end{bmatrix} =
\begin{bmatrix}
0 &amp; 0 &amp; 0
\end{bmatrix} .
</me>
 Since the first column of the row echelon matrix has zeros except in the first row means that <m>\alpha_1 = 0</m>. For the same reason, <m>\alpha_2</m> is zero. We only have two nonzero rows, and they are linearly independent, so the rank of the matrix is 2. This also tells us that if we were trying to solve the system of equations 
<me>
\begin{split}
x_1 + 2x_2 + 3x_3 &amp;= a \\
4x_1 + 5x_2 + 6x_3 &amp;= b \\
7x_1 + 8x_2 + 9x_3 &amp;= c
\end{split}
</me>
 we would get that one row of the reduced augmented matrix has all zeros on the left side, and so this system either has a free variable or is inconsistent, because only two equations here are relevant.
			</p>

			<p>
				Referring back to the examples from earlier in this section, we could carry out the same calculations to say that 
<me>
\text{rank } \begin{bmatrix} 1 &amp; 2 &amp; 3 \\ 4 &amp; 5 &amp; 6 \\ 7 &amp; 8 &amp; 9 \end{bmatrix} = 2 
</me>
 and 
<me>
\text{rank } \begin{bmatrix} 1 &amp; 2 &amp; 0 &amp; 0 &amp; 3 \\ 0 &amp; 0 &amp; 1 &amp; 0 &amp; 4 \\0 &amp; 0 &amp; 0 &amp; 1 &amp; 5 \end{bmatrix} = 3.
</me>

			</p>

			<p>
				We know how to find the set of linearly independent rows, but sometimes it may also be useful to find the linearly independent columns as well. It is a tremendously useful fact that the number of linearly independent columns is always the same as the number of linearly independent rows:
			</p>

<!-- div attr= class="theorem1"-->
			<p>
				<m>\operatorname{rank} A = \operatorname{rank} A^T</m>
			</p><!--</div attr= class="theorem1">-->

			<p>
				Or, in the context of the row and column spaces that we have already discussed:
			</p>

<!-- div attr= class="theorem1"-->
			<p>
				Rank The dimension of the column space and the dimension of the row space of a matrix <m>A</m> are both equal to the rank of <m>A</m>.
			</p><!--</div attr= class="theorem1">-->

			<p>
				This relates to the statement at the start of this section; since the number of vectors that we needed to take to get a basis of linearly independent columns was always the same as the number of pivots, and the number of pivots is the rank, we get that above theorem.
			</p>

		</subsection>

		<subsection xml:id="kernel">
			<title>Kernel</title>

			<p>
				The set of solutions of a linear equation <m>L\vec{x} = \vec{0}</m>, the kernel of <m>L</m>, is a subspace: If <m>\vec{x}</m> and <m>\vec{y}</m> are solutions, then 
<me>
L(\vec{x}+\vec{y}) = 
L\vec{x}+L\vec{y} = 
\vec{0}+\vec{0} = \vec{0} ,
\qquad \text{and} \qquad
L(\alpha \vec{x}) = 
\alpha L \vec{x} = 
\alpha \vec{0} = \vec{0}.
</me>
 So <m>\vec{x}+\vec{y}</m> and <m>\alpha \vec{x}</m> are solutions. The dimension of the kernel is called the <em></em> of the matrix.
			</p>

			<p>
				The same sort of idea governs the solutions of linear differential equations. We try to describe the kernel of a linear differential operator, and as it is a subspace, we look for a basis of this kernel. Much of this book is dedicated to finding such bases.
			</p>

			<p>
				The kernel of a matrix is the same as the kernel of its reduced row echelon form. For a matrix in reduced row echelon form, the kernel is rather easy to find. If a vector <m>\vec{x}</m> is applied to a matrix <m>L</m>, then each entry in <m>\vec{x}</m> corresponds to a column of <m>L</m>, the column that the entry multiplies. To find the kernel, pick a non-pivot column make a vector that has a <m>-1</m> in the entry corresponding to this non-pivot column and zeros at all the other entries corresponding to the other non-pivot columns. Then for all the entries corresponding to pivot columns make it precisely the value in the corresponding row of the non-pivot column to make the vector be a solution to <m>L \vec{x} = \vec{0}</m>. This procedure is best understood by example.
			</p>


<example>
<title> </title>
<statement>			<p>
				Consider 
<me>
L = 
\begin{bmatrix}
\mybxsm{1} &amp; 2 &amp; 0 &amp; 0 &amp; 3 \\
0 &amp; 0 &amp; \mybxsm{1} &amp; 0 &amp; 4 \\
0 &amp; 0 &amp; 0 &amp; \mybxsm{1} &amp; 5
\end{bmatrix} .
</me>
 This matrix is in reduced row echelon form, the pivots are marked. There are two non-pivot columns, so the kernel has dimension 2, that is, it is the span of 2 vectors. Let us find the first vector. We look at the first non-pivot column, the <m>2^{\text{nd}}</m> column, and we put a <m>-1</m> in the <m>2^{\text{nd}}</m> entry of our vector. We put a <m>0</m> in the <m>5^{\text{th}}</m> entry as the <m>5^{\text{th}}</m> column is also a non-pivot column: 
<me>
\begin{bmatrix}
? \\ -1 \\ ? \\ ? \\ 0
\end{bmatrix} .
</me>
 Let us fill the rest. When this vector hits the first row, we get a <m>-2</m> and <m>1</m> times whatever the first question mark is. So make the first question mark <m>2</m>. For the second and third rows, it is sufficient to make it the question marks zero. We are really filling in the non-pivot column into the remaining entries. Let us check while marking which numbers went where: 
<me>
\begin{bmatrix}
1 &amp; \mybxsm{2} &amp; 0 &amp; 0 &amp; 3 \\
0 &amp; \mybxsm{0} &amp; 1 &amp; 0 &amp; 4 \\
0 &amp; \mybxsm{0} &amp; 0 &amp; 1 &amp; 5
\end{bmatrix} 
\begin{bmatrix}
\mybxsm{2} \\ -1 \\ \mybxsm{0} \\ \mybxsm{0} \\ 0
\end{bmatrix}
=
\begin{bmatrix}
0 \\ 0 \\ 0
\end{bmatrix}
.
</me>
 Yay! How about the second vector. We start with 
<me>
\begin{bmatrix}
? \\ 0 \\ ? \\ ? \\ -1 .
\end{bmatrix}
</me>
 We set the first question mark to 3, the second to 4, and the third to 5. Let us check, marking things as previously, 
<me>
\begin{bmatrix}
1 &amp; 2 &amp; 0 &amp; 0 &amp; \mybxsm{3} \\
0 &amp; 0 &amp; 1 &amp; 0 &amp; \mybxsm{4} \\
0 &amp; 0 &amp; 0 &amp; 1 &amp; \mybxsm{5}
\end{bmatrix} 
\begin{bmatrix}
\mybxsm{3} \\ 0 \\ \mybxsm{4} \\ \mybxsm{5} \\ -1
\end{bmatrix}
=
\begin{bmatrix}
0 \\ 0 \\ 0
\end{bmatrix}
.
</me>
 There are two non-pivot columns, so we only need two vectors. We have found the basis of the kernel. So, 
<me>
\text{kernel of $L$} =
\operatorname{span} \left\{
\begin{bmatrix}
2 \\ -1 \\ 0 \\ 0 \\ 0
\end{bmatrix}
,
\begin{bmatrix}
3 \\ 0 \\ 4 \\ 5 \\ -1
\end{bmatrix}
\right\}
</me>

			</p>
</statement>
</example>
			<p>
				What we did in finding a basis of the kernel is we expressed all solutions of <m>L \vec{x} = \vec{0}</m> as a linear combination of some given vectors.
			</p>

			<p>
				The procedure to find the basis of the kernel of a matrix <m>L</m>:
			</p>

			<p><ol>
				<li>
							<p>
								Find the reduced row echelon form of <m>L</m>.
							</p>
				</li>

				<li>
							<p>
								Write down the basis of the kernel as above, one vector for each non-pivot column.
							</p>
				</li>

			</ol></p>

			<p>
				The rank of a matrix is the dimension of the column space, and that is the span of the pivot columns, while the kernel is the span of vectors in the non-pivot columns. So the two numbers must add to the number of columns.
			</p>

<!-- div attr= class="theorem1"-->
			<p>
				Rank–Nullity If a matrix <m>A</m> has <m>n</m> columns, rank <m>r</m>, and nullity <m>k</m> (dimension of the kernel), then 
<me>
n = r+k .
</me>

			</p><!--</div attr= class="theorem1">-->

			<p>
				The theorem is immensely useful in applications. It allows one to compute the rank <m>r</m> if one knows the nullity <m>k</m> and vice versa, without doing any extra work.
			</p>

			<p>
				Let us consider an example application, a simple version of the so-called <em></em>. A similar result is true for differential equations. Consider 
<me>
A \vec{x} = \vec{b} ,
</me>
 where <m>A</m> is a square <m>n \times n</m> matrix. There are then two mutually exclusive possibilities:
			</p>

			<p><ol>
				<li>
							<p>
								A nonzero solution <m>\vec{x}</m> to <m>A \vec{x} = \vec{0}</m> exists.
							</p>
				</li>

				<li>
							<p>
								The equation <m>A \vec{x} = \vec{b}</m> has a unique solution <m>\vec{x}</m> for every <m>\vec{b}</m>.
							</p>
				</li>

			</ol></p>

			<p>
				How does the Rank–Nullity theorem come into the picture? Well, if <m>A</m> has a nonzero solution <m>\vec{x}</m> to <m>A \vec{x} = \vec{0}</m>, then the nullity <m>k</m> is positive. But then the rank <m>r = n-k</m> must be less than <m>n</m>. In particular it means that the column space of <m>A</m> is of dimension less than <m>n</m>, so it is a subspace that does not include everything in <m>{\mathbb{R}}^n</m>. So <m>{\mathbb{R}}^n</m> has to contain some vector <m>\vec{b}</m> not in the column space of <m>A</m>. In fact, most vectors in <m>{\mathbb{R}}^n</m> are not in the column space of <m>A</m>.
			</p>

			<p>
				The idea of a kernel also comes up when defining and discussing eigenvectors. In order to find this vector, we are looking for a vector <m>\vec{v}</m> so that <me>(A - \lambda I)\vec{v} = \vec{0}.</me> This means that we are looking for a vector <m>\vec{v}</m> that is in the kernel of the matrix <m>(A - \lambda I)</m>. Since the kernel is also a subspace, this means that the set of all eigenvectors of a matrix <m>A</m> with a certain eigenvalue is a subspace, so it has a dimension. This dimension is number of linearly independent eigenvectors with that eigenvalue, so it is the geometric multiplicity of this eigenvalue. This also motivates why this is sometimes called the <em>eigenspace</em> for a given eigenvalue. Finding a basis of this subspace (which is also finding the kernel of the matrix <m>A - \lambda I</m> ) is the exact same as the process of finding the eigenvectors of the matrix <m>A</m>.
			</p>

		</subsection>

		<subsection xml:id="computing-the-inverse-adv">
			<title>Computing the inverse</title>

			<p>
				If the matrix <m>A</m> is square and there exists a unique solution <m>\vec{x}</m> to <m>A \vec{x} = \vec{b}</m> for any <m>\vec{b}</m> (there are no free variables), then <m>A</m> is invertible.
			</p>

			<p>
				In particular, if <m>A \vec{x} = \vec{b}</m> then <m>\vec{x} = A^{-1} \vec{b}</m>. Now we just need to compute what <m>A^{-1}</m> is. We can surely do elimination every time we want to find <m>A^{-1} \vec{b}</m>, but that would be ridiculous. The mapping <m>A^{-1}</m> is linear and hence given by a matrix, and we have seen that to figure out the matrix we just need to find where does <m>A^{-1}</m> take the standard basis vectors <m>\vec{e}_1</m>, <m>\vec{e}_2</m>, …, <m>\vec{e}_n</m>.
			</p>

			<p>
				That is, to find the first column of <m>A^{-1}</m> we solve <m>A \vec{x} = \vec{e}_1</m>, because then <m>A^{-1} \vec{e}_1 = \vec{x}</m>. To find the second column of <m>A^{-1}</m> we solve <m>A \vec{x} = \vec{e}_2</m>. And so on. It is really just <m>n</m> eliminations that we need to do. But it gets even easier. If you think about it, the elimination is the same for everything on the left side of the augmented matrix. Doing <m>n</m> eliminations separately we would redo most of the computations. Best is to do all at once.
			</p>

			<p>
				Therefore, to find the inverse of <m>A</m>, we write an <m>n
\times 2n</m> augmented matrix <m>[ \,A ~|~ I\, ]</m>, where <m>I</m> is the identity matrix, whose columns are precisely the standard basis vectors. We then perform row reduction until we arrive at the reduced row echelon form. If <m>A</m> is invertible, then pivots can be found in every column of <m>A</m>, and so the reduced row echelon form of <m>[ \,A ~|~ I\, ]</m> looks like <m>[ \,I ~|~ A^{-1}\, ]</m>. We then just read off the inverse <m>A^{-1}</m>. If you do not find a pivot in every one of the first <m>n</m> columns of the augmented matrix, then <m>A</m> is not invertible.
			</p>

			<p>
				This is best seen by example.
			</p>


<example>
<title> </title>
<statement>			<p>
				Find the inverse of the matrix 
<me>
\begin{bmatrix}
1 &amp; 2 &amp; 3 \\
2 &amp; 0 &amp; 1 \\
3 &amp; 1 &amp; 0
\end{bmatrix} .
</me>

			</p>
</statement>

<solution>			<p>
				We write the augmented matrix and we start reducing: <me>\begin{align*}
&amp; \left[
\begin{array}{ccc|ccc}
\mybxsm{1} &amp; 2 &amp; 3 &amp; 1 &amp; 0 &amp; 0\\
2 &amp; 0 &amp; 1 &amp; 0 &amp; 1 &amp; 0 \\
3 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 1
\end{array}
\right]
\to
\left[
\begin{array}{ccc|ccc}
\mybxsm{1} &amp; 2 &amp; 3 &amp; 1 &amp; 0 &amp; 0\\
0 &amp; -4 &amp; -5 &amp; -2 &amp; 1 &amp; 0 \\
0 &amp; -5 &amp; -9 &amp; -3 &amp; 0 &amp; 1
\end{array}
\right]
\to
\left[
\begin{array}{ccc|ccc}
\mybxsm{1} &amp; 2 &amp; 3 &amp; 1 &amp; 0 &amp; 0\\
0 &amp; \mybxsm{1} &amp; \nicefrac{5}{4} &amp; \nicefrac{1}{2} &amp; \nicefrac{1}{4} &amp; 0 \\
0 &amp; -5 &amp; -9 &amp; -3 &amp; 0 &amp; 1
\end{array}
\right]
\to \\
\to
&amp;
\left[
\begin{array}{ccc|ccc}
\mybxsm{1} &amp; 2 &amp; 3 &amp; 1 &amp; 0 &amp; 0\\
0 &amp; \mybxsm{1} &amp; \nicefrac{5}{4} &amp; \nicefrac{1}{2} &amp; \nicefrac{1}{4} &amp; 0 \\
0 &amp; 0 &amp; \nicefrac{-11}{4} &amp; \nicefrac{-1}{2} &amp; \nicefrac{-5}{4} &amp; 1
\end{array}
\right]
\to
\left[
\begin{array}{ccc|ccc}
\mybxsm{1} &amp; 2 &amp; 3 &amp; 1 &amp; 0 &amp; 0\\
0 &amp; \mybxsm{1} &amp; \nicefrac{5}{4} &amp; \nicefrac{1}{2} &amp; \nicefrac{1}{4} &amp; 0 \\
0 &amp; 0 &amp; \mybxsm{1} &amp; \nicefrac{2}{11} &amp; \nicefrac{5}{11} &amp; \nicefrac{-4}{11}
\end{array}
\right]
\to
\\
\to
&amp;
\left[
\begin{array}{ccc|ccc}
\mybxsm{1} &amp; 2 &amp; 0 &amp; \nicefrac{5}{11} &amp; \nicefrac{-5}{11} &amp; \nicefrac{12}{11} \\
0 &amp; \mybxsm{1} &amp; 0 &amp; \nicefrac{3}{11} &amp; \nicefrac{-9}{11} &amp; \nicefrac{5}{11} \\
0 &amp; 0 &amp; \mybxsm{1} &amp; \nicefrac{2}{11} &amp; \nicefrac{5}{11} &amp; \nicefrac{-4}{11}
\end{array}
\right]
\to
\left[
\begin{array}{ccc|ccc}
\mybxsm{1} &amp; 0 &amp; 0 &amp; \nicefrac{-1}{11} &amp; \nicefrac{3}{11} &amp; \nicefrac{2}{11} \\
0 &amp; \mybxsm{1} &amp; 0 &amp; \nicefrac{3}{11} &amp; \nicefrac{-9}{11} &amp; \nicefrac{5}{11} \\
0 &amp; 0 &amp; \mybxsm{1} &amp; \nicefrac{2}{11} &amp; \nicefrac{5}{11} &amp; \nicefrac{-4}{11}
\end{array}
\right] .
\end{align*}</me> So 
<me>
{\begin{bmatrix}
1 &amp; 2 &amp; 3 \\
2 &amp; 0 &amp; 1 \\
3 &amp; 1 &amp; 0
\end{bmatrix}}^{-1}
=
\begin{bmatrix}
\nicefrac{-1}{11} &amp; \nicefrac{3}{11} &amp; \nicefrac{2}{11} \\
\nicefrac{3}{11} &amp; \nicefrac{-9}{11} &amp; \nicefrac{5}{11} \\
\nicefrac{2}{11} &amp; \nicefrac{5}{11} &amp; \nicefrac{-4}{11}
\end{bmatrix} .
</me>

			</p>
</solution>
</example>
			<p>
				Not too terrible, no? Perhaps harder than inverting a <m>2 \times 2</m> matrix for which we had a formula, but not too bad. Really in practice this is done efficiently by a computer.
			</p>

		</subsection>

		<subsection xml:id="trace-and-determinant-of-matrices">
			<title>Trace and Determinant of Matrices</title>

			<p>
				The next thing to add into our toolbox of matrices is the idea of the trace of a matrix, and how it and the determinant relate to the eigenvalues of said matrix.
			</p>

<!-- div attr= class="definition"-->
			<p>
				Let <m>A</m> be an <m>n \times n</m> square matrix. The <em></em> of <m>A</m> is the sum of all diagonal entries of <m>A</m>.
			</p><!--</div attr= class="definition">-->

			<p>
				For example, if we have the matrix 
<me>
\begin{bmatrix}
1 &amp; 4 &amp; -2 \\
3 &amp; 2 &amp; 5 \\
0 &amp; 1 &amp; 3
\end{bmatrix}
</me>
 the trace is <m>1 + 2 + 3 = 6</m>.
			</p>

			<p>
				The trace is important in our context because it also tells us something about the eigenvalues of a matrix. To work this out, let’s consider the generic <m>2\times 2</m> matrix and how we would find the eigenvalues. If we have a <m>2 \times 2</m> matrix of the form 
<me>
A = \begin{bmatrix} a &amp; b \\ c &amp; d
\end{bmatrix}
</me>
 we can write out the expression <m>\det(A - \lambda I)</m> in order to find the eigenvalues. In this case, we would get 
<me>
\det(A - \lambda I) = \det\left( \begin{bmatrix} a - \lambda &amp; b \\ c &amp; d - \lambda \end{bmatrix} \right) = (a-\lambda)(d-\lambda) - bc = \lambda^2 - (a+d)\lambda + (ad - bc).
</me>

			</p>

			<p>
				However, the coefficients in this polynomial look familiar. <m>(ad-bc)</m> is just the determinant of the matrix <m>A</m>, and <m>a+d</m> is the trace. Therefore, for any <m>2 \times 2</m> matrix, we could write the as 
<men xml:id="eq-CharPoly1">
\det(A - \lambda I) = \lambda^2 - T\lambda + D
</men>
where <m>T</m> is the trace of the matrix and <m>D</m> is the determinant. On the other hand, assume that <m>r_1</m> and <m>r_2</m> are the two eigenvalues of this matrix (whether they be real, complex, or repeated). In that case, we know that this polynomial has <m>r_1</m> and <m>r_2</m> as roots. Therefore, it is equal to 
<men xml:id="eq-CharPoly2">
\det(A - \lambda I) = (\lambda - r_1)(\lambda - r_2) = \lambda^2 - (r_1 + r_2)\lambda + r_1r_2.
</men>
			</p>

			<p>
				Matching up the coefficient of <m>\lambda</m> and the constant term in <xref ref="eq-CharPoly1" /> and <xref ref="eq-CharPoly2" /> gives the relation that 
<me>
T = r_1 + r_2 \qquad D = r_1r_2,
</me>
 that is, the trace of the matrix is the sum of the eigenvalues, and the determinant of the matrix is the product of the eigenvalues. We only showed this fact for <m>2 \times 2</m> matrices, but it does hold for matrices of all sizes, giving us the following theorem.
			</p>

<!-- div attr= class="theorem"-->
			<p>
				Let <m>A</m> be an <m>n \times n</m> square matrix with eigenvalues <m>\lambda_1,\ \lambda_2,\ ..., \lambda_n</m>, written with multiplicity if needed. Then
			</p>

			<p><ol>
				<li>
							<p>
								The trace of <m>A</m> is <m>\lambda_1 + \lambda_2 + \cdots + \lambda_n</m>.
							</p>
				</li>

				<li>
							<p>
								The determinant of <m>A</m> is <m>(\lambda_1)(\lambda_2)\cdots(\lambda_n)</m>.
							</p>
				</li>

			</ol></p><!--</div attr= class="theorem">-->

			<p>
				From the above statement, we note that if any of the eigenvalues is zero, the product of all eigenvalues will be zero, and so the matrix will have zero determinant. This gives an extra follow-up fact, and addition to .
			</p>

<!-- div attr= class="theorem"-->
			<p>
				A matrix <m>A</m> is invertible if and only if all of it’s eigenvalues are non-zero.
			</p><!--</div attr= class="theorem">-->


<example>
<title> </title>
<statement>			<p>
				Use the facts above to analyze the eigenvalues of the matrix 
<me>
A = \begin{bmatrix} 1 &amp; 2 \\ 5 &amp; 4 \end{bmatrix}.
</me>

			</p>
</statement>

<solution>			<p>
				From the matrix <m>A</m>, we can compute that the trace of <m>A</m> is <m>1+4=5</m>, and the determinant is <m>(1)(4) - (2)(5) = -6</m>. Based on the theorem above, we know that the two eigenvalues of this matrix must add to <m>5</m> and multiply to <m>-6</m>. While you could probably guess the numbers here, the important take-aways from this example are what we can learn.
			</p>

			<p>
				The main fact to point out is that this is enough information, in the <m>2 \times 2</m> case, to tell us that the eigenvalues have to be real and distinct. Since their product is a negative number, we can eliminate the other two options. If we have two complex roots, they must be of the form <m>x + iy</m> and <m>x-iy</m>, and so the product is 
<me>
(x+iy)(x-iy) = x^2 + ixy - ixy  - i^2y^2 = x^2 + y^2
</me>
 which is always positive, no matter what <m>x</m> and <m>y</m> are. Similarly, if we have a repeated eigvalue, the product will be that number squared, which is also positive. Therefore, if the determinant of a <m>2 \times 2</m> matrix is negative, the eigenvalues must be real and distinct, with one being positive and one negative (otherwise the product can not be negative). These facts will be important when we start to analyze the solutions to systems of differential equations in .
			</p>
</solution>
</example>

<example>
<title> </title>
<statement>			<p>
				What can be said about the eigenvalues of the matrix 
<me>
A = \begin{bmatrix}
0 &amp; -1 &amp;  0 \\
2 &amp; 2 &amp;  0 \\
-7 &amp;-3 &amp; -1
\end{bmatrix}?
</me>

			</p>
</statement>

<solution>			<p>
				We can find the same information as the previous example. The trace of <m>A</m> is <m>1</m>, and the determinant, by cofactor expansion along column 3, is <m>(-1)(0 + 2) = -2</m>. Therefore, the sum of the <em>three</em> eigenvalues is <m>1</m>, and the product of them is <m>-2</m>. We don’t actually have enough information here to determine what the eigenvalues are. The issue is that with three eigenvalues, there are many different ways to get to a product being negative. There could be three negative eigenvalues, two positive and one negative, or one negative real with two complex eigenvalues. However, the one thing we do know for sure is that there must be one negative real eigenvalue. For this particular example, we can compute that the eigenvalues are <m>-1</m>, <m>1+i</m>, and <m>1-i</m>, so we did end up in the complex case.
			</p>
</solution>
</example>
<exercise>
<statement>
			<p>
				Imagine that we have a <m>3 \times 3</m> matrix with a positive determinant (it doesn’t matter what the trace is). Think about all the scenarios and verify that at least one eigenvalue must be real and positive for this to happen.
			</p>
</statement>

</exercise>
		</subsection>

		<subsection xml:id="extension-of-previous-theorem">
			<title>Extension of Previous Theorem</title>

			<p>
				With all of the new definitions and properties that have been stated, we can add a few more equivalent statements to .
			</p>

<!-- div attr= class="theorem1"-->
			<p>
				Let <m>A</m> be an <m>n \times n</m> matrix. The following are equivalent:
			</p>

			<p><ol>
				<li>
							<p>
								<m>A</m> is invertible.
							</p>
				</li>

				<li>
							<p>
								<m>\det(A) \neq 0</m>.
							</p>
				</li>

				<li>
							<p>
								The rank of <m>A</m> is <m>n</m>.
							</p>
				</li>

				<li>
							<p>
								The rows of <m>A</m> are linearly independent.
							</p>
				</li>

				<li>
							<p>
								The nullity of the matrix is <m>0</m>.
							</p>
				</li>

				<li>
							<p>
								None of the eigenvalues of <m>A</m> are <m>0</m>, or equivalently, the product of the eigenvalues of <m>A</m> is non-zero.
							</p>
				</li>

				<li>
							<p>
								The columns of <m>A</m> are a basis of <m>\R^n</m>.
							</p>
				</li>

				<li>
							<p>
								The rows of <m>A</m> are a basis of <m>\R^n</m>.
							</p>
				</li>

			</ol></p><!--</div attr= class="theorem1">-->

<!-- div attr= class="proof"-->
			<p>
				<em>Proof.</em> Most of these follow from the components of . If <m>A</m> is invertible, then we know that the columns are linearly independent. But there are <m>n</m> columns, so that number must be the rank. This implies that the rows are linearly independent, and if the rank plus the nullity must be <m>n</m>, we must have the nullity equal to zero. On that same train of thought, if we have <m>n</m> linearly independent vectors in <m>\R^n</m>, then they must be a basis, giving (k) and (l). Finally, since the determinant is the product of the eigenvalues, if the determinant is non-zero, that implies fact (j). ◻
			</p><!--</div attr= class="proof">-->

		</subsection>

<exercises>
			<title>Exercises</title>

<exercise>
<statement>
			<p>
				For the following matrices, find a basis for the kernel (nullspace).
			</p>

<!-- div attr= class="tasks"-->
			<p>
				(4) <m>\begin{bmatrix}
1 &amp; 1 &amp; 1 \\
1 &amp; 1 &amp; 5 \\
1 &amp; 1 &amp; -4
\end{bmatrix}</m> <m>\begin{bmatrix}
2 &amp; -1 &amp; -3 \\
4 &amp; 0 &amp; -4 \\
-1 &amp; 1 &amp; 2
\end{bmatrix}</m> <m>\begin{bmatrix}
-4 &amp; 4 &amp; 4 \\
-1 &amp; 1 &amp; 1 \\
-5 &amp; 5 &amp; 5
\end{bmatrix}</m> <m>\begin{bmatrix}
-2 &amp; 1 &amp; 1 &amp; 1 \\
-4 &amp; 2 &amp; 2 &amp; 2 \\
1 &amp; 0 &amp; 4 &amp; 3
\end{bmatrix}</m>
			</p><!--</div attr= class="tasks">-->
</statement>

<answer>			<p>
				 a)  <m>\left\{\left[\begin{smallmatrix} -1 \\ 1 \\ 0 \end{smallmatrix}\right]\right\}</m> b)  <m>\left\{ \left[\begin{smallmatrix} -1 \\ 1 \\-1 \end{smallmatrix}\right] \right\}</m> c)  <m>\left\{\left[\begin{smallmatrix} 1 \\1 \\0 \end{smallmatrix}\right],\ \left[\begin{smallmatrix}  0 \\ -1 \\ 1 \end{smallmatrix}\right]\right\}</m> d)  <m>\left\{ \left[\begin{smallmatrix} -3 \\ -7 \\ 0 \\ 1 \end{smallmatrix}\right],\ \left[\begin{smallmatrix} -4 \\ -9 \\ 1 \\ 0 \end{smallmatrix}\right]\right\}</m> 
			</p>
</answer>
</exercise>

<exercise>
<statement>
			<p>
				For the following matrices, find a basis for the kernel (nullspace).
			</p>

<!-- div attr= class="tasks"-->
			<p>
				(4) <m>\begin{bmatrix}
2 &amp; 6 &amp; 1 &amp; 9 \\
1 &amp; 3 &amp; 2 &amp; 9 \\
3 &amp; 9 &amp; 0 &amp; 9
\end{bmatrix}</m> <m>\begin{bmatrix}
2 &amp; -2 &amp; -5 \\
-1 &amp; 1 &amp; 5 \\
-5 &amp; 5 &amp; -3
\end{bmatrix}</m> <m>\begin{bmatrix}
1 &amp; -5 &amp; -4 \\
2 &amp; 3 &amp; 5 \\
-3 &amp; 5 &amp; 2
\end{bmatrix}</m> <m>\begin{bmatrix}
0 &amp; 4 &amp; 4 \\
0 &amp; 1 &amp; 1 \\
0 &amp; 5 &amp; 5
\end{bmatrix}</m>
			</p><!--</div attr= class="tasks">-->
</statement>

</exercise>
<exercise>
<statement>
			<p>
				Suppose a <m>5 \times 5</m> matrix <m>A</m> has rank 3. What is the nullity?
			</p>
</statement>

<answer>			<p>
				 2 
			</p>
</answer>
</exercise>

<exercise>
<statement>
			<p>
				Consider a square matrix <m>A</m>, and suppose that <m>\vec{x}</m> is a nonzero vector such that <m>A \vec{x} = \vec{0}</m>. What does the Fredholm alternative say about invertibility of <m>A</m>?
			</p>
</statement>

<answer>			<p>
				 <m>A</m> must be non-invertible. 
			</p>
</answer>
</exercise>

<exercise>
<statement>
			<p>
				Compute the rank of the matrix <m>A</m> below. <me>A =  \begin{bmatrix} 0 &amp; -3 &amp; 2 &amp; 4 \\ -5 &amp; -4 &amp;-5 &amp; -1 \\ 1&amp;4&amp;-3 &amp; -5\\ -2 &amp; -3 &amp;-2&amp;1\end{bmatrix}</me> What does this tell you about the invertibility of <m>A</m>? How about the solutions to <m>A\vec{x} = \vec{0}</m>?
			</p>
</statement>

</exercise>
<exercise>
<statement>
			<p>
				Compute the rank of the matrix <m>A</m> below. <me>A =  \begin{bmatrix} 3 &amp; -5 &amp; 5 \\ 2 &amp;-3 &amp; 3\\ 4 &amp; 0 &amp; -1 \end{bmatrix}</me> What does this tell you about the invertibility of <m>A</m>? How about the solutions to <m>A\vec{x} = \begin{bmatrix} 1\\1\\1 \end{bmatrix}</m>?
			</p>
</statement>

</exercise>
<exercise>
<statement>
			<p>
				Consider 
<me>
M =
\begin{bmatrix}
1 &amp; 2 &amp; 3 \\
2 &amp; ? &amp; ? \\
-1 &amp; ? &amp; ?
\end{bmatrix} .
</me>
 If the nullity of this matrix is 2, fill in the question marks. Hint: What is the rank?
			</p>
</statement>

<answer>			<p>
				 <m>M = \left[\begin{smallmatrix} 1 &amp; 2 &amp; 3 \\ 2 &amp; 4 &amp; 6 \\ -1 &amp; -2 &amp; -3 \end{smallmatrix}\right]</m> 
			</p>
</answer>
</exercise>

<exercise>
<statement>
			<p>
				Suppose the column space of a <m>9 \times 5</m> matrix <m>A</m> of dimension 3. Find
			</p>

<!-- div attr= class="tasks"-->
			<p>
				(2) Rank of <m>A</m>. Nullity of <m>A</m>. Dimension of the row space of <m>A</m>. Dimension of the nullspace of <m>A</m>. Size of the maximum subset of linearly independent rows of <m>A</m>.
			</p><!--</div attr= class="tasks">-->
</statement>

</exercise>
<exercise>
<statement>
			<p>
				 Compute the rank of the given matrices
			</p>

<!-- div attr= class="tasks"-->
			<p>
				(3) <m>\begin{bmatrix}
6 &amp; 3 &amp; 5 \\
1 &amp; 4 &amp; 1 \\
7 &amp; 7 &amp; 6
\end{bmatrix}</m> <m>\begin{bmatrix}
5 &amp; -2 &amp; -1 \\
3 &amp; 0 &amp; 6 \\
2 &amp; 4 &amp; 5
\end{bmatrix}</m> <m>\begin{bmatrix}
1 &amp; 2 &amp; 3 \\
-1 &amp; -2 &amp; -3 \\
2 &amp; 4 &amp; 6
\end{bmatrix}</m>
			</p><!--</div attr= class="tasks">-->
</statement>

<answer>			<p>
				 a)  2 b)  3 c)  1 
			</p>
</answer>
</exercise>

<exercise>
<statement>
			<p>
				 Compute the rank of the given matrices
			</p>

<!-- div attr= class="tasks"-->
			<p>
				(3) <m>\begin{bmatrix}
7 &amp; -1 &amp; 6 \\
7 &amp; 7 &amp; 7 \\
7 &amp; 6 &amp; 2
\end{bmatrix}</m> <m>\begin{bmatrix}
1 &amp; 1 &amp; 1 \\
1 &amp; 1 &amp; 1 \\
2 &amp; 2 &amp; 2
\end{bmatrix}</m> <m>\begin{bmatrix}
0 &amp; 3 &amp; -1 \\
6 &amp; 3 &amp; 1 \\
4 &amp; 7 &amp; -1
\end{bmatrix}</m>
			</p><!--</div attr= class="tasks">-->
</statement>

</exercise>
<exercise>
<statement>
			<p>
				For the matrices in , find a linearly independent set of row vectors that span the row space (they don’t need to be rows of the matrix).
			</p>
</statement>

<answer>			<p>
				 a)  <m>[1, 4, 1],\ [0, -21, 1]</m> b) <m>[1,0,0],\ [0,1,0],\ [0,0,1]</m> c) <m>[1,2,3]</m> 
			</p>
</answer>
</exercise>

<exercise>
<statement>
			<p>
				For the matrices in , find a linearly independent set of columns that span the column space. That is, find the pivot columns of the matrices.
			</p>
</statement>

<answer>			<p>
				 a) <m>\left[\begin{smallmatrix} 6 \\ 1 \\ 7 \end{smallmatrix}\right],\ \left[\begin{smallmatrix} 3 \\ 4 \\ 7 \end{smallmatrix}\right]</m> b)  <m>\left[\begin{smallmatrix} 5 \\ 3 \\2 \end{smallmatrix}\right],\ \left[\begin{smallmatrix} -2 \\ 0 \\ 4 \end{smallmatrix}\right],\ \left[\begin{smallmatrix} -1 \\ 6 \\ 5 \end{smallmatrix}\right]</m> c)  <m>\left[\begin{smallmatrix} 1 \\ -1 \\2 \end{smallmatrix}\right]</m> 
			</p>
</answer>
</exercise>

<exercise>
<statement>
			<p>
				For the matrices in , find a linearly independent set of row vectors that span the row space (they don’t need to be rows of the matrix).
			</p>
</statement>

</exercise>
<exercise>
<statement>
			<p>
				For the matrices in , find a linearly independent set of columns that span the column space. That is, find the pivot columns of the matrices.
			</p>
</statement>

</exercise>
<exercise>
<statement>
			<p>
				Compute the rank of the matrix 
<me>
\begin{bmatrix}
10 &amp; -2 &amp; 11 &amp; -7 \\ 
-5 &amp; -2 &amp; -5 &amp; 5 \\
1 &amp; 0 &amp; -4 &amp; -4 \\
1 &amp; 2 &amp; 2 &amp; -1
\end{bmatrix} 
</me>

			</p>
</statement>

</exercise>
<exercise>
<statement>
			<p>
				Compute the rank of the matrix 
<me>
\begin{bmatrix}
4 &amp; -2 &amp; 0 &amp; -4 \\
3 &amp; -5 &amp; 2 &amp; 0 \\
1 &amp; -2 &amp; 0 &amp; 1 \\
-1 &amp; 1 &amp; 3 &amp; -3
\end{bmatrix} 
</me>

			</p>
</statement>

</exercise>
<exercise>
<statement>
			<p>
				Find a linearly independent subset of the following vectors that has the same span. 
<me>
\begin{bmatrix}
-1 \\ 1 \\ 2
\end{bmatrix}
, \quad
\begin{bmatrix}
2 \\ -2 \\ -4
\end{bmatrix}
, \quad
\begin{bmatrix}
-2 \\ 4 \\ 1
\end{bmatrix}
, \quad
\begin{bmatrix}
-1 \\ 3 \\ -2
\end{bmatrix}
</me>

			</p>
</statement>

<answer>			<p>
				 <m>\left[\begin{smallmatrix} -1\\ 1\\ 2 \end{smallmatrix}\right],\ \left[\begin{smallmatrix} -2 \\4  \\1 \end{smallmatrix}\right]</m> 
			</p>
</answer>
</exercise>

<exercise>
<statement>
			<p>
				Find a linearly independent subset of the following vectors that has the same span. 
<me>
\begin{bmatrix}
0 \\ 0 \\ 0
\end{bmatrix}
, \quad
\begin{bmatrix}
3 \\ 1 \\ -5
\end{bmatrix}
, \quad
\begin{bmatrix}
0 \\ 3 \\ -1
\end{bmatrix}
, \quad
\begin{bmatrix}
-3 \\ 2 \\ 4
\end{bmatrix}
</me>

			</p>
</statement>

</exercise>
<exercise>
<statement>
			<p>
				For the following sets of vectors, determine if the set is linearly independent. Then find a basis for the subspace spanned by the vectors, and find the dimension of the subspace.
			</p>

<!-- div attr= class="tasks"-->
			<p>
				(3) <m>\begin{bmatrix}
1 \\ 1 \\ 1
\end{bmatrix}
, \quad
\begin{bmatrix}
-1 \\ -1 \\ -1
\end{bmatrix}</m> <m>\begin{bmatrix}
1 \\ 0 \\ 5
\end{bmatrix}
, \quad
\begin{bmatrix}
0 \\ 1 \\ 0
\end{bmatrix}
, \quad
\begin{bmatrix}
0 \\ -1 \\ 0
\end{bmatrix}</m> <m>\begin{bmatrix}
-4 \\ -3 \\ 5
\end{bmatrix}
, \quad
\begin{bmatrix}
2 \\ 3 \\ 3
\end{bmatrix}
, \quad
\begin{bmatrix}
2 \\ 0 \\ 2
\end{bmatrix}</m> <m>\begin{bmatrix}
1 \\ 3 \\ 0
\end{bmatrix}
, \quad
\begin{bmatrix}
0 \\ 2 \\ 2
\end{bmatrix}
, \quad
\begin{bmatrix}
-1 \\ -1 \\ 2
\end{bmatrix}</m> <m>\begin{bmatrix}
1 \\ 3
\end{bmatrix}
, \quad
\begin{bmatrix}
0 \\ 2
\end{bmatrix}
, \quad
\begin{bmatrix}
-1 \\ -1
\end{bmatrix}</m> <m>\begin{bmatrix}
3 \\ 1 \\ 3
\end{bmatrix}
, \quad
\begin{bmatrix}
2 \\ 4 \\ -4
\end{bmatrix}
, \quad
\begin{bmatrix}
-5 \\ -5 \\ -2
\end{bmatrix}</m>
			</p><!--</div attr= class="tasks">-->
</statement>

<answer>			<p>
				 a) No, <m>\left[\begin{smallmatrix}  1 \\ 1 \\ 1 \end{smallmatrix}\right]</m>, Dimension 1 b) No, <m>\left[\begin{smallmatrix} 1 \\ 0 \\ 5 \end{smallmatrix}\right],\ \left[\begin{smallmatrix} 0 \\ 1 \\ 0 \end{smallmatrix}\right]</m>, Dimension 2 c) Yes, All 3, Dimension 3 d) No, <m>\left[\begin{smallmatrix} 1 \\ 3 \\ 0 \end{smallmatrix}\right],\ \left[\begin{smallmatrix} 0 \\ 2 \\ 2 \end{smallmatrix}\right]</m>, Dimension 2 e) No, <m>\left[\begin{smallmatrix}  1 \\ 3 \end{smallmatrix}\right],\ \left[\begin{smallmatrix} 0 \\ 2 \end{smallmatrix}\right]</m>, Dimension 2 f)  Yes, All 3, Dimension 3 
			</p>
</answer>
</exercise>

<exercise>
<statement>
			<p>
				For the following sets of vectors, determine if the set is linearly independent. Then find a basis for the subspace spanned by the vectors, and find the dimension of the subspace.
			</p>

<!-- div attr= class="tasks"-->
			<p>
				(3) <m>\begin{bmatrix}
1 \\ 2
\end{bmatrix}
, \quad
\begin{bmatrix}
1 \\ 1
\end{bmatrix}</m> <m>\begin{bmatrix}
1 \\ 1 \\ 1
\end{bmatrix}
, \quad
\begin{bmatrix}
2 \\ 2 \\ 2
\end{bmatrix}
, \quad
\begin{bmatrix}
1 \\ 1 \\ 2
\end{bmatrix}</m> <m>\begin{bmatrix}
5 \\ 3 \\ 1
\end{bmatrix}
, \quad
\begin{bmatrix}
5 \\ -1 \\ 5
\end{bmatrix}
, \quad
\begin{bmatrix}
-1 \\ 3 \\ -4
\end{bmatrix}</m> <m>\begin{bmatrix}
2 \\ 2 \\ 4
\end{bmatrix}
, \quad
\begin{bmatrix}
2 \\ 2 \\ 3
\end{bmatrix}
, \quad
\begin{bmatrix}
4 \\ 4 \\ -3
\end{bmatrix}</m> <m>\begin{bmatrix}
1 \\ 0
\end{bmatrix}
, \quad
\begin{bmatrix}
2 \\ 0
\end{bmatrix}
, \quad
\begin{bmatrix}
3 \\ 0
\end{bmatrix}</m> <m>\begin{bmatrix}
1 \\ 0 \\ 0
\end{bmatrix}
, \quad
\begin{bmatrix}
2 \\ 0 \\ 0
\end{bmatrix}
, \quad
\begin{bmatrix}
0 \\ 1 \\ 2
\end{bmatrix}</m>
			</p><!--</div attr= class="tasks">-->
</statement>

</exercise>
<exercise>
<statement>
			<p>
				Suppose that <m>X</m> is the set of all the vectors of <m>{\mathbb{R}}^3</m> whose third component is zero. Is <m>X</m> a subspace? And if so, find a basis and the dimension.
			</p>
</statement>

<answer>			<p>
				 Yes. Basis: <m>\left\{\left[\begin{smallmatrix} 1 \\ 0 \\ 0 \end{smallmatrix}\right],\ \left[\begin{smallmatrix} 0 \\ 1 \\ 0 \end{smallmatrix}\right]\right\}</m> Dimension 2 
			</p>
</answer>
</exercise>

<exercise>
<statement>
			<p>
				Consider a set of 3 component vectors.
			</p>

<!-- div attr= class="tasks"-->
			<p>
				How can it be shown if these vectors are linearly independent? Can a set of 4 of these 3 component vectors be linearly independent? Explain your answer. Can a set of 2 of these 3 component vectors be linearly independent? Explain. How would it be shown if these vectors make up a spanning set for all 3 component vectors? Can 4 vectors be a spanning set? Explain. Can 2 vectors be a spanning set? Explain.
			</p><!--</div attr= class="tasks">-->
</statement>

</exercise>
<exercise>
<statement>
			<p>
				 Consider the vectors 
<me>
\vec{v}_1 = \begin{bmatrix} 4 \\ 2 \\ -1 \end{bmatrix} \quad \vec{v}_2 = \begin{bmatrix} 3 \\ 5 \\ 1 \end{bmatrix} \qquad \begin{bmatrix} 1 \\ -1 \\ -1 \end{bmatrix}. 
</me>
 Let <m>A</m> be the matrix with these vectors as columns and <m>\vec{b}</m> the vector <m>[1\ 0 \ 0]</m>.
			</p>

<!-- div attr= class="tasks"-->
			<p>
				Compute the rank of <m>A</m> to determine how many of these vectors are linearly independent. Determine if <m>\vec{b}</m> is in the span of the given vectors by using row reduction to try to solve <m>A\vec{x} = \vec{b}</m>. Look at the columns of the row-reduced form of <m>A</m>. Is <m>\vec{b}</m> in the span of those vectors? What do these last two parts tell you about the span of the columns of a matrix, and the span of the columns of the row-reduced matrix? Now, build a matrix <m>D</m> with these vectors as rows. Row-reduce this matrix to get a matrix <m>D_2</m>. Is <m>\vec{b}</m> in the span of the rows of <m>D_2</m>? You can’t check this in using the matrix form; instead, just brute force it based on the form of <m>D_2</m>. What does this potentially say about the span of the rows of <m>D</m> and the rows of <m>D_2</m>?
			</p><!--</div attr= class="tasks">-->
</statement>

</exercise>
<exercise>
<statement>
			<p>
				Complete with 
<me>
\vec{v}_1 = \begin{bmatrix} 1 \\ 0 \\ -1 \\ 0 \end{bmatrix} \quad \vec{v}_2 = \begin{bmatrix} -6 \\ 2 \\ 3 \\ -1 \end{bmatrix} \qquad \begin{bmatrix} -13 \\ 3 \\ 1 \\ 1 \end{bmatrix} \quad \vec{v}_4 \begin{bmatrix} 11 \ -1 \\ -5 \\ -1 \end{bmatrix} \quad \vec{b} = \begin{bmatrix} 1 \\ 0 \\ 0 \\ 0 \end{bmatrix}. 
</me>

			</p>
</statement>

<answer>			<p>
				 a)  3 b) No c)  Yes d) They are not the same e)  <m>\left[\begin{smallmatrix} 1 &amp; 0 &amp; -1 &amp; 0 \\ 0 &amp; 1 &amp; -6 &amp; 1 \\ 0 &amp; 0 &amp; 3 &amp; -1 \\ 0 &amp; 0 &amp; 0 &amp; 0 \end{smallmatrix}\right]</m> f)  No 
			</p>
</answer>
</exercise>

<exercise>
<statement>
			<p>
				Compute the inverse of the given matrices
			</p>

<!-- div attr= class="tasks"-->
			<p>
				(3) <m>\begin{bmatrix}
1 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 1 \\
0 &amp; 1 &amp; 0
\end{bmatrix}</m> <m>\begin{bmatrix}
1 &amp; 1 &amp; 1 \\
0 &amp; 2 &amp; 1 \\
0 &amp; 0 &amp; 1
\end{bmatrix}</m> <m>\begin{bmatrix}
1 &amp; 2 &amp; 3 \\
2 &amp; 0 &amp; 1 \\
0 &amp; 2 &amp; 1
\end{bmatrix}</m>
			</p><!--</div attr= class="tasks">-->
</statement>

<answer>			<p>
				 a)  <m>\left[\begin{smallmatrix} 1 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 1 \\ 0 &amp; 1 &amp; 0 \end{smallmatrix}\right]</m> b)  <m>\left[\begin{smallmatrix} 1 &amp; -1/2 &amp; -1/2 \\ 0 &amp; 1/2 &amp; -1/2 \\ 0 &amp; 0 &amp; 1 \end{smallmatrix}\right]</m> c) <m>\left[\begin{smallmatrix} -1/3 &amp; 2/3 &amp; 1/3 \\ -1/3 &amp; 1/6 &amp; 5/6 \\ 2/3 &amp; -1/3 &amp; -2/3 \end{smallmatrix}\right]</m> 
			</p>
</answer>
</exercise>

<exercise>
<statement>
			<p>
				Compute the inverse of the given matrices
			</p>

<!-- div attr= class="tasks"-->
			<p>
				(3) <m>\begin{bmatrix}
0 &amp; 1 &amp; 0 \\
-1 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 1
\end{bmatrix}</m> <m>\begin{bmatrix}
1 &amp; 1 &amp; 1 \\
1 &amp; 1 &amp; 0 \\
1 &amp; 0 &amp; 0
\end{bmatrix}</m> <m>\begin{bmatrix}
2 &amp; 4 &amp; 0 \\
2 &amp; 2 &amp; 3 \\
2 &amp; 4 &amp; 1
\end{bmatrix}</m>
			</p><!--</div attr= class="tasks">-->
</statement>

</exercise>
<exercise>
<statement>
			<p>
				By computing the inverse, solve the following systems for <m>\vec{x}</m>.
			</p>

<!-- div attr= class="tasks"-->
			<p>
				(2) <m>\begin{bmatrix}
4 &amp; 1 \\
-1 &amp; 3
\end{bmatrix} \vec{x} =
\begin{bmatrix} 13 \\ 26 \end{bmatrix}</m> <m>\begin{bmatrix}
3 &amp; 3 \\
3 &amp; 4
\end{bmatrix} \vec{x} =
\begin{bmatrix} 2 \\ -1 \end{bmatrix}</m>
			</p><!--</div attr= class="tasks">-->
</statement>

<answer>			<p>
				 a)  <m>\left[\begin{smallmatrix}  1 \\ 9 \end{smallmatrix}\right]</m> b) <m>\left[\begin{smallmatrix} 11/3 \\ -3 \end{smallmatrix}\right]</m> 
			</p>
</answer>
</exercise>

<exercise>
<statement>
			<p>
				By computing the inverse, solve the following systems for <m>\vec{x}</m>.
			</p>

<!-- div attr= class="tasks"-->
			<p>
				(2) <m>\begin{bmatrix}
-1 &amp; 1 \\
3 &amp; 3
\end{bmatrix} \vec{x} =
\begin{bmatrix} 4 \\ 6 \end{bmatrix}</m> <m>\begin{bmatrix}
2 &amp; 7 \\
1 &amp; 6
\end{bmatrix} \vec{x} =
\begin{bmatrix} 1 \\ 3 \end{bmatrix}</m>
			</p><!--</div attr= class="tasks">-->
</statement>

</exercise>
<exercise>
<statement>
			<p>
				For each of the following matrices below:
			</p>

<!-- div attr= class="tasks"-->
			<p>
				Compute the trace and determinant of the matrix, and Find the eigenvalues of the matrix and verify that the trace is the sum of the eigenvalues and the determinant is the product.
			</p><!--</div attr= class="tasks">-->

			<p>
				
<me>
(i) \ \begin{bmatrix} -4 &amp; 2 \\ -9 &amp; 5 \end{bmatrix} \qquad (ii) \ \begin{bmatrix} 2 &amp; -3 \\ 6 &amp; -4 \end{bmatrix} \qquad (iii)  \ \begin{bmatrix} -10&amp; -12 \\ 6 &amp; 8\end{bmatrix}. \qquad (iv) \ \begin{bmatrix} -7 &amp; -9 \\ 1 &amp; -1 \end{bmatrix}
</me>

			</p>
</statement>

</exercise>
<exercise>
<statement>
			<p>
				For each of the following matrices below:
			</p>

<!-- div attr= class="tasks"-->
			<p>
				Compute the trace and determinant of the matrix, and Find the eigenvalues of the matrix and verify that the trace is the sum of the eigenvalues and the determinant is the product.
			</p><!--</div attr= class="tasks">-->

			<p>
				
<me>
(i) \ \begin{bmatrix} -1 &amp; -16 &amp; -4 \\ 1 &amp; 6 &amp; 1 \\ -2 &amp; -4 &amp; 1  \end{bmatrix} \qquad (ii) \ \begin{bmatrix} 1 &amp; 2 &amp; 0 \\ -12 &amp; -13 &amp; -4 \\ 16 &amp; 14 &amp; 3  \end{bmatrix} \qquad (iii)\ \begin{bmatrix} 10 &amp; -7 &amp; -14 \\ 0 &amp; 5 &amp; 6 \\ 7 &amp; -8 &amp; -14 \end{bmatrix}
</me>

			</p>
</statement>
</exercise></exercises>

	</section>



