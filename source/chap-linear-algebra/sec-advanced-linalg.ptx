<?xml version="1.0" encoding="UTF-8"?>
<!-- Generated by Pandoc using pretext.lua -->


<section xml:id="sec-kernel">
	<title>Related Topics in Linear Algebra</title>


	<subsection xml:id="subspaces-and-span">
		<title>Subspaces and span</title>

		<p> Assume that we find two vectors that solve <m>A\vec{x} = 0</m>. What other vectors also
			solve this equation? In our discussion of linear combinations, we saw that if <m>
			\vec{x}_1</m> and <m>\vec{x}_2</m> solve <m>A\vec{x} = 0</m>, then so does <m>A(\alpha_1\vec{x}_1
			+ \alpha_2\vec{x}_2)</m> for any constants <m>\alpha_1</m> and <m>\alpha_2</m>. Thus,
			all linear combinations will also solve the equation. This leads to the definition of
			the span of a set of vectors. </p>

		<!-- div attr= class="definition"-->
		<p> The set of all linear combinations of a set of vectors is called their <em></em>. <me>
			\operatorname{span} \bigl\{ \vec{x}_1, \vec{x}_2 , \ldots , \vec{x}_n \bigr\}
				=
				\bigl\{
				\text{Set of all linear combinations of
				$\vec{x}_1, \vec{x}_2 , \ldots , \vec{x}_n$}
				\bigr\} .
			</me>

		</p><!--</div
		attr= class="definition">-->

		<p> Thus, if two vectors solve a homogeneous equation, so does everything in the span of
			those two vectors. The span of a collection of vectors is an example of a subspace,
			which is a common object in linear algebra. We say that a set <m>S</m> of vectors in <m>{\mathbb
			R}^n</m> is a <em></em> if whenever <m>\vec{x}</m> and <m>\vec{y}</m> are members of <m>
			S</m> and <m>\alpha</m> is a scalar, then <me>
				\vec{x} + \vec{y}, \qquad \text{and} \qquad \alpha \vec{x}
			</me> are also members of <m>
			S</m>. That is, we can add and multiply by scalars and we still land in <m>S</m>. So
			every linear combination of vectors of <m>S</m> is still in <m>S</m>. That is really
			what a subspace is. It is a subset where we can take linear combinations and still end
			up being in the subset. </p>


		<example>
		<title> </title>
		<statement>
			<p> If we let <m>S = {\mathbb R}^n</m>, then this <m>S</m> is a subspace of <m>{\mathbb
				R}^n</m>. Adding any two vectors in <m>{\mathbb R}^n</m> gets a vector in <m>{\mathbb
				R}^n</m>, and so does multiplying by scalars. </p>

			<p> The set <m>S' = \{ \vec{0} \}</m>, that is, the set of the zero vector by itself, is
				also a subspace of <m>{\mathbb R}^n</m>. There is only one vector in this subspace,
				so we only need to check for that one vector, and everything checks out: <m>\vec{0}+\vec{0}
				= \vec{0}</m> and <m>\alpha \vec{0} = \vec{0}</m>. </p>

			<p> The set <m>S''</m> of all the vectors of the form <m>(a,a)</m> for any real number <m>
				a</m>, such as <m>(1,1)</m>, <m>(3,3)</m>, or <m>(-0.5,-0.5)</m> is a subspace of <m>{\mathbb
				R}^2</m>. Adding two such vectors, say <m>(1,1)+(3,3) = (4,4)</m> again gets a
				vector of the same form, and so does multiplying by a scalar, say <m>8(1,1) = (8,8)</m>
				.
			</p>
		</statement>
	</example>
		<p> We can apply these ideas to the vectors that live inside a matrix. The span of the rows
			of a matrix <m>A</m> is called the <em></em>. The row space of <m>A</m> and the row
			space of the row echelon form of <m>A</m> are the same, because reducing the matrix <m>A</m>
			to its row echelon form involves taking linear combinations, which will preserve the
			span. In the example, <me>
				\begin{split}
				\text{row space of }
				\begin{bmatrix}
				1 &amp; 2 &amp; 3 \\
				4 &amp; 5 &amp; 6 \\
				7 &amp; 8 &amp; 9
				\end{bmatrix}
				&amp; =
				\operatorname{span}
				\left\{
				\begin{bmatrix}
				1 &amp; 2 &amp; 3
				\end{bmatrix}
				,
				\begin{bmatrix}
				4 &amp; 5 &amp; 6
				\end{bmatrix}
				,
				\begin{bmatrix}
				7 &amp; 8 &amp; 9
				\end{bmatrix}
				\right\}
				\\
				&amp; =
				\operatorname{span}
				\left\{
				\begin{bmatrix}
				1 &amp; 2 &amp; 3
				\end{bmatrix}
				,
				\begin{bmatrix}
				0 &amp; 1 &amp; 2
				\end{bmatrix}
				\right\} .
				\end{split}
			</me>

		</p>

		<p> Similarly to row space, the span of columns is called the <em></em>. <me>
				\text{column space of }
				\begin{bmatrix}
				1 &amp; 2 &amp; 3 \\
				4 &amp; 5 &amp; 6 \\
				7 &amp; 8 &amp; 9
				\end{bmatrix}
				=
				\operatorname{span}
				\left\{
				\begin{bmatrix}
				1 \\ 4 \\ 7
				\end{bmatrix}
				,
				\begin{bmatrix}
				2 \\ 5 \\ 8
				\end{bmatrix}
				,
				\begin{bmatrix}
				3 \\ 6 \\ 9
				\end{bmatrix}
				\right\} .
			</me>

		</p>

		<p> In particular, to find a set of linearly independent columns we need to look at where
			the pivots were. If you recall above, when solving <m>A \vec{x}
				= \vec{0}</m> the key was finding the pivots, any non-pivot columns corresponded to
			free variables. That means we can solve for the non-pivot columns in terms of the pivot
			columns. Letâ€™s see an example. </p>


		<example>
			<title> </title>
			<statement>
				<p> Find the linearly independent columns of the matrix <me>
						\begin{bmatrix}
						1 &amp; 2 &amp; 3 &amp; 4 \\
						2 &amp; 4 &amp; 5 &amp; 6 \\
						3 &amp; 6 &amp; 7 &amp; 8
						\end{bmatrix} .
					</me>

				</p>
			</statement>

			<solution>
				<p> We find a pivot and reduce the rows below: <me>
						\begin{bmatrix}
						\mybxsm{1} &amp; 2 &amp; 3 &amp; 4 \\
						2 &amp; 4 &amp; 5 &amp; 6 \\
						3 &amp; 6 &amp; 7 &amp; 8
						\end{bmatrix}
						\to
						\begin{bmatrix}
						\mybxsm{1} &amp; 2 &amp; 3 &amp; 4 \\
						0 &amp; 0 &amp; -1 &amp; -2 \\
						3 &amp; 6 &amp; 7 &amp; 8
						\end{bmatrix}
						\to
						\begin{bmatrix}
						\mybxsm{1} &amp; 2 &amp; 3 &amp; 4 \\
						0 &amp; 0 &amp; -1 &amp; -2 \\
						0 &amp; 0 &amp; -2 &amp; -4
						\end{bmatrix} .
					</me>
					We find the next pivot, make it one, and rinse and repeat: <me>
						\begin{bmatrix}
						\mybxsm{1} &amp; 2 &amp; 3 &amp; 4 \\
						0 &amp; 0 &amp; \mybxsm{-1} &amp; -2 \\
						0 &amp; 0 &amp; -2 &amp; -4
						\end{bmatrix}
						\to
						\begin{bmatrix}
						\mybxsm{1} &amp; 2 &amp; 3 &amp; 4 \\
						0 &amp; 0 &amp; \mybxsm{1} &amp; 2 \\
						0 &amp; 0 &amp; -2 &amp; -4
						\end{bmatrix}
						\to
						\begin{bmatrix}
						\mybxsm{1} &amp; 2 &amp; 3 &amp; 4 \\
						0 &amp; 0 &amp; \mybxsm{1} &amp; 2 \\
						0 &amp; 0 &amp; 0 &amp; 0
						\end{bmatrix} .
					</me>
					The final matrix is the row echelon form of the matrix. Consider the pivots that
					we marked. The pivot columns are the first and the third column. All other
					columns correspond to free variables when solving <m>A \vec{x} = \vec{0}</m>, so
					all other columns can be solved in terms of the first and the third column. In
					other words <me>
						\text{column space of }
						\begin{bmatrix}
						1 &amp; 2 &amp; 3 &amp; 4 \\
						2 &amp; 4 &amp; 5 &amp; 6 \\
						3 &amp; 6 &amp; 7 &amp; 8
						\end{bmatrix}
						=
						\operatorname{span}
						\left\{
						\begin{bmatrix}
						1 \\
						2 \\
						3
						\end{bmatrix}
						,
						\begin{bmatrix}
						2 \\
						4 \\
						6
						\end{bmatrix}
						,
						\begin{bmatrix}
						3 \\
						5 \\
						7
						\end{bmatrix}
						,
						\begin{bmatrix}
						4 \\
						6 \\
						8
						\end{bmatrix}
						\right\}
						=
						\operatorname{span}
						\left\{
						\begin{bmatrix}
						1 \\
						2 \\
						3
						\end{bmatrix}
						,
						\begin{bmatrix}
						3 \\
						5 \\
						7
						\end{bmatrix}
						\right\} .
					</me>

				</p>
			</solution>
		</example>
		<p>
			We could perhaps use another pair of columns to get the same span, but the first and the
			third are guaranteed to work because they are pivot columns.
		</p>

		<p> In the previous example, this means that only the first and third colums are <q>
			important</q> in the sense of generating the full column space as a span. We would like
			to have a way to talk about what these first and third columns do. </p>

		<!-- div attr= class="definition"-->
		<p> Let <m>S</m> be a subspace of a vector space. The set <m>\{\vec{v}_1, \vec{v}_2, ...,
			\vec{v}_k\}</m> is a <em></em> for the subspace <m>S</m> if each of these vectors are in <m>
			S</m> and the span of <m>\{\vec{v}_1, \vec{v}_2, ..., \vec{v}_k\}</m> is equal to <m>S</m>
			.
			</p><!--</div
		attr= class="definition">-->

		<p> In the context of the previous example, for the matrix <me>
				A = \begin{bmatrix}
				1 &amp; 2 &amp; 3 &amp; 4 \\
				2 &amp; 4 &amp; 5 &amp; 6 \\
				3 &amp; 6 &amp; 7 &amp; 8
				\end{bmatrix}
			</me>
			we know that <me>
				\text{column space of }
				\begin{bmatrix}
				1 &amp; 2 &amp; 3 &amp; 4 \\
				2 &amp; 4 &amp; 5 &amp; 6 \\
				3 &amp; 6 &amp; 7 &amp; 8
				\end{bmatrix}
				=
				\operatorname{span}
				\left\{
				\begin{bmatrix}
				1 \\
				2 \\
				3
				\end{bmatrix}
				,
				\begin{bmatrix}
				2 \\
				4 \\
				6
				\end{bmatrix}
				,
				\begin{bmatrix}
				3 \\
				5 \\
				7
				\end{bmatrix}
				,
				\begin{bmatrix}
				4 \\
				6 \\
				8
				\end{bmatrix}
				\right\}
				=
				\operatorname{span}
				\left\{
				\begin{bmatrix}
				1 \\
				2 \\
				3
				\end{bmatrix}
				,
				\begin{bmatrix}
				3 \\
				5 \\
				7
				\end{bmatrix}
				\right\} .
			</me>
			This means that both <me>
				\left\{
				\begin{bmatrix}
				1 \\
				2 \\
				3
				\end{bmatrix}
				,
				\begin{bmatrix}
				2 \\
				4 \\
				6
				\end{bmatrix}
				,
				\begin{bmatrix}
				3 \\
				5 \\
				7
				\end{bmatrix}
				,
				\begin{bmatrix}
				4 \\
				6 \\
				8
				\end{bmatrix}
				\right\}
				\quad \text{ and } \quad
				\left\{
				\begin{bmatrix}
				1 \\
				2 \\
				3
				\end{bmatrix}
				,
				\begin{bmatrix}
				3 \\
				5 \\
				7
				\end{bmatrix}
				\right\}
			</me>
			are spanning sets for this column space. </p>

		<p> The idea also works in reverse. Suppose we have a bunch of column vectors and we just
			need to find a linearly independent set. For example, suppose we started with the
			vectors <me>
				\vec{v}_1 =
				\begin{bmatrix}
				1 \\
				2 \\
				3
				\end{bmatrix}
				,
				\quad
				\vec{v}_2 =
				\begin{bmatrix}
				2 \\
				4 \\
				6
				\end{bmatrix}
				,
				\quad
				\vec{v}_3 =
				\begin{bmatrix}
				3 \\
				5 \\
				7
				\end{bmatrix}
				,
				\quad
				\vec{v}_4 =
				\begin{bmatrix}
				4 \\
				6 \\
				8
				\end{bmatrix} .
			</me>
			These vectors are not linearly independent as we saw above. In particular, the span <m>
			\vec{v}_1</m> and <m>\vec{v}_3</m> is the same as the span of all four of the vectors.
			So <m>\vec{v}_2</m> and <m>\vec{v}_4</m> can both be written as linear combinations of <m>
			\vec{v}_1</m> and <m>\vec{v}_3</m>. A common thing that comes up in practice is that one
			gets a set of vectors whose span is the set of solutions of some problem. But perhaps we
			get way too many vectors, we want to simplify. For example above, all vectors in the
			span of <m>\vec{v}_1, \vec{v}_2, \vec{v}_3, \vec{v}_4</m> can be written <m>\alpha_1
			\vec{v}_1 + \alpha_2 \vec{v}_2 + \alpha_3 \vec{v}_3 + \alpha_4
				\vec{v}_4</m> for some numbers <m>\alpha_1,\alpha_2,\alpha_3,\alpha_4</m>. But it is
			also true that every such vector can be written as <m>a \vec{v}_1 + b \vec{v}_3</m> for
			two numbers <m>a</m> and <m>b</m>. And one has to admit, that looks much simpler.
			Moreover, these numbers <m>a</m> and <m>b</m> are unique. More on that later in this
			section. </p>

		<p> To find this linearly independent set we simply take our vectors and form the matrix <m>[
			\vec{v}_1 ~ \vec{v}_2 ~ \vec{v}_3 ~ \vec{v}_4 ]</m>, that is, the matrix <me>
			\begin{bmatrix}
				1 &amp; 2 &amp; 3 &amp; 4 \\
				2 &amp; 4 &amp; 5 &amp; 6 \\
				3 &amp; 6 &amp; 7 &amp; 8
				\end{bmatrix} .
			</me>
			We crank up the row-reduction machine, feed this matrix into it, and find the pivot
			columns and pick those. In this case, <m>\vec{v}_1</m> and <m>\vec{v}_3</m>. </p>

	</subsection>

	<subsection xml:id="basis-and-dimension">
		<title>Basis and dimension</title>

		<p>
			At this point, we have talked about subspaces, and two other properties of sets of
			vectors: linear independence and being a spanning set for a subspace. In some sense,
			these two properties are in opposition to each other. If I add more vectors to a set, I
			am more likely to become a spanning set (because I have more options for adding to get
			other vectors), but less likely to be independent (because there are more possibilities
			for a linear combination to be zero). Similarly, the reverse is true; removing vectors
			means the set is more likely to be linearly independent, but less likely to span a given
			subspace. The question then becomes if there is a sweet spot where both things are true,
			and that leads to the definition of a basis.
		</p>

		<!-- div attr= class="definition"-->
		<p> If <m>S</m> is a subspace and we can find <m>k</m> linearly independent vectors in <m>S</m> 
<me>
			\vec{v}_1, \vec{v}_2, \ldots, \vec{v}_k ,
			</me> such that every other vector in <m>S</m>
			is a linear combination of <m>\vec{v}_1,
				\vec{v}_2,\ldots, \vec{v}_k</m>, then the set <m>\{ \vec{v}_1, \vec{v}_2, \ldots,
			\vec{v}_k \}</m> is called a <em></em> of <m>S</m>. In other words, <m>S</m> is the span
			of <m>\{ \vec{v}_1, \vec{v}_2, \ldots, \vec{v}_k \}</m>. We say that <m>S</m> has <em></em> <m>
			k</m>, and we write <me>
				\dim S = k .
			</me>

		</p><!--</div
		attr= class="definition">-->

		<p>
			The next theorem illustrates the main properties and classification of a basis of a
			vector space.
		</p>

		<!-- div attr= class="theorem1"-->
		<p> If <m>S \subset {\mathbb R}^n</m> is a subspace and <m>S</m> is not the trivial subspace <m>\{
			\vec{0} \}</m>, then there exists a unique positive integer <m>k</m> (the dimension) and
			a (not unique) basis <m>\{ \vec{v}_1, \vec{v}_2, \ldots, \vec{v}_k \}</m>, such that
			every <m>\vec{w}</m> in <m>S</m> can be uniquely represented by <me>
				\vec{w} =
				\alpha_1 \vec{v}_1 +
				\alpha_2 \vec{v}_2 +
				\cdots
				+
				\alpha_k \vec{v}_k ,
			</me>
			for some scalars <m>\alpha_1</m>, <m>\alpha_2</m>, â€¦, <m>\alpha_k</m>. </p><!--</div
		attr= class="theorem1">-->

		<p> We should reiterate that while <m>k</m> is unique (a subspace cannot have two different
			dimensions), the set of basis vectors is not at all unique. There are lots of different
			bases for any given subspace. Finding just the right basis for a subspace is a large
			part of what one does in linear algebra. In fact, that is what we spend a lot of time on
			in linear differential equations, although at first glance it may not seem like that is
			what we are doing. </p>


		<example>
		<title> </title>
		<statement>
			<p> The standard basis <me>
					\vec{e}_1, \vec{e}_2, \ldots, \vec{e}_n ,
				</me> is a basis of <m>{\mathbb R}^n</m>
				(hence the name). So as expected <me>
					\dim {\mathbb R}^n = n .
				</me>

			</p>

			<p> On the other hand the subspace <m>\{ \vec{0} \}</m> is of dimension <m>0</m>. </p>

			<p> The subspace <m>S''</m> from a previous example, that is, the set of vectors <m>
				(a,a)</m> is of dimensionÂ 1. One possible basis is simply <m>\{ (1,1) \}</m>, the
				single vector <m>(1,1)</m>: every vector in <m>S''</m> can be represented by <m>a
				(1,1) =
					(a,a)</m>. Similarly another possible basis would be <m>\{ (-1,-1) \}</m>. Then
				the vector <m>(a,a)</m> would be represented as <m>(-a) (-1,-1)</m>. In this case,
				the subspace <m>S''</m> has many different bases, two of which are <m>\{(1,1)\}</m>
				and <m>\{(-1,-1)\}</m>, and the vector <m>(a,a)</m> has a different representation
				(different constant) for the different bases. </p>
		</statement>
	</example>
		<p>
			Row and column spaces of a matrix are also examples of subspaces, as they are given as
			the span of vectors. We can use what we know about row spaces and column spaces from the
			previous section to find a basis.
		</p>


		<example>
		<title> </title>
		<statement>
			<p> Earlier, we considered the matrix <me>
					A =
					\begin{bmatrix}
					1 &amp; 2 &amp; 3 &amp; 4 \\
					2 &amp; 4 &amp; 5 &amp; 6 \\
					3 &amp; 6 &amp; 7 &amp; 8
					\end{bmatrix} .
				</me>
				Using row reduction to find the pivot columns, we found <me>
					\text{column space of $A$} \left(
					\begin{bmatrix}
					1 &amp; 2 &amp; 3 &amp; 4 \\
					2 &amp; 4 &amp; 5 &amp; 6 \\
					3 &amp; 6 &amp; 7 &amp; 8
					\end{bmatrix}
					\right)
					=
					\operatorname{span}
					\left\{
					\begin{bmatrix}
					1 \\
					2 \\
					3
					\end{bmatrix}
					,
					\begin{bmatrix}
					3 \\
					5 \\
					7
					\end{bmatrix}
					\right\} .
				</me>
				What we did was we found the basis of the column space. The basis has two elements,
				and so the column space of <m>A</m> is two dimensional. </p>
		</statement>
	</example>
		<p> We would have followed the same procedure if we wanted to find the basis of the subspace <m>
			X</m> spanned by <me>
				\begin{bmatrix}
				1 \\
				2 \\
				3
				\end{bmatrix}
				,
				\begin{bmatrix}
				2 \\
				4 \\
				6
				\end{bmatrix}
				,
				\begin{bmatrix}
				3 \\
				5 \\
				7
				\end{bmatrix}
				,
				\begin{bmatrix}
				4 \\
				6 \\
				8
				\end{bmatrix}
				.
			</me>
			We would have simply formed the matrix <m>A</m> with these vectors as columns and
			repeated the computation above. The subspace <m>X</m> is then the column space of <m>A</m>
			.
			</p>


		<example>
		<title> </title>
		<statement>
			<p> Consider the matrix <me>
					L =
					\begin{bmatrix}
					{1} &amp; 2 &amp; 0 &amp; 0 &amp; 3 \\
					0 &amp; 0 &amp; {1} &amp; 0 &amp; 4 \\
					0 &amp; 0 &amp; 0 &amp; {1} &amp; 5
					\end{bmatrix}
				</me>
				Conveniently, the matrix is in reduced row echelon form. The column space is the
				span of the pivot columns, because the pivot columns always form a basis for the
				column space of a matrix. It is the 3-dimensional space <me>
					\text{column space of $L$} =
					\operatorname{span} \left\{
					\begin{bmatrix}
					1 \\
					0 \\
					0
					\end{bmatrix}
					,
					\begin{bmatrix}
					0 \\
					1 \\
					0
					\end{bmatrix}
					,
					\begin{bmatrix}
					0 \\
					0 \\
					1
					\end{bmatrix}
					\right\}
					= {\mathbb{R}}^3 .
				</me>
				The row space is the 3-dimensional space <me>
					\text{row space of $L$} =
					\operatorname{span} \left\{
					\begin{bmatrix}
					1 &amp; 2 &amp; 0 &amp; 0 &amp; 3
					\end{bmatrix}
					,
					\begin{bmatrix}
					0 &amp; 0 &amp; 1 &amp; 0 &amp; 4
					\end{bmatrix}
					,
					\begin{bmatrix}
					0 &amp; 0 &amp; 0 &amp; 1 &amp; 5
					\end{bmatrix}
					\right\} .
				</me>
				As these vectors have 5 components, we think of the row space of <m>L</m> as a
				subspace of <m>{\mathbb{R}}^5</m>. </p>
		</statement>
	</example>
	</subsection>

	<subsection xml:id="rank">
		<title>Rank</title>

		<p>
			In that last example, we noticed that the dimension of the row space and the column
			space were the same. It turns out that this is not a coincidence. In order to describe
			this in more detail, we need to define one more term.
		</p>

		<!-- div attr= class="definition"-->
		<p> Given a matrix <m>A</m>, the maximal number of linearly independent rows is called the <em></em>
			of <m>A</m>, and we write for the rank. </p><!--</div
		attr= class="definition">-->

		<p> For example, <me>
				\operatorname{rank}
				\begin{bmatrix}
				1 &amp; 1 &amp; 1 \\
				2 &amp; 2 &amp; 2 \\
				-1 &amp; -1 &amp; -1
				\end{bmatrix}
				=
				1 .
			</me>
			The second and third row are multiples of the first one. We cannot choose more than one
			row and still have a linearly independent set. But what is <me>
				\operatorname{rank}
				\begin{bmatrix}
				1 &amp; 2 &amp; 3 \\
				4 &amp; 5 &amp; 6 \\
				7 &amp; 8 &amp; 9
				\end{bmatrix} \quad = \quad ?
			</me>
			That seems to be a tougher question to answer. The first two rows are linearly
			independent, so the rank is at least two. If we would set up the equations for the <m>
			\alpha_1</m>, <m>\alpha_2</m>, and <m>\alpha_3</m>, we would find a system with
			infinitely many solutions. One solution is <me>
				\begin{bmatrix}
				1 &amp; 2 &amp; 3
				\end{bmatrix} -2
				\begin{bmatrix}
				4 &amp; 5 &amp; 6
				\end{bmatrix} +
				\begin{bmatrix}
				7 &amp; 8 &amp; 9
				\end{bmatrix} =
				\begin{bmatrix}
				0 &amp; 0 &amp; 0
				\end{bmatrix} .
			</me>
			So the set of all three rows is linearly dependent, the rank cannot be 3. Therefore the
			rank is 2. </p>

		<p> But how can we do this in a more systematic way? We find the row echelon form! <me>
			\text{Row echelon form of}
				\quad
				\begin{bmatrix}
				1 &amp; 2 &amp; 3 \\
				4 &amp; 5 &amp; 6 \\
				7 &amp; 8 &amp; 9
				\end{bmatrix}
				\quad
				\text{is}
				\quad
				\begin{bmatrix}
				1 &amp; 2 &amp; 3 \\
				0 &amp; 1 &amp; 2 \\
				0 &amp; 0 &amp; 0
				\end{bmatrix} .
			</me>
			The elementary row operations do not change the set of linear combinations of the rows
			(that was one of the main reasons for defining them as they were). In other words, the
			span of the rows of the <m>A</m> is the same as the span of the rows of the row echelon
			form of <m>A</m>. In particular, the number of linearly independent rows is the same.
			And in the row echelon form, all nonzero rows are linearly independent. This is not hard
			to see. Consider the two nonzero rows in the example above. Suppose we tried to solve
			for the <m>\alpha_1</m> and <m>\alpha_2</m> in <me>
				\alpha_1
				\begin{bmatrix}
				1 &amp; 2 &amp; 3
				\end{bmatrix}
				+
				\alpha_2
				\begin{bmatrix}
				0 &amp; 1 &amp; 2
				\end{bmatrix} =
				\begin{bmatrix}
				0 &amp; 0 &amp; 0
				\end{bmatrix} .
			</me>
			Since the first column of the row echelon matrix has zeros except in the first row means
			that <m>\alpha_1 = 0</m>. For the same reason, <m>\alpha_2</m> is zero. We only have two
			nonzero rows, and they are linearly independent, so the rank of the matrix is 2. This
			also tells us that if we were trying to solve the system of equations <me>
				\begin{split}
				x_1 + 2x_2 + 3x_3 &amp;= a \\
				4x_1 + 5x_2 + 6x_3 &amp;= b \\
				7x_1 + 8x_2 + 9x_3 &amp;= c
				\end{split}
			</me>
			we would get that one row of the reduced augmented matrix has all zeros on the left
			side, and so this system either has a free variable or is inconsistent, because only two
			equations here are relevant. </p>

		<p> Referring back to the examples from earlier in this section, we could carry out the same
			calculations to say that <me>
				\text{rank } \begin{bmatrix} 1 &amp; 2 &amp; 3 \\ 4 &amp; 5 &amp; 6 \\ 7 &amp; 8
			&amp; 9 \end{bmatrix} = 2
			</me>
			and <me>
				\text{rank } \begin{bmatrix} 1 &amp; 2 &amp; 0 &amp; 0 &amp; 3 \\ 0 &amp; 0 &amp; 1
			&amp; 0 &amp; 4 \\0 &amp; 0 &amp; 0 &amp; 1 &amp; 5 \end{bmatrix} = 3.
			</me>

		</p>

		<p>
			We know how to find the set of linearly independent rows, but sometimes it may also be
			useful to find the linearly independent columns as well. It is a tremendously useful
			fact that the number of linearly independent columns is always the same as the number of
			linearly independent rows:
		</p>

		<!-- div attr= class="theorem1"-->
		<p>
			<m>\operatorname{rank} A = \operatorname{rank} A^T</m>
		</p><!--</div
		attr= class="theorem1">-->

		<p>
			Or, in the context of the row and column spaces that we have already discussed:
		</p>

		<!-- div attr= class="theorem1"-->
		<p> Rank The dimension of the column space and the dimension of the row space of a matrix <m>
			A</m> are both equal to the rank of <m>A</m>. </p><!--</div
		attr= class="theorem1">-->

		<p>
			This relates to the statement at the start of this section; since the number of vectors
			that we needed to take to get a basis of linearly independent columns was always the
			same as the number of pivots, and the number of pivots is the rank, we get that above
			theorem.
		</p>

	</subsection>

	<subsection xml:id="kernel">
		<title>Kernel</title>

		<p> The set of solutions of a linear equation <m>L\vec{x} = \vec{0}</m>, the kernel of <m>L</m>,
			is a subspace: If <m>\vec{x}</m> and <m>\vec{y}</m> are solutions, then <me>
			L(\vec{x}+\vec{y}) =
				L\vec{x}+L\vec{y} =
				\vec{0}+\vec{0} = \vec{0} ,
				\qquad \text{and} \qquad
				L(\alpha \vec{x}) =
				\alpha L \vec{x} =
				\alpha \vec{0} = \vec{0}.
			</me>
			So <m>\vec{x}+\vec{y}</m> and <m>\alpha \vec{x}</m> are solutions. The dimension of the
			kernel is called the <em></em> of the matrix. </p>

		<p>
			The same sort of idea governs the solutions of linear differential equations. We try to
			describe the kernel of a linear differential operator, and as it is a subspace, we look
			for a basis of this kernel. Much of this book is dedicated to finding such bases.
		</p>

		<p> The kernel of a matrix is the same as the kernel of its reduced row echelon form. For a
			matrix in reduced row echelon form, the kernel is rather easy to find. If a vector <m>
			\vec{x}</m> is applied to a matrix <m>L</m>, then each entry in <m>\vec{x}</m>
			corresponds to a column of <m>L</m>, the column that the entry multiplies. To find the
			kernel, pick a non-pivot column make a vector that has a <m>-1</m> in the entry
			corresponding to this non-pivot column and zeros at all the other entries corresponding
			to the other non-pivot columns. Then for all the entries corresponding to pivot columns
			make it precisely the value in the corresponding row of the non-pivot column to make the
			vector be a solution to <m>L \vec{x} = \vec{0}</m>. This procedure is best understood by
			example. </p>


		<example>
		<title> </title>
		<statement>
			<p> Consider <me>
					L =
					\begin{bmatrix}
					\mybxsm{1} &amp; 2 &amp; 0 &amp; 0 &amp; 3 \\
					0 &amp; 0 &amp; \mybxsm{1} &amp; 0 &amp; 4 \\
					0 &amp; 0 &amp; 0 &amp; \mybxsm{1} &amp; 5
					\end{bmatrix} .
				</me>
				This matrix is in reduced row echelon form, the pivots are marked. There are two
				non-pivot columns, so the kernel has dimension 2, that is, it is the span of 2
				vectors. Let us find the first vector. We look at the first non-pivot column, the <m>
				2^{\text{nd}}</m> column, and we put a <m>-1</m> in the <m>2^{\text{nd}}</m> entry
				of our vector. We put a <m>0</m> in the <m>5^{\text{th}}</m> entry as the <m>
				5^{\text{th}}</m> column is also a non-pivot column: <me>
					\begin{bmatrix}
					? \\ -1 \\ ? \\ ? \\ 0
					\end{bmatrix} .
				</me> Let us fill the
				rest. When this vector hits the first row, we get a <m>-2</m> and <m>1</m> times
				whatever the first question mark is. So make the first question mark <m>2</m>. For
				the second and third rows, it is sufficient to make it the question marks zero. We
				are really filling in the non-pivot column into the remaining entries. Let us check
				while marking which numbers went where: <me>
					\begin{bmatrix}
					1 &amp; \mybxsm{2} &amp; 0 &amp; 0 &amp; 3 \\
					0 &amp; \mybxsm{0} &amp; 1 &amp; 0 &amp; 4 \\
					0 &amp; \mybxsm{0} &amp; 0 &amp; 1 &amp; 5
					\end{bmatrix}
					\begin{bmatrix}
					\mybxsm{2} \\ -1 \\ \mybxsm{0} \\ \mybxsm{0} \\ 0
					\end{bmatrix}
					=
					\begin{bmatrix}
					0 \\ 0 \\ 0
					\end{bmatrix}
					.
				</me>
				Yay! How about the second vector. We start with <me>
					\begin{bmatrix}
					? \\ 0 \\ ? \\ ? \\ -1 .
					\end{bmatrix}
				</me> We set the first
				question mark to 3, the second to 4, and the third to 5. Let us check, marking
				things as previously, <me>
					\begin{bmatrix}
					1 &amp; 2 &amp; 0 &amp; 0 &amp; \mybxsm{3} \\
					0 &amp; 0 &amp; 1 &amp; 0 &amp; \mybxsm{4} \\
					0 &amp; 0 &amp; 0 &amp; 1 &amp; \mybxsm{5}
					\end{bmatrix}
					\begin{bmatrix}
					\mybxsm{3} \\ 0 \\ \mybxsm{4} \\ \mybxsm{5} \\ -1
					\end{bmatrix}
					=
					\begin{bmatrix}
					0 \\ 0 \\ 0
					\end{bmatrix}
					.
				</me>
				There are two non-pivot columns, so we only need two vectors. We have found the
				basis of the kernel. So, <me>
					\text{kernel of $L$} =
					\operatorname{span} \left\{
					\begin{bmatrix}
					2 \\ -1 \\ 0 \\ 0 \\ 0
					\end{bmatrix}
					,
					\begin{bmatrix}
					3 \\ 0 \\ 4 \\ 5 \\ -1
					\end{bmatrix}
					\right\}
				</me>

			</p>
		</statement>
	</example>
		<p> What we did in finding a basis of the kernel is we expressed all solutions of <m>L
			\vec{x} = \vec{0}</m> as a linear combination of some given vectors. </p>

		<p> The procedure to find the basis of the kernel of a matrix <m>L</m>: </p>

		<p>
			<ol>
				<li>
					<p> Find the reduced row echelon form of <m>L</m>. </p>
				</li>

				<li>
					<p>
						Write down the basis of the kernel as above, one vector for each non-pivot
						column.
					</p>
				</li>

			</ol>
		</p>

		<p>
			The rank of a matrix is the dimension of the column space, and that is the span of the
			pivot columns, while the kernel is the span of vectors in the non-pivot columns. So the
			two numbers must add to the number of columns.
		</p>

		<!-- div attr= class="theorem1"-->
		<p> Rankâ€“Nullity If a matrix <m>A</m> has <m>n</m> columns, rank <m>r</m>, and nullity <m>k</m>
			(dimension of the kernel), then <me>
				n = r+k .
			</me>

		</p><!--</div
		attr= class="theorem1">-->

		<p> The theorem is immensely useful in applications. It allows one to compute the rank <m>r</m>
			if one knows the nullity <m>k</m> and vice versa, without doing any extra work. </p>

		<p> Let us consider an example application, a simple version of the so-called <em></em>. A
			similar result is true for differential equations. Consider <me>
				A \vec{x} = \vec{b} ,
			</me> where <m>A</m> is a square <m>n \times n</m> matrix.
			There are then two mutually exclusive possibilities: </p>

		<p>
			<ol>
				<li>
					<p> A nonzero solution <m>\vec{x}</m> to <m>A \vec{x} = \vec{0}</m> exists. </p>
				</li>

				<li>
					<p> The equation <m>A \vec{x} = \vec{b}</m> has a unique solution <m>\vec{x}</m>
						for every <m>\vec{b}</m>. </p>
				</li>

			</ol>
		</p>

		<p> How does the Rankâ€“Nullity theorem come into the picture? Well, if <m>A</m> has a nonzero
			solution <m>\vec{x}</m> to <m>A \vec{x} = \vec{0}</m>, then the nullity <m>k</m> is
			positive. But then the rank <m>r = n-k</m> must be less than <m>n</m>. In particular it
			means that the column space of <m>A</m> is of dimension less than <m>n</m>, so it is a
			subspace that does not include everything in <m>{\mathbb{R}}^n</m>. So <m>{\mathbb{R}}^n</m>
			has to contain some vector <m>\vec{b}</m> not in the column space of <m>A</m>. In fact,
			most vectors in <m>{\mathbb{R}}^n</m> are not in the column space of <m>A</m>. </p>

		<p> The idea of a kernel also comes up when defining and discussing eigenvectors. In order
			to find this vector, we are looking for a vector <m>\vec{v}</m> so that <me>(A - \lambda
			I)\vec{v} = \vec{0}.</me> This means that we are looking for a vector <m>\vec{v}</m>
			that is in the kernel of the matrix <m>(A - \lambda I)</m>. Since the kernel is also a
			subspace, this means that the set of all eigenvectors of a matrix <m>A</m> with a
			certain eigenvalue is a subspace, so it has a dimension. This dimension is number of
			linearly independent eigenvectors with that eigenvalue, so it is the geometric
			multiplicity of this eigenvalue. This also motivates why this is sometimes called the <em>
			eigenspace</em> for a given eigenvalue. Finding a basis of this subspace (which is also
			finding the kernel of the matrix <m>A - \lambda I</m> ) is the exact same as the process
			of finding the eigenvectors of the matrix <m>A</m>. </p>

	</subsection>

	<subsection xml:id="computing-the-inverse">
		<title>Computing the inverse</title>

		<p> If the matrix <m>A</m> is square and there exists a unique solution <m>\vec{x}</m> to <m>A
			\vec{x} = \vec{b}</m> for any <m>\vec{b}</m> (there are no free variables), then <m>A</m>
			is invertible. </p>

		<p> In particular, if <m>A \vec{x} = \vec{b}</m> then <m>\vec{x} = A^{-1} \vec{b}</m>. Now
			we just need to compute what <m>A^{-1}</m> is. We can surely do elimination every time
			we want to find <m>A^{-1} \vec{b}</m>, but that would be ridiculous. The mapping <m>
			A^{-1}</m> is linear and hence given by a matrix, and we have seen that to figure out
			the matrix we just need to find where does <m>A^{-1}</m> take the standard basis vectors <m>
			\vec{e}_1</m>, <m>\vec{e}_2</m>, â€¦, <m>\vec{e}_n</m>. </p>

		<p> That is, to find the first column of <m>A^{-1}</m> we solve <m>A \vec{x} = \vec{e}_1</m>,
			because then <m>A^{-1} \vec{e}_1 = \vec{x}</m>. To find the second column of <m>A^{-1}</m>
			we solve <m>A \vec{x} = \vec{e}_2</m>. And so on. It is really just <m>n</m>
			eliminations that we need to do. But it gets even easier. If you think about it, the
			elimination is the same for everything on the left side of the augmented matrix. Doing <m>
			n</m> eliminations separately we would redo most of the computations. Best is to do all
			at once. </p>

		<p> Therefore, to find the inverse of <m>A</m>, we write an <m>n
				\times 2n</m> augmented matrix <m>[ \,A ~|~ I\, ]</m>, where <m>I</m> is the
			identity matrix, whose columns are precisely the standard basis vectors. We then perform
			row reduction until we arrive at the reduced row echelon form. If <m>A</m> is
			invertible, then pivots can be found in every column of <m>A</m>, and so the reduced row
			echelon form of <m>[ \,A ~|~ I\, ]</m> looks like <m>[ \,I ~|~ A^{-1}\, ]</m>. We then
			just read off the inverse <m>A^{-1}</m>. If you do not find a pivot in every one of the
			first <m>n</m> columns of the augmented matrix, then <m>A</m> is not invertible. </p>

		<p>
			This is best seen by example.
		</p>


		<example>
			<title> </title>
			<statement>
				<p> Find the inverse of the matrix <me>
						\begin{bmatrix}
						1 &amp; 2 &amp; 3 \\
						2 &amp; 0 &amp; 1 \\
						3 &amp; 1 &amp; 0
						\end{bmatrix} .
					</me>

				</p>
			</statement>

			<solution>
				<p> We write the augmented matrix and we start reducing: <me>\begin{align*}
						&amp; \left[
						\begin{array}{ccc|ccc}
						\mybxsm{1} &amp; 2 &amp; 3 &amp; 1 &amp; 0 &amp; 0\\
						2 &amp; 0 &amp; 1 &amp; 0 &amp; 1 &amp; 0 \\
						3 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 1
						\end{array}
						\right]
						\to
						\left[
						\begin{array}{ccc|ccc}
						\mybxsm{1} &amp; 2 &amp; 3 &amp; 1 &amp; 0 &amp; 0\\
						0 &amp; -4 &amp; -5 &amp; -2 &amp; 1 &amp; 0 \\
						0 &amp; -5 &amp; -9 &amp; -3 &amp; 0 &amp; 1
						\end{array}
						\right]
						\to
						\left[
						\begin{array}{ccc|ccc}
						\mybxsm{1} &amp; 2 &amp; 3 &amp; 1 &amp; 0 &amp; 0\\
						0 &amp; \mybxsm{1} &amp; \nicefrac{5}{4} &amp; \nicefrac{1}{2} &amp;
					\nicefrac{1}{4} &amp; 0 \\
						0 &amp; -5 &amp; -9 &amp; -3 &amp; 0 &amp; 1
						\end{array}
						\right]
						\to \\
						\to
						&amp;
						\left[
						\begin{array}{ccc|ccc}
						\mybxsm{1} &amp; 2 &amp; 3 &amp; 1 &amp; 0 &amp; 0\\
						0 &amp; \mybxsm{1} &amp; \nicefrac{5}{4} &amp; \nicefrac{1}{2} &amp;
					\nicefrac{1}{4} &amp; 0 \\
						0 &amp; 0 &amp; \nicefrac{-11}{4} &amp; \nicefrac{-1}{2} &amp;
					\nicefrac{-5}{4} &amp; 1
						\end{array}
						\right]
						\to
						\left[
						\begin{array}{ccc|ccc}
						\mybxsm{1} &amp; 2 &amp; 3 &amp; 1 &amp; 0 &amp; 0\\
						0 &amp; \mybxsm{1} &amp; \nicefrac{5}{4} &amp; \nicefrac{1}{2} &amp;
					\nicefrac{1}{4} &amp; 0 \\
						0 &amp; 0 &amp; \mybxsm{1} &amp; \nicefrac{2}{11} &amp; \nicefrac{5}{11}
					&amp; \nicefrac{-4}{11}
						\end{array}
						\right]
						\to
						\\
						\to
						&amp;
						\left[
						\begin{array}{ccc|ccc}
						\mybxsm{1} &amp; 2 &amp; 0 &amp; \nicefrac{5}{11} &amp; \nicefrac{-5}{11}
					&amp; \nicefrac{12}{11} \\
						0 &amp; \mybxsm{1} &amp; 0 &amp; \nicefrac{3}{11} &amp; \nicefrac{-9}{11}
					&amp; \nicefrac{5}{11} \\
						0 &amp; 0 &amp; \mybxsm{1} &amp; \nicefrac{2}{11} &amp; \nicefrac{5}{11}
					&amp; \nicefrac{-4}{11}
						\end{array}
						\right]
						\to
						\left[
						\begin{array}{ccc|ccc}
						\mybxsm{1} &amp; 0 &amp; 0 &amp; \nicefrac{-1}{11} &amp; \nicefrac{3}{11}
					&amp; \nicefrac{2}{11} \\
						0 &amp; \mybxsm{1} &amp; 0 &amp; \nicefrac{3}{11} &amp; \nicefrac{-9}{11}
					&amp; \nicefrac{5}{11} \\
						0 &amp; 0 &amp; \mybxsm{1} &amp; \nicefrac{2}{11} &amp; \nicefrac{5}{11}
					&amp; \nicefrac{-4}{11}
						\end{array}
						\right] .
						\end{align*}</me> So <me>
						{\begin{bmatrix}
						1 &amp; 2 &amp; 3 \\
						2 &amp; 0 &amp; 1 \\
						3 &amp; 1 &amp; 0
						\end{bmatrix}}^{-1}
						=
						\begin{bmatrix}
						\nicefrac{-1}{11} &amp; \nicefrac{3}{11} &amp; \nicefrac{2}{11} \\
					\nicefrac{3}{11} &amp; \nicefrac{-9}{11} &amp; \nicefrac{5}{11} \\
					\nicefrac{2}{11} &amp; \nicefrac{5}{11} &amp; \nicefrac{-4}{11}
						\end{bmatrix} .
					</me>

				</p>
			</solution>
		</example>
		<p> Not too terrible, no? Perhaps harder than inverting a <m>2 \times 2</m> matrix for which
			we had a formula, but not too bad. Really in practice this is done efficiently by a
			computer. </p>

	</subsection>

	<subsection xml:id="trace-and-determinant-of-matrices">
		<title>Trace and Determinant of Matrices</title>

		<p>
			The next thing to add into our toolbox of matrices is the idea of the trace of a matrix,
			and how it and the determinant relate to the eigenvalues of said matrix.
		</p>

		<!-- div attr= class="definition"-->
		<p> Let <m>A</m> be an <m>n \times n</m> square matrix. The <em></em> of <m>A</m> is the sum
			of all diagonal entries of <m>A</m>. </p><!--</div
		attr= class="definition">-->

		<p> For example, if we have the matrix <me>
				\begin{bmatrix}
				1 &amp; 4 &amp; -2 \\
				3 &amp; 2 &amp; 5 \\
				0 &amp; 1 &amp; 3
				\end{bmatrix}
			</me>
			the trace is <m>1 + 2 + 3 = 6</m>. </p>

		<p> The trace is important in our context because it also tells us something about the
			eigenvalues of a matrix. To work this out, letâ€™s consider the generic <m>2\times 2</m>
			matrix and how we would find the eigenvalues. If we have a <m>2 \times 2</m> matrix of
			the form <me>
				A = \begin{bmatrix} a &amp; b \\ c &amp; d
				\end{bmatrix}
			</me> we can write out the
			expression <m>\det(A - \lambda I)</m> in order to find the eigenvalues. In this case, we
			would get <me>
				\det(A - \lambda I) = \det\left( \begin{bmatrix} a - \lambda &amp; b \\ c &amp; d -
			\lambda \end{bmatrix} \right) = (a-\lambda)(d-\lambda) - bc = \lambda^2 - (a+d)\lambda +
			(ad - bc).
			</me>

		</p>

		<p> However, the coefficients in this polynomial look familiar. <m>(ad-bc)</m> is just the
			determinant of the matrix <m>A</m>, and <m>a+d</m> is the trace. Therefore, for any <m>2
			\times 2</m> matrix, we could write the as <men xml:id="eq-CharPoly1">
				\det(A - \lambda I) = \lambda^2 - T\lambda + D
			</men> where <m>T</m> is the trace of
			the matrix and <m>D</m> is the determinant. On the other hand, assume that <m>r_1</m>
			and <m>r_2</m> are the two eigenvalues of this matrix (whether they be real, complex, or
			repeated). In that case, we know that this polynomial has <m>r_1</m> and <m>r_2</m> as
			roots. Therefore, it is equal to <men xml:id="eq-CharPoly2">
				\det(A - \lambda I) = (\lambda - r_1)(\lambda - r_2) = \lambda^2 - (r_1 +
			r_2)\lambda + r_1r_2.
			</men>
		</p>

		<p> Matching up the coefficient of <m>\lambda</m> and the constant term in <xref
				ref="eq-CharPoly1" /> and <xref ref="eq-CharPoly2" /> gives the relation that <me>
				T = r_1 + r_2 \qquad D = r_1r_2,
			</me> that is, the trace of the matrix is the sum of
			the eigenvalues, and the determinant of the matrix is the product of the eigenvalues. We
			only showed this fact for <m>2 \times 2</m> matrices, but it does hold for matrices of
			all sizes, giving us the following theorem. </p>

		<!-- div attr= class="theorem"-->
		<p> Let <m>A</m> be an <m>n \times n</m> square matrix with eigenvalues <m>\lambda_1,\
			\lambda_2,\ ..., \lambda_n</m>, written with multiplicity if needed. Then </p>

		<p>
			<ol>
				<li>
					<p> The trace of <m>A</m> is <m>\lambda_1 + \lambda_2 + \cdots + \lambda_n</m>. </p>
				</li>

				<li>
					<p> The determinant of <m>A</m> is <m>(\lambda_1)(\lambda_2)\cdots(\lambda_n)</m>
						.
							</p>
				</li>

			</ol>
		</p><!--</div
		attr= class="theorem">-->

		<p>
			From the above statement, we note that if any of the eigenvalues is zero, the product of
			all eigenvalues will be zero, and so the matrix will have zero determinant. This gives
			an extra follow-up fact, and addition to .
		</p>

		<!-- div attr= class="theorem"-->
		<p> A matrix <m>A</m> is invertible if and only if all of itâ€™s eigenvalues are non-zero. </p><!--</div
		attr= class="theorem">-->


		<example>
			<title> </title>
			<statement>
				<p> Use the facts above to analyze the eigenvalues of the matrix <me>
						A = \begin{bmatrix} 1 &amp; 2 \\ 5 &amp; 4 \end{bmatrix}.
					</me>

				</p>
			</statement>

			<solution>
				<p> From the matrix <m>A</m>, we can compute that the trace of <m>A</m> is <m>1+4=5</m>,
					and the determinant is <m>(1)(4) - (2)(5) = -6</m>. Based on the theorem above,
					we know that the two eigenvalues of this matrix must add to <m>5</m> and
					multiply to <m>-6</m>. While you could probably guess the numbers here, the
					important take-aways from this example are what we can learn. </p>

				<p> The main fact to point out is that this is enough information, in the <m>2
					\times 2</m> case, to tell us that the eigenvalues have to be real and distinct.
					Since their product is a negative number, we can eliminate the other two
					options. If we have two complex roots, they must be of the form <m>x + iy</m>
					and <m>x-iy</m>, and so the product is <me>
						(x+iy)(x-iy) = x^2 + ixy - ixy - i^2y^2 = x^2 + y^2
					</me> which is always
					positive, no matter what <m>x</m> and <m>y</m> are. Similarly, if we have a
					repeated eigvalue, the product will be that number squared, which is also
					positive. Therefore, if the determinant of a <m>2 \times 2</m> matrix is
					negative, the eigenvalues must be real and distinct, with one being positive and
					one negative (otherwise the product can not be negative). These facts will be
					important when we start to analyze the solutions to systems of differential
					equations in . </p>
			</solution>
		</example>

		<example>
			<title> </title>
			<statement>
				<p> What can be said about the eigenvalues of the matrix <me>
						A = \begin{bmatrix}
						0 &amp; -1 &amp; 0 \\
						2 &amp; 2 &amp; 0 \\
						-7 &amp;-3 &amp; -1
						\end{bmatrix}?
					</me>

				</p>
			</statement>

			<solution>
				<p> We can find the same information as the previous example. The trace of <m>A</m>
					is <m>1</m>, and the determinant, by cofactor expansion along column 3, is <m>(-1)(0
					+ 2) = -2</m>. Therefore, the sum of the <em>three</em> eigenvalues is <m>1</m>,
					and the product of them is <m>-2</m>. We donâ€™t actually have enough information
					here to determine what the eigenvalues are. The issue is that with three
					eigenvalues, there are many different ways to get to a product being negative.
					There could be three negative eigenvalues, two positive and one negative, or one
					negative real with two complex eigenvalues. However, the one thing we do know
					for sure is that there must be one negative real eigenvalue. For this particular
					example, we can compute that the eigenvalues are <m>-1</m>, <m>1+i</m>, and <m>
					1-i</m>, so we did end up in the complex case. </p>
			</solution>
		</example>
		<exercise>
			<statement>
				<p> Imagine that we have a <m>3 \times 3</m> matrix with a positive determinant (it
					doesnâ€™t matter what the trace is). Think about all the scenarios and verify that
					at least one eigenvalue must be real and positive for this to happen. </p>
			</statement>

		</exercise>
	</subsection>

	<subsection xml:id="extension-of-previous-theorem">
		<title>Extension of Previous Theorem</title>

		<p>
			With all of the new definitions and properties that have been stated, we can add a few
			more equivalent statements to .
		</p>

		<!-- div attr= class="theorem1"-->
		<p> Let <m>A</m> be an <m>n \times n</m> matrix. The following are equivalent: </p>

		<p>
			<ol>
				<li>
					<p>
						<m>A</m> is invertible. </p>
				</li>

				<li>
					<p>
						<m>\det(A) \neq 0</m>. </p>
				</li>

				<li>
					<p> The rank of <m>A</m> is <m>n</m>. </p>
				</li>

				<li>
					<p> The rows of <m>A</m> are linearly independent. </p>
				</li>

				<li>
					<p> The nullity of the matrix is <m>0</m>. </p>
				</li>

				<li>
					<p> None of the eigenvalues of <m>A</m> are <m>0</m>, or equivalently, the
						product of the eigenvalues of <m>A</m> is non-zero. </p>
				</li>

				<li>
					<p> The columns of <m>A</m> are a basis of <m>\R^n</m>. </p>
				</li>

				<li>
					<p> The rows of <m>A</m> are a basis of <m>\R^n</m>. </p>
				</li>

			</ol>
		</p><!--</div
		attr= class="theorem1">-->

		<!-- div attr= class="proof"-->
		<p>
			<em>Proof.</em> Most of these follow from the components of . If <m>A</m> is invertible,
			then we know that the columns are linearly independent. But there are <m>n</m> columns,
			so that number must be the rank. This implies that the rows are linearly independent,
			and if the rank plus the nullity must be <m>n</m>, we must have the nullity equal to
			zero. On that same train of thought, if we have <m>n</m> linearly independent vectors in <m>
			\R^n</m>, then they must be a basis, giving (k) and (l). Finally, since the determinant
			is the product of the eigenvalues, if the determinant is non-zero, that implies fact
			(j).Â â—» </p><!--</div
		attr= class="proof">-->

	</subsection>

	<exercises>
		<title>Exercises</title>

		<exercise>
			<statement>
				<p>
					For the following matrices, find a basis for the kernel (nullspace).
				</p>

				<!-- div attr= class="tasks"-->
				<p> (4) <m>\begin{bmatrix}
						1 &amp; 1 &amp; 1 \\
						1 &amp; 1 &amp; 5 \\
						1 &amp; 1 &amp; -4
						\end{bmatrix}</m> <m>\begin{bmatrix}
						2 &amp; -1 &amp; -3 \\
						4 &amp; 0 &amp; -4 \\
						-1 &amp; 1 &amp; 2
						\end{bmatrix}</m> <m>\begin{bmatrix}
						-4 &amp; 4 &amp; 4 \\
						-1 &amp; 1 &amp; 1 \\
						-5 &amp; 5 &amp; 5
						\end{bmatrix}</m> <m>\begin{bmatrix}
						-2 &amp; 1 &amp; 1 &amp; 1 \\
						-4 &amp; 2 &amp; 2 &amp; 2 \\
						1 &amp; 0 &amp; 4 &amp; 3
						\end{bmatrix}</m>
				</p><!--</div
				attr= class="tasks">-->
			</statement>

			<answer>
				<p> a)Â  <m>\left\{\left[\begin{smallmatrix} -1 \\ 1 \\ 0
					\end{smallmatrix}\right]\right\}</m> b)Â  <m>\left\{ \left[\begin{smallmatrix} -1
					\\ 1 \\-1 \end{smallmatrix}\right] \right\}</m> c)Â  <m>\left\{\left[\begin{smallmatrix}
					1 \\1 \\0 \end{smallmatrix}\right],\ \left[\begin{smallmatrix} 0 \\ -1 \\ 1
					\end{smallmatrix}\right]\right\}</m> d)Â  <m>\left\{ \left[\begin{smallmatrix} -3
					\\ -7 \\ 0 \\ 1 \end{smallmatrix}\right],\ \left[\begin{smallmatrix} -4 \\ -9 \\
					1 \\ 0 \end{smallmatrix}\right]\right\}</m>
				</p>
			</answer>
		</exercise>

		<exercise>
			<statement>
				<p>
					For the following matrices, find a basis for the kernel (nullspace).
				</p>

				<!-- div attr= class="tasks"-->
				<p> (4) <m>\begin{bmatrix}
						2 &amp; 6 &amp; 1 &amp; 9 \\
						1 &amp; 3 &amp; 2 &amp; 9 \\
						3 &amp; 9 &amp; 0 &amp; 9
						\end{bmatrix}</m> <m>\begin{bmatrix}
						2 &amp; -2 &amp; -5 \\
						-1 &amp; 1 &amp; 5 \\
						-5 &amp; 5 &amp; -3
						\end{bmatrix}</m> <m>\begin{bmatrix}
						1 &amp; -5 &amp; -4 \\
						2 &amp; 3 &amp; 5 \\
						-3 &amp; 5 &amp; 2
						\end{bmatrix}</m> <m>\begin{bmatrix}
						0 &amp; 4 &amp; 4 \\
						0 &amp; 1 &amp; 1 \\
						0 &amp; 5 &amp; 5
						\end{bmatrix}</m>
				</p><!--</div
				attr= class="tasks">-->
			</statement>

		</exercise>
		<exercise>
			<statement>
				<p> Suppose a <m>5 \times 5</m> matrix <m>A</m> has rank 3. What is the nullity? </p>
			</statement>

			<answer>
				<p>
					2
				</p>
			</answer>
		</exercise>

		<exercise>
			<statement>
				<p> Consider a square matrix <m>A</m>, and suppose that <m>\vec{x}</m> is a nonzero
					vector such that <m>A \vec{x} = \vec{0}</m>. What does the Fredholm alternative
					say about invertibility of <m>A</m>? </p>
			</statement>

			<answer>
				<p>
					<m>A</m> must be non-invertible. </p>
			</answer>
		</exercise>

		<exercise>
			<statement>
				<p> Compute the rank of the matrix <m>A</m> below. <me>A = \begin{bmatrix} 0 &amp;
					-3 &amp; 2 &amp; 4 \\ -5 &amp; -4 &amp;-5 &amp; -1 \\ 1&amp;4&amp;-3 &amp; -5\\
					-2 &amp; -3 &amp;-2&amp;1\end{bmatrix}</me> What does this tell you about the
					invertibility of <m>A</m>? How about the solutions to <m>A\vec{x} = \vec{0}</m>? </p>
			</statement>

		</exercise>
		<exercise>
			<statement>
				<p> Compute the rank of the matrix <m>A</m> below. <me>A = \begin{bmatrix} 3 &amp;
					-5 &amp; 5 \\ 2 &amp;-3 &amp; 3\\ 4 &amp; 0 &amp; -1 \end{bmatrix}</me> What
					does this tell you about the invertibility of <m>A</m>? How about the solutions
					to <m>A\vec{x} = \begin{bmatrix} 1\\1\\1 \end{bmatrix}</m>? </p>
			</statement>

		</exercise>
		<exercise>
			<statement>
				<p> Consider <me>
						M =
						\begin{bmatrix}
						1 &amp; 2 &amp; 3 \\
						2 &amp; ? &amp; ? \\
						-1 &amp; ? &amp; ?
						\end{bmatrix} .
					</me>
					If the nullity of this matrix is 2, fill in the question marks. Hint: What is
					the rank? </p>
			</statement>

			<answer>
				<p>
					<m>M = \left[\begin{smallmatrix} 1 &amp; 2 &amp; 3 \\ 2 &amp; 4 &amp; 6 \\ -1
						&amp; -2 &amp; -3 \end{smallmatrix}\right]</m>
				</p>
			</answer>
		</exercise>

		<exercise>
			<statement>
				<p> Suppose the column space of a <m>9 \times 5</m> matrix <m>A</m> of dimension 3.
					Find </p>

				<!-- div attr= class="tasks"-->
				<p> (2) Rank of <m>A</m>. Nullity of <m>A</m>. Dimension of the row space of <m>A</m>.
					Dimension of the nullspace of <m>A</m>. Size of the maximum subset of linearly
					independent rows of <m>A</m>. </p><!--</div
				attr= class="tasks">-->
			</statement>

		</exercise>
		<exercise>
			<statement>
				<p>
					Compute the rank of the given matrices
				</p>

				<!-- div attr= class="tasks"-->
				<p> (3) <m>\begin{bmatrix}
						6 &amp; 3 &amp; 5 \\
						1 &amp; 4 &amp; 1 \\
						7 &amp; 7 &amp; 6
						\end{bmatrix}</m> <m>\begin{bmatrix}
						5 &amp; -2 &amp; -1 \\
						3 &amp; 0 &amp; 6 \\
						2 &amp; 4 &amp; 5
						\end{bmatrix}</m> <m>\begin{bmatrix}
						1 &amp; 2 &amp; 3 \\
						-1 &amp; -2 &amp; -3 \\
						2 &amp; 4 &amp; 6
						\end{bmatrix}</m>
				</p><!--</div
				attr= class="tasks">-->
			</statement>

			<answer>
				<p>
					a)Â  2 b)Â  3 c)Â  1
				</p>
			</answer>
		</exercise>

		<exercise>
			<statement>
				<p>
					Compute the rank of the given matrices
				</p>

				<!-- div attr= class="tasks"-->
				<p> (3) <m>\begin{bmatrix}
						7 &amp; -1 &amp; 6 \\
						7 &amp; 7 &amp; 7 \\
						7 &amp; 6 &amp; 2
						\end{bmatrix}</m> <m>\begin{bmatrix}
						1 &amp; 1 &amp; 1 \\
						1 &amp; 1 &amp; 1 \\
						2 &amp; 2 &amp; 2
						\end{bmatrix}</m> <m>\begin{bmatrix}
						0 &amp; 3 &amp; -1 \\
						6 &amp; 3 &amp; 1 \\
						4 &amp; 7 &amp; -1
						\end{bmatrix}</m>
				</p><!--</div
				attr= class="tasks">-->
			</statement>

		</exercise>
		<exercise>
			<statement>
				<p>
					For the matrices in , find a linearly independent set of row vectors that span
					the row space (they donâ€™t need to be rows of the matrix).
				</p>
			</statement>

			<answer>
				<p> a)Â  <m>[1, 4, 1],\ [0, -21, 1]</m> b)Â <m>[1,0,0],\ [0,1,0],\ [0,0,1]</m> c)Â <m>
					[1,2,3]</m>
				</p>
			</answer>
		</exercise>

		<exercise>
			<statement>
				<p>
					For the matrices in , find a linearly independent set of columns that span the
					column space. That is, find the pivot columns of the matrices.
				</p>
			</statement>

			<answer>
				<p> a)Â <m>\left[\begin{smallmatrix} 6 \\ 1 \\ 7 \end{smallmatrix}\right],\
					\left[\begin{smallmatrix} 3 \\ 4 \\ 7 \end{smallmatrix}\right]</m> b)Â  <m>\left[\begin{smallmatrix}
					5 \\ 3 \\2 \end{smallmatrix}\right],\ \left[\begin{smallmatrix} -2 \\ 0 \\ 4
					\end{smallmatrix}\right],\ \left[\begin{smallmatrix} -1 \\ 6 \\ 5
					\end{smallmatrix}\right]</m> c)Â  <m>\left[\begin{smallmatrix} 1 \\ -1 \\2
					\end{smallmatrix}\right]</m>
				</p>
			</answer>
		</exercise>

		<exercise>
			<statement>
				<p>
					For the matrices in , find a linearly independent set of row vectors that span
					the row space (they donâ€™t need to be rows of the matrix).
				</p>
			</statement>

		</exercise>
		<exercise>
			<statement>
				<p>
					For the matrices in , find a linearly independent set of columns that span the
					column space. That is, find the pivot columns of the matrices.
				</p>
			</statement>

		</exercise>
		<exercise>
			<statement>
				<p> Compute the rank of the matrix <me>
						\begin{bmatrix}
						10 &amp; -2 &amp; 11 &amp; -7 \\
						-5 &amp; -2 &amp; -5 &amp; 5 \\
						1 &amp; 0 &amp; -4 &amp; -4 \\
						1 &amp; 2 &amp; 2 &amp; -1
						\end{bmatrix}
					</me>

				</p>
			</statement>

		</exercise>
		<exercise>
			<statement>
				<p> Compute the rank of the matrix <me>
						\begin{bmatrix}
						4 &amp; -2 &amp; 0 &amp; -4 \\
						3 &amp; -5 &amp; 2 &amp; 0 \\
						1 &amp; -2 &amp; 0 &amp; 1 \\
						-1 &amp; 1 &amp; 3 &amp; -3
						\end{bmatrix}
					</me>

				</p>
			</statement>

		</exercise>
		<exercise>
			<statement>
				<p> Find a linearly independent subset of the following vectors that has the same
					span. <me>
						\begin{bmatrix}
						-1 \\ 1 \\ 2
						\end{bmatrix}
						, \quad
						\begin{bmatrix}
						2 \\ -2 \\ -4
						\end{bmatrix}
						, \quad
						\begin{bmatrix}
						-2 \\ 4 \\ 1
						\end{bmatrix}
						, \quad
						\begin{bmatrix}
						-1 \\ 3 \\ -2
						\end{bmatrix}
					</me>

				</p>
			</statement>

			<answer>
				<p>
					<m>\left[\begin{smallmatrix} -1\\ 1\\ 2 \end{smallmatrix}\right],\
						\left[\begin{smallmatrix} -2 \\4 \\1 \end{smallmatrix}\right]</m>
				</p>
			</answer>
		</exercise>

		<exercise>
			<statement>
				<p> Find a linearly independent subset of the following vectors that has the same
					span. <me>
						\begin{bmatrix}
						0 \\ 0 \\ 0
						\end{bmatrix}
						, \quad
						\begin{bmatrix}
						3 \\ 1 \\ -5
						\end{bmatrix}
						, \quad
						\begin{bmatrix}
						0 \\ 3 \\ -1
						\end{bmatrix}
						, \quad
						\begin{bmatrix}
						-3 \\ 2 \\ 4
						\end{bmatrix}
					</me>

				</p>
			</statement>

		</exercise>
		<exercise>
			<statement>
				<p>
					For the following sets of vectors, determine if the set is linearly independent.
					Then find a basis for the subspace spanned by the vectors, and find the
					dimension of the subspace.
				</p>

				<!-- div attr= class="tasks"-->
				<p> (3) <m>\begin{bmatrix}
						1 \\ 1 \\ 1
						\end{bmatrix}
						, \quad
						\begin{bmatrix}
						-1 \\ -1 \\ -1
						\end{bmatrix}</m> <m>\begin{bmatrix}
						1 \\ 0 \\ 5
						\end{bmatrix}
						, \quad
						\begin{bmatrix}
						0 \\ 1 \\ 0
						\end{bmatrix}
						, \quad
						\begin{bmatrix}
						0 \\ -1 \\ 0
						\end{bmatrix}</m> <m>\begin{bmatrix}
						-4 \\ -3 \\ 5
						\end{bmatrix}
						, \quad
						\begin{bmatrix}
						2 \\ 3 \\ 3
						\end{bmatrix}
						, \quad
						\begin{bmatrix}
						2 \\ 0 \\ 2
						\end{bmatrix}</m> <m>\begin{bmatrix}
						1 \\ 3 \\ 0
						\end{bmatrix}
						, \quad
						\begin{bmatrix}
						0 \\ 2 \\ 2
						\end{bmatrix}
						, \quad
						\begin{bmatrix}
						-1 \\ -1 \\ 2
						\end{bmatrix}</m> <m>\begin{bmatrix}
						1 \\ 3
						\end{bmatrix}
						, \quad
						\begin{bmatrix}
						0 \\ 2
						\end{bmatrix}
						, \quad
						\begin{bmatrix}
						-1 \\ -1
						\end{bmatrix}</m> <m>\begin{bmatrix}
						3 \\ 1 \\ 3
						\end{bmatrix}
						, \quad
						\begin{bmatrix}
						2 \\ 4 \\ -4
						\end{bmatrix}
						, \quad
						\begin{bmatrix}
						-5 \\ -5 \\ -2
						\end{bmatrix}</m>
				</p><!--</div
				attr= class="tasks">-->
			</statement>

			<answer>
				<p> a)Â No, <m>\left[\begin{smallmatrix} 1 \\ 1 \\ 1 \end{smallmatrix}\right]</m>,
					Dimension 1 b)Â No, <m>\left[\begin{smallmatrix} 1 \\ 0 \\ 5
					\end{smallmatrix}\right],\ \left[\begin{smallmatrix} 0 \\ 1 \\ 0
					\end{smallmatrix}\right]</m>, Dimension 2 c)Â Yes, All 3, Dimension 3 d)Â No, <m>\left[\begin{smallmatrix}
					1 \\ 3 \\ 0 \end{smallmatrix}\right],\ \left[\begin{smallmatrix} 0 \\ 2 \\ 2
					\end{smallmatrix}\right]</m>, Dimension 2 e)Â No, <m>\left[\begin{smallmatrix} 1
					\\ 3 \end{smallmatrix}\right],\ \left[\begin{smallmatrix} 0 \\ 2
					\end{smallmatrix}\right]</m>, Dimension 2 f)Â  Yes, All 3, Dimension 3 </p>
			</answer>
		</exercise>

		<exercise>
			<statement>
				<p>
					For the following sets of vectors, determine if the set is linearly independent.
					Then find a basis for the subspace spanned by the vectors, and find the
					dimension of the subspace.
				</p>

				<!-- div attr= class="tasks"-->
				<p> (3) <m>\begin{bmatrix}
						1 \\ 2
						\end{bmatrix}
						, \quad
						\begin{bmatrix}
						1 \\ 1
						\end{bmatrix}</m> <m>\begin{bmatrix}
						1 \\ 1 \\ 1
						\end{bmatrix}
						, \quad
						\begin{bmatrix}
						2 \\ 2 \\ 2
						\end{bmatrix}
						, \quad
						\begin{bmatrix}
						1 \\ 1 \\ 2
						\end{bmatrix}</m> <m>\begin{bmatrix}
						5 \\ 3 \\ 1
						\end{bmatrix}
						, \quad
						\begin{bmatrix}
						5 \\ -1 \\ 5
						\end{bmatrix}
						, \quad
						\begin{bmatrix}
						-1 \\ 3 \\ -4
						\end{bmatrix}</m> <m>\begin{bmatrix}
						2 \\ 2 \\ 4
						\end{bmatrix}
						, \quad
						\begin{bmatrix}
						2 \\ 2 \\ 3
						\end{bmatrix}
						, \quad
						\begin{bmatrix}
						4 \\ 4 \\ -3
						\end{bmatrix}</m> <m>\begin{bmatrix}
						1 \\ 0
						\end{bmatrix}
						, \quad
						\begin{bmatrix}
						2 \\ 0
						\end{bmatrix}
						, \quad
						\begin{bmatrix}
						3 \\ 0
						\end{bmatrix}</m> <m>\begin{bmatrix}
						1 \\ 0 \\ 0
						\end{bmatrix}
						, \quad
						\begin{bmatrix}
						2 \\ 0 \\ 0
						\end{bmatrix}
						, \quad
						\begin{bmatrix}
						0 \\ 1 \\ 2
						\end{bmatrix}</m>
				</p><!--</div
				attr= class="tasks">-->
			</statement>

		</exercise>
		<exercise>
			<statement>
				<p> Suppose that <m>X</m> is the set of all the vectors of <m>{\mathbb{R}}^3</m>
					whose third component is zero. Is <m>X</m> a subspace? And if so, find a basis
					and the dimension. </p>
			</statement>

			<answer>
				<p> Yes. Basis: <m>\left\{\left[\begin{smallmatrix} 1 \\ 0 \\ 0
					\end{smallmatrix}\right],\ \left[\begin{smallmatrix} 0 \\ 1 \\ 0
					\end{smallmatrix}\right]\right\}</m> Dimension 2 </p>
			</answer>
		</exercise>

		<exercise>
			<statement>
				<p>
					Consider a set of 3 component vectors.
				</p>

				<!-- div attr= class="tasks"-->
				<p>
					How can it be shown if these vectors are linearly independent? Can a set of 4 of
					these 3 component vectors be linearly independent? Explain your answer. Can a
					set of 2 of these 3 component vectors be linearly independent? Explain. How
					would it be shown if these vectors make up a spanning set for all 3 component
					vectors? Can 4 vectors be a spanning set? Explain. Can 2 vectors be a spanning
					set? Explain.
				</p><!--</div
				attr= class="tasks">-->
			</statement>

		</exercise>
		<exercise>
			<statement>
				<p> Consider the vectors <me>
						\vec{v}_1 = \begin{bmatrix} 4 \\ 2 \\ -1 \end{bmatrix} \quad \vec{v}_2 =
					\begin{bmatrix} 3 \\ 5 \\ 1 \end{bmatrix} \qquad \begin{bmatrix} 1 \\ -1 \\ -1
					\end{bmatrix}.
					</me>
					Let <m>A</m> be the matrix with these vectors as columns and <m>\vec{b}</m> the
					vector <m>[1\ 0 \ 0]</m>. </p>

				<!-- div attr= class="tasks"-->
				<p> Compute the rank of <m>A</m> to determine how many of these vectors are linearly
					independent. Determine if <m>\vec{b}</m> is in the span of the given vectors by
					using row reduction to try to solve <m>A\vec{x} = \vec{b}</m>. Look at the
					columns of the row-reduced form of <m>A</m>. Is <m>\vec{b}</m> in the span of
					those vectors? What do these last two parts tell you about the span of the
					columns of a matrix, and the span of the columns of the row-reduced matrix? Now,
					build a matrix <m>D</m> with these vectors as rows. Row-reduce this matrix to
					get a matrix <m>D_2</m>. Is <m>\vec{b}</m> in the span of the rows of <m>D_2</m>?
					You canâ€™t check this in using the matrix form; instead, just brute force it
					based on the form of <m>D_2</m>. What does this potentially say about the span
					of the rows of <m>D</m> and the rows of <m>D_2</m>? </p><!--</div
				attr= class="tasks">-->
			</statement>

		</exercise>
		<exercise>
			<statement>
				<p> Complete with <me>
						\vec{v}_1 = \begin{bmatrix} 1 \\ 0 \\ -1 \\ 0 \end{bmatrix} \quad \vec{v}_2
					= \begin{bmatrix} -6 \\ 2 \\ 3 \\ -1 \end{bmatrix} \qquad \begin{bmatrix} -13 \\
					3 \\ 1 \\ 1 \end{bmatrix} \quad \vec{v}_4 \begin{bmatrix} 11 \ -1 \\ -5 \\ -1
					\end{bmatrix} \quad \vec{b} = \begin{bmatrix} 1 \\ 0 \\ 0 \\ 0 \end{bmatrix}.
					</me>

				</p>
			</statement>

			<answer>
				<p> a)Â  3 b)Â No c)Â  Yes d)Â They are not the same e)Â  <m>\left[\begin{smallmatrix} 1
					&amp; 0 &amp; -1 &amp; 0 \\ 0 &amp; 1 &amp; -6 &amp; 1 \\ 0 &amp; 0 &amp; 3
					&amp; -1 \\ 0 &amp; 0 &amp; 0 &amp; 0 \end{smallmatrix}\right]</m> f)Â  No </p>
			</answer>
		</exercise>

		<exercise>
			<statement>
				<p>
					Compute the inverse of the given matrices
				</p>

				<!-- div attr= class="tasks"-->
				<p> (3) <m>\begin{bmatrix}
						1 &amp; 0 &amp; 0 \\
						0 &amp; 0 &amp; 1 \\
						0 &amp; 1 &amp; 0
						\end{bmatrix}</m> <m>\begin{bmatrix}
						1 &amp; 1 &amp; 1 \\
						0 &amp; 2 &amp; 1 \\
						0 &amp; 0 &amp; 1
						\end{bmatrix}</m> <m>\begin{bmatrix}
						1 &amp; 2 &amp; 3 \\
						2 &amp; 0 &amp; 1 \\
						0 &amp; 2 &amp; 1
						\end{bmatrix}</m>
				</p><!--</div
				attr= class="tasks">-->
			</statement>

			<answer>
				<p> a)Â  <m>\left[\begin{smallmatrix} 1 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 1 \\ 0
					&amp; 1 &amp; 0 \end{smallmatrix}\right]</m> b)Â  <m>\left[\begin{smallmatrix} 1
					&amp; -1/2 &amp; -1/2 \\ 0 &amp; 1/2 &amp; -1/2 \\ 0 &amp; 0 &amp; 1
					\end{smallmatrix}\right]</m> c)Â <m>\left[\begin{smallmatrix} -1/3 &amp; 2/3
					&amp; 1/3 \\ -1/3 &amp; 1/6 &amp; 5/6 \\ 2/3 &amp; -1/3 &amp; -2/3
					\end{smallmatrix}\right]</m>
				</p>
			</answer>
		</exercise>

		<exercise>
			<statement>
				<p>
					Compute the inverse of the given matrices
				</p>

				<!-- div attr= class="tasks"-->
				<p> (3) <m>\begin{bmatrix}
						0 &amp; 1 &amp; 0 \\
						-1 &amp; 0 &amp; 0 \\
						0 &amp; 0 &amp; 1
						\end{bmatrix}</m> <m>\begin{bmatrix}
						1 &amp; 1 &amp; 1 \\
						1 &amp; 1 &amp; 0 \\
						1 &amp; 0 &amp; 0
						\end{bmatrix}</m> <m>\begin{bmatrix}
						2 &amp; 4 &amp; 0 \\
						2 &amp; 2 &amp; 3 \\
						2 &amp; 4 &amp; 1
						\end{bmatrix}</m>
				</p><!--</div
				attr= class="tasks">-->
			</statement>

		</exercise>
		<exercise>
			<statement>
				<p> By computing the inverse, solve the following systems for <m>\vec{x}</m>. </p>

				<!-- div attr= class="tasks"-->
				<p> (2) <m>\begin{bmatrix}
						4 &amp; 1 \\
						-1 &amp; 3
						\end{bmatrix} \vec{x} =
						\begin{bmatrix} 13 \\ 26 \end{bmatrix}</m> <m>\begin{bmatrix}
						3 &amp; 3 \\
						3 &amp; 4
						\end{bmatrix} \vec{x} =
						\begin{bmatrix} 2 \\ -1 \end{bmatrix}</m>
				</p><!--</div
				attr= class="tasks">-->
			</statement>

			<answer>
				<p> a)Â  <m>\left[\begin{smallmatrix} 1 \\ 9 \end{smallmatrix}\right]</m> b)Â <m>\left[\begin{smallmatrix}
					11/3 \\ -3 \end{smallmatrix}\right]</m>
				</p>
			</answer>
		</exercise>

		<exercise>
			<statement>
				<p> By computing the inverse, solve the following systems for <m>\vec{x}</m>. </p>

				<!-- div attr= class="tasks"-->
				<p> (2) <m>\begin{bmatrix}
						-1 &amp; 1 \\
						3 &amp; 3
						\end{bmatrix} \vec{x} =
						\begin{bmatrix} 4 \\ 6 \end{bmatrix}</m> <m>\begin{bmatrix}
						2 &amp; 7 \\
						1 &amp; 6
						\end{bmatrix} \vec{x} =
						\begin{bmatrix} 1 \\ 3 \end{bmatrix}</m>
				</p><!--</div
				attr= class="tasks">-->
			</statement>

		</exercise>
		<exercise>
			<statement>
				<p>
					For each of the following matrices below:
				</p>

				<!-- div attr= class="tasks"-->
				<p>
					Compute the trace and determinant of the matrix, and Find the eigenvalues of the
					matrix and verify that the trace is the sum of the eigenvalues and the
					determinant is the product.
				</p><!--</div
				attr= class="tasks">-->

				<p>

					<me>
						(i) \ \begin{bmatrix} -4 &amp; 2 \\ -9 &amp; 5 \end{bmatrix} \qquad (ii) \
						\begin{bmatrix} 2 &amp; -3 \\ 6 &amp; -4 \end{bmatrix} \qquad (iii) \
						\begin{bmatrix} -10&amp; -12 \\ 6 &amp; 8\end{bmatrix}. \qquad (iv) \
						\begin{bmatrix} -7 &amp; -9 \\ 1 &amp; -1 \end{bmatrix}
					</me>

				</p>
			</statement>

		</exercise>
		<exercise>
			<statement>
				<p>
					For each of the following matrices below:
				</p>

				<!-- div attr= class="tasks"-->
				<p>
					Compute the trace and determinant of the matrix, and Find the eigenvalues of the
					matrix and verify that the trace is the sum of the eigenvalues and the
					determinant is the product.
				</p><!--</div
				attr= class="tasks">-->

				<p>

					<me>
						(i) \ \begin{bmatrix} -1 &amp; -16 &amp; -4 \\ 1 &amp; 6 &amp; 1 \\ -2 &amp;
						-4 &amp; 1 \end{bmatrix} \qquad (ii) \ \begin{bmatrix} 1 &amp; 2 &amp; 0 \\
						-12 &amp; -13 &amp; -4 \\ 16 &amp; 14 &amp; 3 \end{bmatrix} \qquad (iii)\
						\begin{bmatrix} 10 &amp; -7 &amp; -14 \\ 0 &amp; 5 &amp; 6 \\ 7 &amp; -8
						&amp; -14 \end{bmatrix}
					</me>

				</p>
			</statement>
		</exercise>
	</exercises>

</section>



